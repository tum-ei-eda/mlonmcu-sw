#define VL s3

#define t7 a6
#define t8 a7

#if __riscv_xlen == 32
#define IF64(...)
#define lx lw
#define sx sw
#else
#define IF64(...) __VA_ARGS__
#define lx ld
#define sx sd
#endif

#if __riscv_flen == 64
#define IF_F64(...) __VA_ARGS__
#else
#define IF_F64(...)
#endif

#if __riscv_v_elen_fp == 64
#define IF_VF64(...) __VA_ARGS__
#else
#define IF_VF64(...)
#endif

#include "config.h"



.macro m_nop
.endm

.macro m_1bit
vid.v v0
remu t0, t0, VL
vmseq.vx v8, v0, t0
.endm




























.macro m_mod_v24_e8_vl
	csrr t8, vtype
	vsetvli t0, x0, e8, m8, ta, ma
	vremu.vx v24, v24, VL
	vsetvl x0, VL, t8
.endm

.macro m_mod_v24_e16_vl
	csrr t8, vtype
	vsetvli t0, x0, e16, m8, ta, ma
	vremu.vx v24, v24, VL
	vsetvl x0, VL, t8
.endm

.macro m_mod_t0_vl
	remu t0, t0, VL
.endm

.macro m_unimpl
	li a0, -1
	ret
.endm






# calls $1 with: name type setup code:vararg



.data


#if __riscv_xlen == 32
#define defptr .word
#else
#define defptr .dword
#endif






.balign 8


.global bench_mf8
	bench_mf8:

	defptr bench_add_m1
	defptr bench_mul_m1
	defptr bench_vaddvv_m1
	defptr bench_vaddvvm_m1
	defptr bench_vaddvx_m1
	defptr bench_vaddvxm_m1
	defptr bench_vaddvi_m1
	defptr bench_vaddvim_m1
	defptr bench_vsubvv_m1
	defptr bench_vsubvvm_m1
	defptr bench_vsubvx_m1
	defptr bench_vsubvxm_m1
	defptr bench_vrsubvx_m1
	defptr bench_vrsubvxm_m1
	defptr bench_vrsubvi_m1
	defptr bench_vrsubvim_m1
	defptr bench_vminuvv_m1
	defptr bench_vminuvvm_m1
	defptr bench_vminuvx_m1
	defptr bench_vminuvxm_m1
	defptr bench_vminvv_m1
	defptr bench_vminvvm_m1
	defptr bench_vminvx_m1
	defptr bench_vminvxm_m1
	defptr bench_vmaxuvv_m1
	defptr bench_vmaxuvvm_m1
	defptr bench_vmaxuvx_m1
	defptr bench_vmaxuvxm_m1
	defptr bench_vmaxvv_m1
	defptr bench_vmaxvvm_m1
	defptr bench_vmaxvx_m1
	defptr bench_vmaxvxm_m1
	defptr bench_vandvv_m1
	defptr bench_vandvvm_m1
	defptr bench_vandvx_m1
	defptr bench_vandvxm_m1
	defptr bench_vandvi_m1
	defptr bench_vandvim_m1
	defptr bench_vorvv_m1
	defptr bench_vorvvm_m1
	defptr bench_vorvx_m1
	defptr bench_vorvxm_m1
	defptr bench_vorvi_m1
	defptr bench_vorvim_m1
	defptr bench_vxorvv_m1
	defptr bench_vxorvvm_m1
	defptr bench_vxorvx_m1
	defptr bench_vxorvxm_m1
	defptr bench_vxorvi_m1
	defptr bench_vxorvim_m1

	defptr bench_vrgathervv_m1
	defptr bench_vrgathervvm_m1
	defptr bench_vrgathervx_m1
	defptr bench_vrgathervxm_m1
	defptr bench_vrgathervi_m1
	defptr bench_vrgathervim_m1
	defptr bench_vslideupvx_m1
	defptr bench_vslideupvxm_m1
	defptr bench_vslideupvi_m1
	defptr bench_vslideupvim_m1
	defptr bench_vrgatherei16vv_m1
	defptr bench_vrgatherei16vvm_m1

	defptr bench_vslidedownvx_m1
	defptr bench_vslidedownvxm_m1
	defptr bench_vslidedownvi_m1
	defptr bench_vslidedownvim_m1

	defptr bench_vredsumvs_m1
	defptr bench_vredsumvsm_m1
	defptr bench_vredandvs_m1
	defptr bench_vredandvsm_m1
	defptr bench_vredorvs_m1
	defptr bench_vredorvsm_m1
	defptr bench_vredxorvs_m1
	defptr bench_vredxorvsm_m1
	defptr bench_vredminuvs_m1
	defptr bench_vredminuvsm_m1
	defptr bench_vredminvs_m1
	defptr bench_vredminvsm_m1
	defptr bench_vredmaxuvs_m1
	defptr bench_vredmaxuvsm_m1
	defptr bench_vredmaxvs_m1
	defptr bench_vredmaxvsm_m1

	defptr bench_vaadduvv_m1
	defptr bench_vaadduvvm_m1
	defptr bench_vaadduvx_m1
	defptr bench_vaadduvxm_m1
	defptr bench_vaaddvv_m1
	defptr bench_vaaddvvm_m1
	defptr bench_vaaddvx_m1
	defptr bench_vaaddvxm_m1
	defptr bench_vasubuvv_m1
	defptr bench_vasubuvvm_m1
	defptr bench_vasubuvx_m1
	defptr bench_vasubuvxm_m1
	defptr bench_vasubvv_m1
	defptr bench_vasubvvm_m1
	defptr bench_vasubvx_m1
	defptr bench_vasubvxm_m1

	defptr bench_vslide1upvx_m1
	defptr bench_vslide1upvxm_m1
	defptr bench_vslide1downvx_m1
	defptr bench_vslide1downvxm_m1

	defptr bench_vadcvvm_m1
	 defptr bench_vadcvxm_m1
	 defptr bench_vadcvim_m1
	defptr bench_vmadcvvm_m1
	 defptr bench_vmadcvxm_m1
	 defptr bench_vmadcvim_m1
	defptr bench_vmadcvv_m1
	defptr bench_vmadcvx_m1
	defptr bench_vmadcvi_m1
	defptr bench_vsbcvvm_m1
	 defptr bench_vsbcvxm_m1
	defptr bench_vmsbcvvm_m1
	 defptr bench_vmsbcvxm_m1
	defptr bench_vmsbcvv_m1
	defptr bench_vmsbcvx_m1

	defptr bench_vmergevvm_m1
	 defptr bench_vmergevxm_m1
	 defptr bench_vmergevim_m1
	defptr bench_vmvvv_m1
	defptr bench_vmvvx_m1
	defptr bench_vmvvi_m1
	defptr bench_vmseqvv_m1
	defptr bench_vmseqvvm_m1
	defptr bench_vmseqvx_m1
	defptr bench_vmseqvxm_m1
	defptr bench_vmseqvi_m1
	defptr bench_vmseqvim_m1
	defptr bench_vmsnevv_m1
	defptr bench_vmsnevvm_m1
	defptr bench_vmsnevx_m1
	defptr bench_vmsnevxm_m1
	defptr bench_vmsnevi_m1
	defptr bench_vmsnevim_m1
	defptr bench_vmsltuvv_m1
	defptr bench_vmsltuvvm_m1
	defptr bench_vmsltuvx_m1
	defptr bench_vmsltuvxm_m1
	defptr bench_vmsltvv_m1
	defptr bench_vmsltvvm_m1
	defptr bench_vmsltvx_m1
	defptr bench_vmsltvxm_m1
	defptr bench_vmsleuvv_m1
	defptr bench_vmsleuvvm_m1
	defptr bench_vmsleuvx_m1
	defptr bench_vmsleuvxm_m1
	defptr bench_vmsleuvi_m1
	defptr bench_vmsleuvim_m1
	defptr bench_vmslevv_m1
	defptr bench_vmslevvm_m1
	defptr bench_vmslevx_m1
	defptr bench_vmslevxm_m1
	defptr bench_vmslevi_m1
	defptr bench_vmslevim_m1
	defptr bench_vmsgtuvx_m1
	defptr bench_vmsgtuvxm_m1
	defptr bench_vmsgtuvi_m1
	defptr bench_vmsgtuvim_m1
	defptr bench_vmsgtvx_m1
	defptr bench_vmsgtvxm_m1
	defptr bench_vmsgtvi_m1
	defptr bench_vmsgtvim_m1

	defptr bench_vcompressvm_m1

	defptr bench_vmandnmm_m1
	defptr bench_vmandmm_m1
	defptr bench_vmormm_m1
	defptr bench_vmxormm_m1
	defptr bench_vmornmm_m1
	defptr bench_vmnandmm_m1
	defptr bench_vmnormm_m1
	defptr bench_vmxnormm_m1

	defptr bench_vsadduvv_m1
	defptr bench_vsadduvvm_m1
	defptr bench_vsadduvx_m1
	defptr bench_vsadduvxm_m1
	defptr bench_vsadduvi_m1
	defptr bench_vsadduvim_m1
	defptr bench_vsaddvv_m1
	defptr bench_vsaddvvm_m1
	defptr bench_vsaddvx_m1
	defptr bench_vsaddvxm_m1
	defptr bench_vsaddvi_m1
	defptr bench_vsaddvim_m1
	defptr bench_vssubuvv_m1
	defptr bench_vssubuvvm_m1
	defptr bench_vssubuvx_m1
	defptr bench_vssubuvxm_m1
	defptr bench_vssubvv_m1
	defptr bench_vssubvvm_m1
	defptr bench_vssubvx_m1
	defptr bench_vssubvxm_m1
	defptr bench_vsllvv_m1
	defptr bench_vsllvvm_m1
	defptr bench_vsllvx_m1
	defptr bench_vsllvxm_m1
	defptr bench_vsllvi_m1
	defptr bench_vsllvim_m1
	defptr bench_vsmulvv_m1
	defptr bench_vsmulvvm_m1
	defptr bench_vsmulvx_m1
	defptr bench_vsmulvxm_m1
	defptr bench_vmv1rv_m1
	defptr bench_vmv2rv_m1
	defptr bench_vmv4rv_m1
	defptr bench_vmv8rv_m1
	defptr bench_vsrlvv_m1
	defptr bench_vsrlvvm_m1
	defptr bench_vsrlvx_m1
	defptr bench_vsrlvxm_m1
	defptr bench_vsrlvi_m1
	defptr bench_vsrlvim_m1
	defptr bench_vsravv_m1
	defptr bench_vsravvm_m1
	defptr bench_vsravx_m1
	defptr bench_vsravxm_m1
	defptr bench_vsravi_m1
	defptr bench_vsravim_m1
	defptr bench_vssrlvv_m1
	defptr bench_vssrlvvm_m1
	defptr bench_vssrlvx_m1
	defptr bench_vssrlvxm_m1
	defptr bench_vssrlvi_m1
	defptr bench_vssrlvim_m1

	defptr bench_vdivuvv_m1
	defptr bench_vdivuvvm_m1
	defptr bench_vdivuvx_m1
	defptr bench_vdivuvxm_m1
	defptr bench_vdivvv_m1
	defptr bench_vdivvvm_m1
	defptr bench_vdivvx_m1
	defptr bench_vdivvxm_m1
	defptr bench_vremuvv_m1
	defptr bench_vremuvvm_m1
	defptr bench_vremuvx_m1
	defptr bench_vremuvxm_m1
	defptr bench_vremvv_m1
	defptr bench_vremvvm_m1
	defptr bench_vremvx_m1
	defptr bench_vremvxm_m1
	defptr bench_vmulhuvv_m1
	defptr bench_vmulhuvvm_m1
	defptr bench_vmulhuvx_m1
	defptr bench_vmulhuvxm_m1
	defptr bench_vmulvv_m1
	defptr bench_vmulvvm_m1
	defptr bench_vmulvx_m1
	defptr bench_vmulvxm_m1
	defptr bench_vmulhsuvv_m1
	defptr bench_vmulhsuvvm_m1
	defptr bench_vmulhsuvx_m1
	defptr bench_vmulhsuvxm_m1
	defptr bench_vmulhvv_m1
	defptr bench_vmulhvvm_m1
	defptr bench_vmulhvx_m1
	defptr bench_vmulhvxm_m1
	defptr bench_vmaddvv_m1
	defptr bench_vmaddvvm_m1
	defptr bench_vmaddvx_m1
	defptr bench_vmaddvxm_m1
	defptr bench_vmaccvv_m1
	defptr bench_vmaccvvm_m1
	defptr bench_vmaccvx_m1
	defptr bench_vmaccvxm_m1

	defptr bench_vnsrlwv_m1
	defptr bench_vnsrlwvm_m1
	defptr bench_vnsrlwx_m1
	defptr bench_vnsrlwxm_m1
	defptr bench_vnsrlwi_m1
	defptr bench_vnsrlwim_m1
	defptr bench_vnsrawv_m1
	defptr bench_vnsrawvm_m1
	defptr bench_vnsrawx_m1
	defptr bench_vnsrawxm_m1
	defptr bench_vnsrawi_m1
	defptr bench_vnsrawim_m1
	defptr bench_vnclipuwv_m1
	defptr bench_vnclipuwvm_m1
	defptr bench_vnclipuwx_m1
	defptr bench_vnclipuwxm_m1
	defptr bench_vnclipuwi_m1
	defptr bench_vnclipuwim_m1
	defptr bench_vnclipwv_m1
	defptr bench_vnclipwvm_m1
	defptr bench_vnclipwx_m1
	defptr bench_vnclipwxm_m1
	defptr bench_vnclipwi_m1
	defptr bench_vnclipwim_m1
	defptr bench_vnmsubvv_m1
	defptr bench_vnmsubvvm_m1
	defptr bench_vnmsubvx_m1
	defptr bench_vnmsubvxm_m1
	defptr bench_vnmsacvv_m1
	defptr bench_vnmsacvvm_m1
	defptr bench_vnmsacvx_m1
	defptr bench_vnmsacvxm_m1

	defptr bench_vwadduvv_m1
	defptr bench_vwadduvvm_m1
	defptr bench_vwadduvx_m1
	defptr bench_vwadduvxm_m1
	defptr bench_vwaddvv_m1
	defptr bench_vwaddvvm_m1
	defptr bench_vwaddvx_m1
	defptr bench_vwaddvxm_m1
	defptr bench_vwsubuvv_m1
	defptr bench_vwsubuvvm_m1
	defptr bench_vwsubuvx_m1
	defptr bench_vwsubuvxm_m1
	defptr bench_vwsubvv_m1
	defptr bench_vwsubvvm_m1
	defptr bench_vwsubvx_m1
	defptr bench_vwsubvxm_m1
	defptr bench_vwadduwv_m1
	defptr bench_vwadduwvm_m1
	defptr bench_vwadduwx_m1
	defptr bench_vwadduwxm_m1
	defptr bench_vwaddwv_m1
	defptr bench_vwaddwvm_m1
	defptr bench_vwaddwx_m1
	defptr bench_vwaddwxm_m1
	defptr bench_vwsubuwv_m1
	defptr bench_vwsubuwvm_m1
	defptr bench_vwsubuwx_m1
	defptr bench_vwsubuwxm_m1
	defptr bench_vwsubwv_m1
	defptr bench_vwsubwvm_m1
	defptr bench_vwsubwx_m1
	defptr bench_vwsubwxm_m1
	defptr bench_vwmuluvv_m1
	defptr bench_vwmuluvvm_m1
	defptr bench_vwmuluvx_m1
	defptr bench_vwmuluvxm_m1
	defptr bench_vwmulsuvv_m1
	defptr bench_vwmulsuvvm_m1
	defptr bench_vwmulsuvx_m1
	defptr bench_vwmulsuvxm_m1
	defptr bench_vwmulvv_m1
	defptr bench_vwmulvvm_m1
	defptr bench_vwmulvx_m1
	defptr bench_vwmulvxm_m1
	defptr bench_vwmaccuvv_m1
	defptr bench_vwmaccuvvm_m1
	defptr bench_vwmaccuvx_m1
	defptr bench_vwmaccuvxm_m1
	defptr bench_vwmaccvv_m1
	defptr bench_vwmaccvvm_m1
	defptr bench_vwmaccvx_m1
	defptr bench_vwmaccvxm_m1
	defptr bench_vwmaccsuvv_m1
	defptr bench_vwmaccsuvvm_m1
	defptr bench_vwmaccsuvx_m1
	defptr bench_vwmaccsuvxm_m1
	defptr bench_vwmaccusvx_m1
	defptr bench_vwmaccusvxm_m1

	defptr bench_vfaddvv_m1
	defptr bench_vfaddvvm_m1
	defptr bench_vfaddvf_m1
	defptr bench_vfaddvfm_m1
	defptr bench_vfsubvv_m1
	defptr bench_vfsubvvm_m1
	defptr bench_vfsubvf_m1
	defptr bench_vfsubvfm_m1
	defptr bench_vfminvv_m1
	defptr bench_vfminvvm_m1
	defptr bench_vfminvf_m1
	defptr bench_vfminvfm_m1
	defptr bench_vfmaxvv_m1
	defptr bench_vfmaxvvm_m1
	defptr bench_vfmaxvf_m1
	defptr bench_vfmaxvfm_m1
	defptr bench_vfsgnjvv_m1
	defptr bench_vfsgnjvvm_m1
	defptr bench_vfsgnjvf_m1
	defptr bench_vfsgnjvfm_m1
	defptr bench_vfsgnjnvv_m1
	defptr bench_vfsgnjnvvm_m1
	defptr bench_vfsgnjnvf_m1
	defptr bench_vfsgnjnvfm_m1
	defptr bench_vfsgnjxvv_m1
	defptr bench_vfsgnjxvvm_m1
	defptr bench_vfsgnjxvf_m1
	defptr bench_vfsgnjxvfm_m1
	defptr bench_vfslide1upvf_m1
	defptr bench_vfslide1upvfm_m1
	defptr bench_vfslide1downvf_m1
	defptr bench_vfslide1downvfm_m1

	defptr bench_vfredusumvs_m1
	defptr bench_vfredusumvsm_m1
	defptr bench_vfredosumvs_m1
	defptr bench_vfredosumvsm_m1
	defptr bench_vfredminvs_m1
	defptr bench_vfredminvsm_m1
	defptr bench_vfredmaxvs_m1
	defptr bench_vfredmaxvsm_m1

	defptr bench_vfmergevfm_m1
	defptr bench_vfmvvf_m1

	defptr bench_vmfeqvv_m1
	defptr bench_vmfeqvvm_m1
	defptr bench_vmfeqvf_m1
	defptr bench_vmfeqvfm_m1
	defptr bench_vmflevv_m1
	defptr bench_vmflevvm_m1
	defptr bench_vmflevf_m1
	defptr bench_vmflevfm_m1
	defptr bench_vmfltvv_m1
	defptr bench_vmfltvvm_m1
	defptr bench_vmfltvf_m1
	defptr bench_vmfltvfm_m1
	defptr bench_vmfnevv_m1
	defptr bench_vmfnevvm_m1
	defptr bench_vmfnevf_m1
	defptr bench_vmfnevfm_m1
	defptr bench_vmfgtvv_m1
	defptr bench_vmfgtvvm_m1
	defptr bench_vmfgtvf_m1
	defptr bench_vmfgtvfm_m1
	defptr bench_vmfgevv_m1
	defptr bench_vmfgevvm_m1
	defptr bench_vmfgevf_m1
	defptr bench_vmfgevfm_m1

	defptr bench_vfdivvv_m1
	defptr bench_vfdivvvm_m1
	defptr bench_vfdivvf_m1
	defptr bench_vfdivvfm_m1
	defptr bench_vfrdivvf_m1
	defptr bench_vfrdivvfm_m1
	defptr bench_vfmulvv_m1
	defptr bench_vfmulvvm_m1
	defptr bench_vfmulvf_m1
	defptr bench_vfmulvfm_m1
	defptr bench_vfrsubvf_m1
	defptr bench_vfrsubvfm_m1
	defptr bench_vfmaddvv_m1
	defptr bench_vfmaddvvm_m1
	defptr bench_vfmaddvf_m1
	defptr bench_vfmaddvfm_m1
	defptr bench_vfmsubvv_m1
	defptr bench_vfmsubvvm_m1
	defptr bench_vfmsubvf_m1
	defptr bench_vfmsubvfm_m1
	defptr bench_vfmaccvv_m1
	defptr bench_vfmaccvvm_m1
	defptr bench_vfmaccvf_m1
	defptr bench_vfmaccvfm_m1
	defptr bench_vfmsacvv_m1
	defptr bench_vfmsacvvm_m1
	defptr bench_vfmsacvf_m1
	defptr bench_vfmsacvfm_m1

	defptr bench_vfnmsacvv_m1
	defptr bench_vfnmsacvvm_m1
	defptr bench_vfnmsacvf_m1
	defptr bench_vfnmsacvfm_m1
	defptr bench_vfnmaccvv_m1
	defptr bench_vfnmaccvvm_m1
	defptr bench_vfnmaccvf_m1
	defptr bench_vfnmaccvfm_m1
	defptr bench_vfnmsubvv_m1
	defptr bench_vfnmsubvvm_m1
	defptr bench_vfnmsubvf_m1
	defptr bench_vfnmsubvfm_m1
	defptr bench_vfnmaddvv_m1
	defptr bench_vfnmaddvvm_m1
	defptr bench_vfnmaddvf_m1
	defptr bench_vfnmaddvfm_m1

	defptr bench_vwredsumuvs_m1
	defptr bench_vwredsumuvsm_m1
	defptr bench_vwredsumvs_m1
	defptr bench_vwredsumvsm_m1

	defptr bench_vfwaddvv_m1
	defptr bench_vfwaddvvm_m1
	defptr bench_vfwaddvf_m1
	defptr bench_vfwaddvfm_m1
	defptr bench_vfwsubvv_m1
	defptr bench_vfwsubvvm_m1
	defptr bench_vfwsubvf_m1
	defptr bench_vfwsubvfm_m1
	defptr bench_vfwaddwv_m1
	defptr bench_vfwaddwvm_m1
	defptr bench_vfwaddwf_m1
	defptr bench_vfwaddwfm_m1
	defptr bench_vfwsubwv_m1
	defptr bench_vfwsubwvm_m1
	defptr bench_vfwsubwf_m1
	defptr bench_vfwsubwfm_m1
	defptr bench_vfwmulvv_m1
	defptr bench_vfwmulvvm_m1
	defptr bench_vfwmulvf_m1
	defptr bench_vfwmulvfm_m1
	defptr bench_vfwmaccvv_m1
	defptr bench_vfwmaccvvm_m1
	defptr bench_vfwmaccvf_m1
	defptr bench_vfwmaccvfm_m1
	defptr bench_vfwnmaccvv_m1
	defptr bench_vfwnmaccvvm_m1
	defptr bench_vfwnmaccvf_m1
	defptr bench_vfwnmaccvfm_m1
	defptr bench_vfwmsacvv_m1
	defptr bench_vfwmsacvvm_m1
	defptr bench_vfwmsacvf_m1
	defptr bench_vfwmsacvfm_m1
	defptr bench_vfwnmsacvv_m1
	defptr bench_vfwnmsacvvm_m1
	defptr bench_vfwnmsacvf_m1
	defptr bench_vfwnmsacvfm_m1

	defptr bench_vfwredosumvs_m1
	defptr bench_vfwredosumvsm_m1
	defptr bench_vfwredusumvs_m1
	defptr bench_vfwredusumvsm_m1

	defptr bench_vmvsx_m1
	defptr bench_vmvxs_m1

	defptr bench_vcpopm_m1
	defptr bench_vcpopmm_m1
	defptr bench_vfirstm_m1
	defptr bench_vfirstmm_m1
	defptr bench_vzextvf2_m1
	defptr bench_vzextvf2m_m1
	defptr bench_vsextvf2_m1
	defptr bench_vsextvf2m_m1
	defptr bench_vzextvf4_m1
	defptr bench_vzextvf4m_m1
	defptr bench_vsextvf4_m1
	defptr bench_vsextvf4m_m1
	defptr bench_vzextvf8_m1
	defptr bench_vzextvf8m_m1
	defptr bench_vsextvf8_m1
	defptr bench_vsextvf8m_m1

	defptr bench_vfmvfs_m1
	defptr bench_vfmvsf_m1

	defptr bench_vfcvtxufv_m1
	defptr bench_vfcvtxufvm_m1
	defptr bench_vfcvtxfv_m1
	defptr bench_vfcvtxfvm_m1
	defptr bench_vfcvtfxuv_m1
	defptr bench_vfcvtfxuvm_m1
	defptr bench_vfcvtfxv_m1
	defptr bench_vfcvtfxvm_m1
	defptr bench_vfcvtrtzxfv_m1
	defptr bench_vfcvtrtzxfvm_m1
	defptr bench_vfcvtrtzxufv_m1
	defptr bench_vfcvtrtzxufvm_m1

	defptr bench_vfwcvtxufv_m1
	defptr bench_vfwcvtxufvm_m1
	defptr bench_vfwcvtxfv_m1
	defptr bench_vfwcvtxfvm_m1
	defptr bench_vfwcvtfxuv_m1
	defptr bench_vfwcvtfxuvm_m1
	defptr bench_vfwcvtfxv_m1
	defptr bench_vfwcvtfxvm_m1
	defptr bench_vfwcvtffv_m1
	defptr bench_vfwcvtffvm_m1
	defptr bench_vfwcvtrtzxufv_m1
	defptr bench_vfwcvtrtzxufvm_m1
	defptr bench_vfwcvtrtzxfv_m1
	defptr bench_vfwcvtrtzxfvm_m1

	defptr bench_vfncvtxufw_m1
	defptr bench_vfncvtxufwm_m1
	defptr bench_vfncvtxfw_m1
	defptr bench_vfncvtxfwm_m1
	defptr bench_vfncvtfxuw_m1
	defptr bench_vfncvtfxuwm_m1
	defptr bench_vfncvtfxw_m1
	defptr bench_vfncvtfxwm_m1
	defptr bench_vfncvtffw_m1
	defptr bench_vfncvtffwm_m1
	defptr bench_vfncvtrtzxfw_m1
	defptr bench_vfncvtrtzxfwm_m1
	defptr bench_vfncvtrtzxufw_m1
	defptr bench_vfncvtrtzxufwm_m1
	defptr bench_vfncvt.rod.f.f.w_m1
	defptr bench_vfncvt.rod.f.f.wm_m1

	defptr bench_vfsqrtv_m1
	defptr bench_vfsqrtvm_m1
	defptr bench_vfrsqrt7v_m1
	defptr bench_vfrsqrt7vm_m1
	defptr bench_vfrec7v_m1
	defptr bench_vfrec7vm_m1
	defptr bench_vfclassv_m1
	defptr bench_vfclassvm_m1

	defptr bench_vmsbfm_m1
	defptr bench_vmsbfmm_m1
	defptr bench_vmsofm_m1
	defptr bench_vmsofmm_m1
	defptr bench_vmsifm_m1
	defptr bench_vmsifmm_m1
	defptr bench_viotam_m1
	defptr bench_viotamm_m1
	defptr bench_vidv_m1
	defptr bench_vidvm_m1



.balign 8


.global bench_mf4
	bench_mf4:

	defptr bench_add_m1
	defptr bench_mul_m1
	defptr bench_vaddvv_m1
	defptr bench_vaddvvm_m1
	defptr bench_vaddvx_m1
	defptr bench_vaddvxm_m1
	defptr bench_vaddvi_m1
	defptr bench_vaddvim_m1
	defptr bench_vsubvv_m1
	defptr bench_vsubvvm_m1
	defptr bench_vsubvx_m1
	defptr bench_vsubvxm_m1
	defptr bench_vrsubvx_m1
	defptr bench_vrsubvxm_m1
	defptr bench_vrsubvi_m1
	defptr bench_vrsubvim_m1
	defptr bench_vminuvv_m1
	defptr bench_vminuvvm_m1
	defptr bench_vminuvx_m1
	defptr bench_vminuvxm_m1
	defptr bench_vminvv_m1
	defptr bench_vminvvm_m1
	defptr bench_vminvx_m1
	defptr bench_vminvxm_m1
	defptr bench_vmaxuvv_m1
	defptr bench_vmaxuvvm_m1
	defptr bench_vmaxuvx_m1
	defptr bench_vmaxuvxm_m1
	defptr bench_vmaxvv_m1
	defptr bench_vmaxvvm_m1
	defptr bench_vmaxvx_m1
	defptr bench_vmaxvxm_m1
	defptr bench_vandvv_m1
	defptr bench_vandvvm_m1
	defptr bench_vandvx_m1
	defptr bench_vandvxm_m1
	defptr bench_vandvi_m1
	defptr bench_vandvim_m1
	defptr bench_vorvv_m1
	defptr bench_vorvvm_m1
	defptr bench_vorvx_m1
	defptr bench_vorvxm_m1
	defptr bench_vorvi_m1
	defptr bench_vorvim_m1
	defptr bench_vxorvv_m1
	defptr bench_vxorvvm_m1
	defptr bench_vxorvx_m1
	defptr bench_vxorvxm_m1
	defptr bench_vxorvi_m1
	defptr bench_vxorvim_m1

	defptr bench_vrgathervv_m1
	defptr bench_vrgathervvm_m1
	defptr bench_vrgathervx_m1
	defptr bench_vrgathervxm_m1
	defptr bench_vrgathervi_m1
	defptr bench_vrgathervim_m1
	defptr bench_vslideupvx_m1
	defptr bench_vslideupvxm_m1
	defptr bench_vslideupvi_m1
	defptr bench_vslideupvim_m1
	defptr bench_vrgatherei16vv_m1
	defptr bench_vrgatherei16vvm_m1

	defptr bench_vslidedownvx_m1
	defptr bench_vslidedownvxm_m1
	defptr bench_vslidedownvi_m1
	defptr bench_vslidedownvim_m1

	defptr bench_vredsumvs_m1
	defptr bench_vredsumvsm_m1
	defptr bench_vredandvs_m1
	defptr bench_vredandvsm_m1
	defptr bench_vredorvs_m1
	defptr bench_vredorvsm_m1
	defptr bench_vredxorvs_m1
	defptr bench_vredxorvsm_m1
	defptr bench_vredminuvs_m1
	defptr bench_vredminuvsm_m1
	defptr bench_vredminvs_m1
	defptr bench_vredminvsm_m1
	defptr bench_vredmaxuvs_m1
	defptr bench_vredmaxuvsm_m1
	defptr bench_vredmaxvs_m1
	defptr bench_vredmaxvsm_m1

	defptr bench_vaadduvv_m1
	defptr bench_vaadduvvm_m1
	defptr bench_vaadduvx_m1
	defptr bench_vaadduvxm_m1
	defptr bench_vaaddvv_m1
	defptr bench_vaaddvvm_m1
	defptr bench_vaaddvx_m1
	defptr bench_vaaddvxm_m1
	defptr bench_vasubuvv_m1
	defptr bench_vasubuvvm_m1
	defptr bench_vasubuvx_m1
	defptr bench_vasubuvxm_m1
	defptr bench_vasubvv_m1
	defptr bench_vasubvvm_m1
	defptr bench_vasubvx_m1
	defptr bench_vasubvxm_m1

	defptr bench_vslide1upvx_m1
	defptr bench_vslide1upvxm_m1
	defptr bench_vslide1downvx_m1
	defptr bench_vslide1downvxm_m1

	defptr bench_vadcvvm_m1
	 defptr bench_vadcvxm_m1
	 defptr bench_vadcvim_m1
	defptr bench_vmadcvvm_m1
	 defptr bench_vmadcvxm_m1
	 defptr bench_vmadcvim_m1
	defptr bench_vmadcvv_m1
	defptr bench_vmadcvx_m1
	defptr bench_vmadcvi_m1
	defptr bench_vsbcvvm_m1
	 defptr bench_vsbcvxm_m1
	defptr bench_vmsbcvvm_m1
	 defptr bench_vmsbcvxm_m1
	defptr bench_vmsbcvv_m1
	defptr bench_vmsbcvx_m1

	defptr bench_vmergevvm_m1
	 defptr bench_vmergevxm_m1
	 defptr bench_vmergevim_m1
	defptr bench_vmvvv_m1
	defptr bench_vmvvx_m1
	defptr bench_vmvvi_m1
	defptr bench_vmseqvv_m1
	defptr bench_vmseqvvm_m1
	defptr bench_vmseqvx_m1
	defptr bench_vmseqvxm_m1
	defptr bench_vmseqvi_m1
	defptr bench_vmseqvim_m1
	defptr bench_vmsnevv_m1
	defptr bench_vmsnevvm_m1
	defptr bench_vmsnevx_m1
	defptr bench_vmsnevxm_m1
	defptr bench_vmsnevi_m1
	defptr bench_vmsnevim_m1
	defptr bench_vmsltuvv_m1
	defptr bench_vmsltuvvm_m1
	defptr bench_vmsltuvx_m1
	defptr bench_vmsltuvxm_m1
	defptr bench_vmsltvv_m1
	defptr bench_vmsltvvm_m1
	defptr bench_vmsltvx_m1
	defptr bench_vmsltvxm_m1
	defptr bench_vmsleuvv_m1
	defptr bench_vmsleuvvm_m1
	defptr bench_vmsleuvx_m1
	defptr bench_vmsleuvxm_m1
	defptr bench_vmsleuvi_m1
	defptr bench_vmsleuvim_m1
	defptr bench_vmslevv_m1
	defptr bench_vmslevvm_m1
	defptr bench_vmslevx_m1
	defptr bench_vmslevxm_m1
	defptr bench_vmslevi_m1
	defptr bench_vmslevim_m1
	defptr bench_vmsgtuvx_m1
	defptr bench_vmsgtuvxm_m1
	defptr bench_vmsgtuvi_m1
	defptr bench_vmsgtuvim_m1
	defptr bench_vmsgtvx_m1
	defptr bench_vmsgtvxm_m1
	defptr bench_vmsgtvi_m1
	defptr bench_vmsgtvim_m1

	defptr bench_vcompressvm_m1

	defptr bench_vmandnmm_m1
	defptr bench_vmandmm_m1
	defptr bench_vmormm_m1
	defptr bench_vmxormm_m1
	defptr bench_vmornmm_m1
	defptr bench_vmnandmm_m1
	defptr bench_vmnormm_m1
	defptr bench_vmxnormm_m1

	defptr bench_vsadduvv_m1
	defptr bench_vsadduvvm_m1
	defptr bench_vsadduvx_m1
	defptr bench_vsadduvxm_m1
	defptr bench_vsadduvi_m1
	defptr bench_vsadduvim_m1
	defptr bench_vsaddvv_m1
	defptr bench_vsaddvvm_m1
	defptr bench_vsaddvx_m1
	defptr bench_vsaddvxm_m1
	defptr bench_vsaddvi_m1
	defptr bench_vsaddvim_m1
	defptr bench_vssubuvv_m1
	defptr bench_vssubuvvm_m1
	defptr bench_vssubuvx_m1
	defptr bench_vssubuvxm_m1
	defptr bench_vssubvv_m1
	defptr bench_vssubvvm_m1
	defptr bench_vssubvx_m1
	defptr bench_vssubvxm_m1
	defptr bench_vsllvv_m1
	defptr bench_vsllvvm_m1
	defptr bench_vsllvx_m1
	defptr bench_vsllvxm_m1
	defptr bench_vsllvi_m1
	defptr bench_vsllvim_m1
	defptr bench_vsmulvv_m1
	defptr bench_vsmulvvm_m1
	defptr bench_vsmulvx_m1
	defptr bench_vsmulvxm_m1
	defptr bench_vmv1rv_m1
	defptr bench_vmv2rv_m1
	defptr bench_vmv4rv_m1
	defptr bench_vmv8rv_m1
	defptr bench_vsrlvv_m1
	defptr bench_vsrlvvm_m1
	defptr bench_vsrlvx_m1
	defptr bench_vsrlvxm_m1
	defptr bench_vsrlvi_m1
	defptr bench_vsrlvim_m1
	defptr bench_vsravv_m1
	defptr bench_vsravvm_m1
	defptr bench_vsravx_m1
	defptr bench_vsravxm_m1
	defptr bench_vsravi_m1
	defptr bench_vsravim_m1
	defptr bench_vssrlvv_m1
	defptr bench_vssrlvvm_m1
	defptr bench_vssrlvx_m1
	defptr bench_vssrlvxm_m1
	defptr bench_vssrlvi_m1
	defptr bench_vssrlvim_m1

	defptr bench_vdivuvv_m1
	defptr bench_vdivuvvm_m1
	defptr bench_vdivuvx_m1
	defptr bench_vdivuvxm_m1
	defptr bench_vdivvv_m1
	defptr bench_vdivvvm_m1
	defptr bench_vdivvx_m1
	defptr bench_vdivvxm_m1
	defptr bench_vremuvv_m1
	defptr bench_vremuvvm_m1
	defptr bench_vremuvx_m1
	defptr bench_vremuvxm_m1
	defptr bench_vremvv_m1
	defptr bench_vremvvm_m1
	defptr bench_vremvx_m1
	defptr bench_vremvxm_m1
	defptr bench_vmulhuvv_m1
	defptr bench_vmulhuvvm_m1
	defptr bench_vmulhuvx_m1
	defptr bench_vmulhuvxm_m1
	defptr bench_vmulvv_m1
	defptr bench_vmulvvm_m1
	defptr bench_vmulvx_m1
	defptr bench_vmulvxm_m1
	defptr bench_vmulhsuvv_m1
	defptr bench_vmulhsuvvm_m1
	defptr bench_vmulhsuvx_m1
	defptr bench_vmulhsuvxm_m1
	defptr bench_vmulhvv_m1
	defptr bench_vmulhvvm_m1
	defptr bench_vmulhvx_m1
	defptr bench_vmulhvxm_m1
	defptr bench_vmaddvv_m1
	defptr bench_vmaddvvm_m1
	defptr bench_vmaddvx_m1
	defptr bench_vmaddvxm_m1
	defptr bench_vmaccvv_m1
	defptr bench_vmaccvvm_m1
	defptr bench_vmaccvx_m1
	defptr bench_vmaccvxm_m1

	defptr bench_vnsrlwv_m1
	defptr bench_vnsrlwvm_m1
	defptr bench_vnsrlwx_m1
	defptr bench_vnsrlwxm_m1
	defptr bench_vnsrlwi_m1
	defptr bench_vnsrlwim_m1
	defptr bench_vnsrawv_m1
	defptr bench_vnsrawvm_m1
	defptr bench_vnsrawx_m1
	defptr bench_vnsrawxm_m1
	defptr bench_vnsrawi_m1
	defptr bench_vnsrawim_m1
	defptr bench_vnclipuwv_m1
	defptr bench_vnclipuwvm_m1
	defptr bench_vnclipuwx_m1
	defptr bench_vnclipuwxm_m1
	defptr bench_vnclipuwi_m1
	defptr bench_vnclipuwim_m1
	defptr bench_vnclipwv_m1
	defptr bench_vnclipwvm_m1
	defptr bench_vnclipwx_m1
	defptr bench_vnclipwxm_m1
	defptr bench_vnclipwi_m1
	defptr bench_vnclipwim_m1
	defptr bench_vnmsubvv_m1
	defptr bench_vnmsubvvm_m1
	defptr bench_vnmsubvx_m1
	defptr bench_vnmsubvxm_m1
	defptr bench_vnmsacvv_m1
	defptr bench_vnmsacvvm_m1
	defptr bench_vnmsacvx_m1
	defptr bench_vnmsacvxm_m1

	defptr bench_vwadduvv_m1
	defptr bench_vwadduvvm_m1
	defptr bench_vwadduvx_m1
	defptr bench_vwadduvxm_m1
	defptr bench_vwaddvv_m1
	defptr bench_vwaddvvm_m1
	defptr bench_vwaddvx_m1
	defptr bench_vwaddvxm_m1
	defptr bench_vwsubuvv_m1
	defptr bench_vwsubuvvm_m1
	defptr bench_vwsubuvx_m1
	defptr bench_vwsubuvxm_m1
	defptr bench_vwsubvv_m1
	defptr bench_vwsubvvm_m1
	defptr bench_vwsubvx_m1
	defptr bench_vwsubvxm_m1
	defptr bench_vwadduwv_m1
	defptr bench_vwadduwvm_m1
	defptr bench_vwadduwx_m1
	defptr bench_vwadduwxm_m1
	defptr bench_vwaddwv_m1
	defptr bench_vwaddwvm_m1
	defptr bench_vwaddwx_m1
	defptr bench_vwaddwxm_m1
	defptr bench_vwsubuwv_m1
	defptr bench_vwsubuwvm_m1
	defptr bench_vwsubuwx_m1
	defptr bench_vwsubuwxm_m1
	defptr bench_vwsubwv_m1
	defptr bench_vwsubwvm_m1
	defptr bench_vwsubwx_m1
	defptr bench_vwsubwxm_m1
	defptr bench_vwmuluvv_m1
	defptr bench_vwmuluvvm_m1
	defptr bench_vwmuluvx_m1
	defptr bench_vwmuluvxm_m1
	defptr bench_vwmulsuvv_m1
	defptr bench_vwmulsuvvm_m1
	defptr bench_vwmulsuvx_m1
	defptr bench_vwmulsuvxm_m1
	defptr bench_vwmulvv_m1
	defptr bench_vwmulvvm_m1
	defptr bench_vwmulvx_m1
	defptr bench_vwmulvxm_m1
	defptr bench_vwmaccuvv_m1
	defptr bench_vwmaccuvvm_m1
	defptr bench_vwmaccuvx_m1
	defptr bench_vwmaccuvxm_m1
	defptr bench_vwmaccvv_m1
	defptr bench_vwmaccvvm_m1
	defptr bench_vwmaccvx_m1
	defptr bench_vwmaccvxm_m1
	defptr bench_vwmaccsuvv_m1
	defptr bench_vwmaccsuvvm_m1
	defptr bench_vwmaccsuvx_m1
	defptr bench_vwmaccsuvxm_m1
	defptr bench_vwmaccusvx_m1
	defptr bench_vwmaccusvxm_m1

	defptr bench_vfaddvv_m1
	defptr bench_vfaddvvm_m1
	defptr bench_vfaddvf_m1
	defptr bench_vfaddvfm_m1
	defptr bench_vfsubvv_m1
	defptr bench_vfsubvvm_m1
	defptr bench_vfsubvf_m1
	defptr bench_vfsubvfm_m1
	defptr bench_vfminvv_m1
	defptr bench_vfminvvm_m1
	defptr bench_vfminvf_m1
	defptr bench_vfminvfm_m1
	defptr bench_vfmaxvv_m1
	defptr bench_vfmaxvvm_m1
	defptr bench_vfmaxvf_m1
	defptr bench_vfmaxvfm_m1
	defptr bench_vfsgnjvv_m1
	defptr bench_vfsgnjvvm_m1
	defptr bench_vfsgnjvf_m1
	defptr bench_vfsgnjvfm_m1
	defptr bench_vfsgnjnvv_m1
	defptr bench_vfsgnjnvvm_m1
	defptr bench_vfsgnjnvf_m1
	defptr bench_vfsgnjnvfm_m1
	defptr bench_vfsgnjxvv_m1
	defptr bench_vfsgnjxvvm_m1
	defptr bench_vfsgnjxvf_m1
	defptr bench_vfsgnjxvfm_m1
	defptr bench_vfslide1upvf_m1
	defptr bench_vfslide1upvfm_m1
	defptr bench_vfslide1downvf_m1
	defptr bench_vfslide1downvfm_m1

	defptr bench_vfredusumvs_m1
	defptr bench_vfredusumvsm_m1
	defptr bench_vfredosumvs_m1
	defptr bench_vfredosumvsm_m1
	defptr bench_vfredminvs_m1
	defptr bench_vfredminvsm_m1
	defptr bench_vfredmaxvs_m1
	defptr bench_vfredmaxvsm_m1

	defptr bench_vfmergevfm_m1
	defptr bench_vfmvvf_m1

	defptr bench_vmfeqvv_m1
	defptr bench_vmfeqvvm_m1
	defptr bench_vmfeqvf_m1
	defptr bench_vmfeqvfm_m1
	defptr bench_vmflevv_m1
	defptr bench_vmflevvm_m1
	defptr bench_vmflevf_m1
	defptr bench_vmflevfm_m1
	defptr bench_vmfltvv_m1
	defptr bench_vmfltvvm_m1
	defptr bench_vmfltvf_m1
	defptr bench_vmfltvfm_m1
	defptr bench_vmfnevv_m1
	defptr bench_vmfnevvm_m1
	defptr bench_vmfnevf_m1
	defptr bench_vmfnevfm_m1
	defptr bench_vmfgtvv_m1
	defptr bench_vmfgtvvm_m1
	defptr bench_vmfgtvf_m1
	defptr bench_vmfgtvfm_m1
	defptr bench_vmfgevv_m1
	defptr bench_vmfgevvm_m1
	defptr bench_vmfgevf_m1
	defptr bench_vmfgevfm_m1

	defptr bench_vfdivvv_m1
	defptr bench_vfdivvvm_m1
	defptr bench_vfdivvf_m1
	defptr bench_vfdivvfm_m1
	defptr bench_vfrdivvf_m1
	defptr bench_vfrdivvfm_m1
	defptr bench_vfmulvv_m1
	defptr bench_vfmulvvm_m1
	defptr bench_vfmulvf_m1
	defptr bench_vfmulvfm_m1
	defptr bench_vfrsubvf_m1
	defptr bench_vfrsubvfm_m1
	defptr bench_vfmaddvv_m1
	defptr bench_vfmaddvvm_m1
	defptr bench_vfmaddvf_m1
	defptr bench_vfmaddvfm_m1
	defptr bench_vfmsubvv_m1
	defptr bench_vfmsubvvm_m1
	defptr bench_vfmsubvf_m1
	defptr bench_vfmsubvfm_m1
	defptr bench_vfmaccvv_m1
	defptr bench_vfmaccvvm_m1
	defptr bench_vfmaccvf_m1
	defptr bench_vfmaccvfm_m1
	defptr bench_vfmsacvv_m1
	defptr bench_vfmsacvvm_m1
	defptr bench_vfmsacvf_m1
	defptr bench_vfmsacvfm_m1

	defptr bench_vfnmsacvv_m1
	defptr bench_vfnmsacvvm_m1
	defptr bench_vfnmsacvf_m1
	defptr bench_vfnmsacvfm_m1
	defptr bench_vfnmaccvv_m1
	defptr bench_vfnmaccvvm_m1
	defptr bench_vfnmaccvf_m1
	defptr bench_vfnmaccvfm_m1
	defptr bench_vfnmsubvv_m1
	defptr bench_vfnmsubvvm_m1
	defptr bench_vfnmsubvf_m1
	defptr bench_vfnmsubvfm_m1
	defptr bench_vfnmaddvv_m1
	defptr bench_vfnmaddvvm_m1
	defptr bench_vfnmaddvf_m1
	defptr bench_vfnmaddvfm_m1

	defptr bench_vwredsumuvs_m1
	defptr bench_vwredsumuvsm_m1
	defptr bench_vwredsumvs_m1
	defptr bench_vwredsumvsm_m1

	defptr bench_vfwaddvv_m1
	defptr bench_vfwaddvvm_m1
	defptr bench_vfwaddvf_m1
	defptr bench_vfwaddvfm_m1
	defptr bench_vfwsubvv_m1
	defptr bench_vfwsubvvm_m1
	defptr bench_vfwsubvf_m1
	defptr bench_vfwsubvfm_m1
	defptr bench_vfwaddwv_m1
	defptr bench_vfwaddwvm_m1
	defptr bench_vfwaddwf_m1
	defptr bench_vfwaddwfm_m1
	defptr bench_vfwsubwv_m1
	defptr bench_vfwsubwvm_m1
	defptr bench_vfwsubwf_m1
	defptr bench_vfwsubwfm_m1
	defptr bench_vfwmulvv_m1
	defptr bench_vfwmulvvm_m1
	defptr bench_vfwmulvf_m1
	defptr bench_vfwmulvfm_m1
	defptr bench_vfwmaccvv_m1
	defptr bench_vfwmaccvvm_m1
	defptr bench_vfwmaccvf_m1
	defptr bench_vfwmaccvfm_m1
	defptr bench_vfwnmaccvv_m1
	defptr bench_vfwnmaccvvm_m1
	defptr bench_vfwnmaccvf_m1
	defptr bench_vfwnmaccvfm_m1
	defptr bench_vfwmsacvv_m1
	defptr bench_vfwmsacvvm_m1
	defptr bench_vfwmsacvf_m1
	defptr bench_vfwmsacvfm_m1
	defptr bench_vfwnmsacvv_m1
	defptr bench_vfwnmsacvvm_m1
	defptr bench_vfwnmsacvf_m1
	defptr bench_vfwnmsacvfm_m1

	defptr bench_vfwredosumvs_m1
	defptr bench_vfwredosumvsm_m1
	defptr bench_vfwredusumvs_m1
	defptr bench_vfwredusumvsm_m1

	defptr bench_vmvsx_m1
	defptr bench_vmvxs_m1

	defptr bench_vcpopm_m1
	defptr bench_vcpopmm_m1
	defptr bench_vfirstm_m1
	defptr bench_vfirstmm_m1
	defptr bench_vzextvf2_m1
	defptr bench_vzextvf2m_m1
	defptr bench_vsextvf2_m1
	defptr bench_vsextvf2m_m1
	defptr bench_vzextvf4_m1
	defptr bench_vzextvf4m_m1
	defptr bench_vsextvf4_m1
	defptr bench_vsextvf4m_m1
	defptr bench_vzextvf8_m1
	defptr bench_vzextvf8m_m1
	defptr bench_vsextvf8_m1
	defptr bench_vsextvf8m_m1

	defptr bench_vfmvfs_m1
	defptr bench_vfmvsf_m1

	defptr bench_vfcvtxufv_m1
	defptr bench_vfcvtxufvm_m1
	defptr bench_vfcvtxfv_m1
	defptr bench_vfcvtxfvm_m1
	defptr bench_vfcvtfxuv_m1
	defptr bench_vfcvtfxuvm_m1
	defptr bench_vfcvtfxv_m1
	defptr bench_vfcvtfxvm_m1
	defptr bench_vfcvtrtzxfv_m1
	defptr bench_vfcvtrtzxfvm_m1
	defptr bench_vfcvtrtzxufv_m1
	defptr bench_vfcvtrtzxufvm_m1

	defptr bench_vfwcvtxufv_m1
	defptr bench_vfwcvtxufvm_m1
	defptr bench_vfwcvtxfv_m1
	defptr bench_vfwcvtxfvm_m1
	defptr bench_vfwcvtfxuv_m1
	defptr bench_vfwcvtfxuvm_m1
	defptr bench_vfwcvtfxv_m1
	defptr bench_vfwcvtfxvm_m1
	defptr bench_vfwcvtffv_m1
	defptr bench_vfwcvtffvm_m1
	defptr bench_vfwcvtrtzxufv_m1
	defptr bench_vfwcvtrtzxufvm_m1
	defptr bench_vfwcvtrtzxfv_m1
	defptr bench_vfwcvtrtzxfvm_m1

	defptr bench_vfncvtxufw_m1
	defptr bench_vfncvtxufwm_m1
	defptr bench_vfncvtxfw_m1
	defptr bench_vfncvtxfwm_m1
	defptr bench_vfncvtfxuw_m1
	defptr bench_vfncvtfxuwm_m1
	defptr bench_vfncvtfxw_m1
	defptr bench_vfncvtfxwm_m1
	defptr bench_vfncvtffw_m1
	defptr bench_vfncvtffwm_m1
	defptr bench_vfncvtrtzxfw_m1
	defptr bench_vfncvtrtzxfwm_m1
	defptr bench_vfncvtrtzxufw_m1
	defptr bench_vfncvtrtzxufwm_m1
	defptr bench_vfncvt.rod.f.f.w_m1
	defptr bench_vfncvt.rod.f.f.wm_m1

	defptr bench_vfsqrtv_m1
	defptr bench_vfsqrtvm_m1
	defptr bench_vfrsqrt7v_m1
	defptr bench_vfrsqrt7vm_m1
	defptr bench_vfrec7v_m1
	defptr bench_vfrec7vm_m1
	defptr bench_vfclassv_m1
	defptr bench_vfclassvm_m1

	defptr bench_vmsbfm_m1
	defptr bench_vmsbfmm_m1
	defptr bench_vmsofm_m1
	defptr bench_vmsofmm_m1
	defptr bench_vmsifm_m1
	defptr bench_vmsifmm_m1
	defptr bench_viotam_m1
	defptr bench_viotamm_m1
	defptr bench_vidv_m1
	defptr bench_vidvm_m1



.balign 8


.global bench_mf2
	bench_mf2:

	defptr bench_add_m1
	defptr bench_mul_m1
	defptr bench_vaddvv_m1
	defptr bench_vaddvvm_m1
	defptr bench_vaddvx_m1
	defptr bench_vaddvxm_m1
	defptr bench_vaddvi_m1
	defptr bench_vaddvim_m1
	defptr bench_vsubvv_m1
	defptr bench_vsubvvm_m1
	defptr bench_vsubvx_m1
	defptr bench_vsubvxm_m1
	defptr bench_vrsubvx_m1
	defptr bench_vrsubvxm_m1
	defptr bench_vrsubvi_m1
	defptr bench_vrsubvim_m1
	defptr bench_vminuvv_m1
	defptr bench_vminuvvm_m1
	defptr bench_vminuvx_m1
	defptr bench_vminuvxm_m1
	defptr bench_vminvv_m1
	defptr bench_vminvvm_m1
	defptr bench_vminvx_m1
	defptr bench_vminvxm_m1
	defptr bench_vmaxuvv_m1
	defptr bench_vmaxuvvm_m1
	defptr bench_vmaxuvx_m1
	defptr bench_vmaxuvxm_m1
	defptr bench_vmaxvv_m1
	defptr bench_vmaxvvm_m1
	defptr bench_vmaxvx_m1
	defptr bench_vmaxvxm_m1
	defptr bench_vandvv_m1
	defptr bench_vandvvm_m1
	defptr bench_vandvx_m1
	defptr bench_vandvxm_m1
	defptr bench_vandvi_m1
	defptr bench_vandvim_m1
	defptr bench_vorvv_m1
	defptr bench_vorvvm_m1
	defptr bench_vorvx_m1
	defptr bench_vorvxm_m1
	defptr bench_vorvi_m1
	defptr bench_vorvim_m1
	defptr bench_vxorvv_m1
	defptr bench_vxorvvm_m1
	defptr bench_vxorvx_m1
	defptr bench_vxorvxm_m1
	defptr bench_vxorvi_m1
	defptr bench_vxorvim_m1

	defptr bench_vrgathervv_m1
	defptr bench_vrgathervvm_m1
	defptr bench_vrgathervx_m1
	defptr bench_vrgathervxm_m1
	defptr bench_vrgathervi_m1
	defptr bench_vrgathervim_m1
	defptr bench_vslideupvx_m1
	defptr bench_vslideupvxm_m1
	defptr bench_vslideupvi_m1
	defptr bench_vslideupvim_m1
	defptr bench_vrgatherei16vv_m1
	defptr bench_vrgatherei16vvm_m1

	defptr bench_vslidedownvx_m1
	defptr bench_vslidedownvxm_m1
	defptr bench_vslidedownvi_m1
	defptr bench_vslidedownvim_m1

	defptr bench_vredsumvs_m1
	defptr bench_vredsumvsm_m1
	defptr bench_vredandvs_m1
	defptr bench_vredandvsm_m1
	defptr bench_vredorvs_m1
	defptr bench_vredorvsm_m1
	defptr bench_vredxorvs_m1
	defptr bench_vredxorvsm_m1
	defptr bench_vredminuvs_m1
	defptr bench_vredminuvsm_m1
	defptr bench_vredminvs_m1
	defptr bench_vredminvsm_m1
	defptr bench_vredmaxuvs_m1
	defptr bench_vredmaxuvsm_m1
	defptr bench_vredmaxvs_m1
	defptr bench_vredmaxvsm_m1

	defptr bench_vaadduvv_m1
	defptr bench_vaadduvvm_m1
	defptr bench_vaadduvx_m1
	defptr bench_vaadduvxm_m1
	defptr bench_vaaddvv_m1
	defptr bench_vaaddvvm_m1
	defptr bench_vaaddvx_m1
	defptr bench_vaaddvxm_m1
	defptr bench_vasubuvv_m1
	defptr bench_vasubuvvm_m1
	defptr bench_vasubuvx_m1
	defptr bench_vasubuvxm_m1
	defptr bench_vasubvv_m1
	defptr bench_vasubvvm_m1
	defptr bench_vasubvx_m1
	defptr bench_vasubvxm_m1

	defptr bench_vslide1upvx_m1
	defptr bench_vslide1upvxm_m1
	defptr bench_vslide1downvx_m1
	defptr bench_vslide1downvxm_m1

	defptr bench_vadcvvm_m1
	 defptr bench_vadcvxm_m1
	 defptr bench_vadcvim_m1
	defptr bench_vmadcvvm_m1
	 defptr bench_vmadcvxm_m1
	 defptr bench_vmadcvim_m1
	defptr bench_vmadcvv_m1
	defptr bench_vmadcvx_m1
	defptr bench_vmadcvi_m1
	defptr bench_vsbcvvm_m1
	 defptr bench_vsbcvxm_m1
	defptr bench_vmsbcvvm_m1
	 defptr bench_vmsbcvxm_m1
	defptr bench_vmsbcvv_m1
	defptr bench_vmsbcvx_m1

	defptr bench_vmergevvm_m1
	 defptr bench_vmergevxm_m1
	 defptr bench_vmergevim_m1
	defptr bench_vmvvv_m1
	defptr bench_vmvvx_m1
	defptr bench_vmvvi_m1
	defptr bench_vmseqvv_m1
	defptr bench_vmseqvvm_m1
	defptr bench_vmseqvx_m1
	defptr bench_vmseqvxm_m1
	defptr bench_vmseqvi_m1
	defptr bench_vmseqvim_m1
	defptr bench_vmsnevv_m1
	defptr bench_vmsnevvm_m1
	defptr bench_vmsnevx_m1
	defptr bench_vmsnevxm_m1
	defptr bench_vmsnevi_m1
	defptr bench_vmsnevim_m1
	defptr bench_vmsltuvv_m1
	defptr bench_vmsltuvvm_m1
	defptr bench_vmsltuvx_m1
	defptr bench_vmsltuvxm_m1
	defptr bench_vmsltvv_m1
	defptr bench_vmsltvvm_m1
	defptr bench_vmsltvx_m1
	defptr bench_vmsltvxm_m1
	defptr bench_vmsleuvv_m1
	defptr bench_vmsleuvvm_m1
	defptr bench_vmsleuvx_m1
	defptr bench_vmsleuvxm_m1
	defptr bench_vmsleuvi_m1
	defptr bench_vmsleuvim_m1
	defptr bench_vmslevv_m1
	defptr bench_vmslevvm_m1
	defptr bench_vmslevx_m1
	defptr bench_vmslevxm_m1
	defptr bench_vmslevi_m1
	defptr bench_vmslevim_m1
	defptr bench_vmsgtuvx_m1
	defptr bench_vmsgtuvxm_m1
	defptr bench_vmsgtuvi_m1
	defptr bench_vmsgtuvim_m1
	defptr bench_vmsgtvx_m1
	defptr bench_vmsgtvxm_m1
	defptr bench_vmsgtvi_m1
	defptr bench_vmsgtvim_m1

	defptr bench_vcompressvm_m1

	defptr bench_vmandnmm_m1
	defptr bench_vmandmm_m1
	defptr bench_vmormm_m1
	defptr bench_vmxormm_m1
	defptr bench_vmornmm_m1
	defptr bench_vmnandmm_m1
	defptr bench_vmnormm_m1
	defptr bench_vmxnormm_m1

	defptr bench_vsadduvv_m1
	defptr bench_vsadduvvm_m1
	defptr bench_vsadduvx_m1
	defptr bench_vsadduvxm_m1
	defptr bench_vsadduvi_m1
	defptr bench_vsadduvim_m1
	defptr bench_vsaddvv_m1
	defptr bench_vsaddvvm_m1
	defptr bench_vsaddvx_m1
	defptr bench_vsaddvxm_m1
	defptr bench_vsaddvi_m1
	defptr bench_vsaddvim_m1
	defptr bench_vssubuvv_m1
	defptr bench_vssubuvvm_m1
	defptr bench_vssubuvx_m1
	defptr bench_vssubuvxm_m1
	defptr bench_vssubvv_m1
	defptr bench_vssubvvm_m1
	defptr bench_vssubvx_m1
	defptr bench_vssubvxm_m1
	defptr bench_vsllvv_m1
	defptr bench_vsllvvm_m1
	defptr bench_vsllvx_m1
	defptr bench_vsllvxm_m1
	defptr bench_vsllvi_m1
	defptr bench_vsllvim_m1
	defptr bench_vsmulvv_m1
	defptr bench_vsmulvvm_m1
	defptr bench_vsmulvx_m1
	defptr bench_vsmulvxm_m1
	defptr bench_vmv1rv_m1
	defptr bench_vmv2rv_m1
	defptr bench_vmv4rv_m1
	defptr bench_vmv8rv_m1
	defptr bench_vsrlvv_m1
	defptr bench_vsrlvvm_m1
	defptr bench_vsrlvx_m1
	defptr bench_vsrlvxm_m1
	defptr bench_vsrlvi_m1
	defptr bench_vsrlvim_m1
	defptr bench_vsravv_m1
	defptr bench_vsravvm_m1
	defptr bench_vsravx_m1
	defptr bench_vsravxm_m1
	defptr bench_vsravi_m1
	defptr bench_vsravim_m1
	defptr bench_vssrlvv_m1
	defptr bench_vssrlvvm_m1
	defptr bench_vssrlvx_m1
	defptr bench_vssrlvxm_m1
	defptr bench_vssrlvi_m1
	defptr bench_vssrlvim_m1

	defptr bench_vdivuvv_m1
	defptr bench_vdivuvvm_m1
	defptr bench_vdivuvx_m1
	defptr bench_vdivuvxm_m1
	defptr bench_vdivvv_m1
	defptr bench_vdivvvm_m1
	defptr bench_vdivvx_m1
	defptr bench_vdivvxm_m1
	defptr bench_vremuvv_m1
	defptr bench_vremuvvm_m1
	defptr bench_vremuvx_m1
	defptr bench_vremuvxm_m1
	defptr bench_vremvv_m1
	defptr bench_vremvvm_m1
	defptr bench_vremvx_m1
	defptr bench_vremvxm_m1
	defptr bench_vmulhuvv_m1
	defptr bench_vmulhuvvm_m1
	defptr bench_vmulhuvx_m1
	defptr bench_vmulhuvxm_m1
	defptr bench_vmulvv_m1
	defptr bench_vmulvvm_m1
	defptr bench_vmulvx_m1
	defptr bench_vmulvxm_m1
	defptr bench_vmulhsuvv_m1
	defptr bench_vmulhsuvvm_m1
	defptr bench_vmulhsuvx_m1
	defptr bench_vmulhsuvxm_m1
	defptr bench_vmulhvv_m1
	defptr bench_vmulhvvm_m1
	defptr bench_vmulhvx_m1
	defptr bench_vmulhvxm_m1
	defptr bench_vmaddvv_m1
	defptr bench_vmaddvvm_m1
	defptr bench_vmaddvx_m1
	defptr bench_vmaddvxm_m1
	defptr bench_vmaccvv_m1
	defptr bench_vmaccvvm_m1
	defptr bench_vmaccvx_m1
	defptr bench_vmaccvxm_m1

	defptr bench_vnsrlwv_m1
	defptr bench_vnsrlwvm_m1
	defptr bench_vnsrlwx_m1
	defptr bench_vnsrlwxm_m1
	defptr bench_vnsrlwi_m1
	defptr bench_vnsrlwim_m1
	defptr bench_vnsrawv_m1
	defptr bench_vnsrawvm_m1
	defptr bench_vnsrawx_m1
	defptr bench_vnsrawxm_m1
	defptr bench_vnsrawi_m1
	defptr bench_vnsrawim_m1
	defptr bench_vnclipuwv_m1
	defptr bench_vnclipuwvm_m1
	defptr bench_vnclipuwx_m1
	defptr bench_vnclipuwxm_m1
	defptr bench_vnclipuwi_m1
	defptr bench_vnclipuwim_m1
	defptr bench_vnclipwv_m1
	defptr bench_vnclipwvm_m1
	defptr bench_vnclipwx_m1
	defptr bench_vnclipwxm_m1
	defptr bench_vnclipwi_m1
	defptr bench_vnclipwim_m1
	defptr bench_vnmsubvv_m1
	defptr bench_vnmsubvvm_m1
	defptr bench_vnmsubvx_m1
	defptr bench_vnmsubvxm_m1
	defptr bench_vnmsacvv_m1
	defptr bench_vnmsacvvm_m1
	defptr bench_vnmsacvx_m1
	defptr bench_vnmsacvxm_m1

	defptr bench_vwadduvv_m1
	defptr bench_vwadduvvm_m1
	defptr bench_vwadduvx_m1
	defptr bench_vwadduvxm_m1
	defptr bench_vwaddvv_m1
	defptr bench_vwaddvvm_m1
	defptr bench_vwaddvx_m1
	defptr bench_vwaddvxm_m1
	defptr bench_vwsubuvv_m1
	defptr bench_vwsubuvvm_m1
	defptr bench_vwsubuvx_m1
	defptr bench_vwsubuvxm_m1
	defptr bench_vwsubvv_m1
	defptr bench_vwsubvvm_m1
	defptr bench_vwsubvx_m1
	defptr bench_vwsubvxm_m1
	defptr bench_vwadduwv_m1
	defptr bench_vwadduwvm_m1
	defptr bench_vwadduwx_m1
	defptr bench_vwadduwxm_m1
	defptr bench_vwaddwv_m1
	defptr bench_vwaddwvm_m1
	defptr bench_vwaddwx_m1
	defptr bench_vwaddwxm_m1
	defptr bench_vwsubuwv_m1
	defptr bench_vwsubuwvm_m1
	defptr bench_vwsubuwx_m1
	defptr bench_vwsubuwxm_m1
	defptr bench_vwsubwv_m1
	defptr bench_vwsubwvm_m1
	defptr bench_vwsubwx_m1
	defptr bench_vwsubwxm_m1
	defptr bench_vwmuluvv_m1
	defptr bench_vwmuluvvm_m1
	defptr bench_vwmuluvx_m1
	defptr bench_vwmuluvxm_m1
	defptr bench_vwmulsuvv_m1
	defptr bench_vwmulsuvvm_m1
	defptr bench_vwmulsuvx_m1
	defptr bench_vwmulsuvxm_m1
	defptr bench_vwmulvv_m1
	defptr bench_vwmulvvm_m1
	defptr bench_vwmulvx_m1
	defptr bench_vwmulvxm_m1
	defptr bench_vwmaccuvv_m1
	defptr bench_vwmaccuvvm_m1
	defptr bench_vwmaccuvx_m1
	defptr bench_vwmaccuvxm_m1
	defptr bench_vwmaccvv_m1
	defptr bench_vwmaccvvm_m1
	defptr bench_vwmaccvx_m1
	defptr bench_vwmaccvxm_m1
	defptr bench_vwmaccsuvv_m1
	defptr bench_vwmaccsuvvm_m1
	defptr bench_vwmaccsuvx_m1
	defptr bench_vwmaccsuvxm_m1
	defptr bench_vwmaccusvx_m1
	defptr bench_vwmaccusvxm_m1

	defptr bench_vfaddvv_m1
	defptr bench_vfaddvvm_m1
	defptr bench_vfaddvf_m1
	defptr bench_vfaddvfm_m1
	defptr bench_vfsubvv_m1
	defptr bench_vfsubvvm_m1
	defptr bench_vfsubvf_m1
	defptr bench_vfsubvfm_m1
	defptr bench_vfminvv_m1
	defptr bench_vfminvvm_m1
	defptr bench_vfminvf_m1
	defptr bench_vfminvfm_m1
	defptr bench_vfmaxvv_m1
	defptr bench_vfmaxvvm_m1
	defptr bench_vfmaxvf_m1
	defptr bench_vfmaxvfm_m1
	defptr bench_vfsgnjvv_m1
	defptr bench_vfsgnjvvm_m1
	defptr bench_vfsgnjvf_m1
	defptr bench_vfsgnjvfm_m1
	defptr bench_vfsgnjnvv_m1
	defptr bench_vfsgnjnvvm_m1
	defptr bench_vfsgnjnvf_m1
	defptr bench_vfsgnjnvfm_m1
	defptr bench_vfsgnjxvv_m1
	defptr bench_vfsgnjxvvm_m1
	defptr bench_vfsgnjxvf_m1
	defptr bench_vfsgnjxvfm_m1
	defptr bench_vfslide1upvf_m1
	defptr bench_vfslide1upvfm_m1
	defptr bench_vfslide1downvf_m1
	defptr bench_vfslide1downvfm_m1

	defptr bench_vfredusumvs_m1
	defptr bench_vfredusumvsm_m1
	defptr bench_vfredosumvs_m1
	defptr bench_vfredosumvsm_m1
	defptr bench_vfredminvs_m1
	defptr bench_vfredminvsm_m1
	defptr bench_vfredmaxvs_m1
	defptr bench_vfredmaxvsm_m1

	defptr bench_vfmergevfm_m1
	defptr bench_vfmvvf_m1

	defptr bench_vmfeqvv_m1
	defptr bench_vmfeqvvm_m1
	defptr bench_vmfeqvf_m1
	defptr bench_vmfeqvfm_m1
	defptr bench_vmflevv_m1
	defptr bench_vmflevvm_m1
	defptr bench_vmflevf_m1
	defptr bench_vmflevfm_m1
	defptr bench_vmfltvv_m1
	defptr bench_vmfltvvm_m1
	defptr bench_vmfltvf_m1
	defptr bench_vmfltvfm_m1
	defptr bench_vmfnevv_m1
	defptr bench_vmfnevvm_m1
	defptr bench_vmfnevf_m1
	defptr bench_vmfnevfm_m1
	defptr bench_vmfgtvv_m1
	defptr bench_vmfgtvvm_m1
	defptr bench_vmfgtvf_m1
	defptr bench_vmfgtvfm_m1
	defptr bench_vmfgevv_m1
	defptr bench_vmfgevvm_m1
	defptr bench_vmfgevf_m1
	defptr bench_vmfgevfm_m1

	defptr bench_vfdivvv_m1
	defptr bench_vfdivvvm_m1
	defptr bench_vfdivvf_m1
	defptr bench_vfdivvfm_m1
	defptr bench_vfrdivvf_m1
	defptr bench_vfrdivvfm_m1
	defptr bench_vfmulvv_m1
	defptr bench_vfmulvvm_m1
	defptr bench_vfmulvf_m1
	defptr bench_vfmulvfm_m1
	defptr bench_vfrsubvf_m1
	defptr bench_vfrsubvfm_m1
	defptr bench_vfmaddvv_m1
	defptr bench_vfmaddvvm_m1
	defptr bench_vfmaddvf_m1
	defptr bench_vfmaddvfm_m1
	defptr bench_vfmsubvv_m1
	defptr bench_vfmsubvvm_m1
	defptr bench_vfmsubvf_m1
	defptr bench_vfmsubvfm_m1
	defptr bench_vfmaccvv_m1
	defptr bench_vfmaccvvm_m1
	defptr bench_vfmaccvf_m1
	defptr bench_vfmaccvfm_m1
	defptr bench_vfmsacvv_m1
	defptr bench_vfmsacvvm_m1
	defptr bench_vfmsacvf_m1
	defptr bench_vfmsacvfm_m1

	defptr bench_vfnmsacvv_m1
	defptr bench_vfnmsacvvm_m1
	defptr bench_vfnmsacvf_m1
	defptr bench_vfnmsacvfm_m1
	defptr bench_vfnmaccvv_m1
	defptr bench_vfnmaccvvm_m1
	defptr bench_vfnmaccvf_m1
	defptr bench_vfnmaccvfm_m1
	defptr bench_vfnmsubvv_m1
	defptr bench_vfnmsubvvm_m1
	defptr bench_vfnmsubvf_m1
	defptr bench_vfnmsubvfm_m1
	defptr bench_vfnmaddvv_m1
	defptr bench_vfnmaddvvm_m1
	defptr bench_vfnmaddvf_m1
	defptr bench_vfnmaddvfm_m1

	defptr bench_vwredsumuvs_m1
	defptr bench_vwredsumuvsm_m1
	defptr bench_vwredsumvs_m1
	defptr bench_vwredsumvsm_m1

	defptr bench_vfwaddvv_m1
	defptr bench_vfwaddvvm_m1
	defptr bench_vfwaddvf_m1
	defptr bench_vfwaddvfm_m1
	defptr bench_vfwsubvv_m1
	defptr bench_vfwsubvvm_m1
	defptr bench_vfwsubvf_m1
	defptr bench_vfwsubvfm_m1
	defptr bench_vfwaddwv_m1
	defptr bench_vfwaddwvm_m1
	defptr bench_vfwaddwf_m1
	defptr bench_vfwaddwfm_m1
	defptr bench_vfwsubwv_m1
	defptr bench_vfwsubwvm_m1
	defptr bench_vfwsubwf_m1
	defptr bench_vfwsubwfm_m1
	defptr bench_vfwmulvv_m1
	defptr bench_vfwmulvvm_m1
	defptr bench_vfwmulvf_m1
	defptr bench_vfwmulvfm_m1
	defptr bench_vfwmaccvv_m1
	defptr bench_vfwmaccvvm_m1
	defptr bench_vfwmaccvf_m1
	defptr bench_vfwmaccvfm_m1
	defptr bench_vfwnmaccvv_m1
	defptr bench_vfwnmaccvvm_m1
	defptr bench_vfwnmaccvf_m1
	defptr bench_vfwnmaccvfm_m1
	defptr bench_vfwmsacvv_m1
	defptr bench_vfwmsacvvm_m1
	defptr bench_vfwmsacvf_m1
	defptr bench_vfwmsacvfm_m1
	defptr bench_vfwnmsacvv_m1
	defptr bench_vfwnmsacvvm_m1
	defptr bench_vfwnmsacvf_m1
	defptr bench_vfwnmsacvfm_m1

	defptr bench_vfwredosumvs_m1
	defptr bench_vfwredosumvsm_m1
	defptr bench_vfwredusumvs_m1
	defptr bench_vfwredusumvsm_m1

	defptr bench_vmvsx_m1
	defptr bench_vmvxs_m1

	defptr bench_vcpopm_m1
	defptr bench_vcpopmm_m1
	defptr bench_vfirstm_m1
	defptr bench_vfirstmm_m1
	defptr bench_vzextvf2_m1
	defptr bench_vzextvf2m_m1
	defptr bench_vsextvf2_m1
	defptr bench_vsextvf2m_m1
	defptr bench_vzextvf4_m1
	defptr bench_vzextvf4m_m1
	defptr bench_vsextvf4_m1
	defptr bench_vsextvf4m_m1
	defptr bench_vzextvf8_m1
	defptr bench_vzextvf8m_m1
	defptr bench_vsextvf8_m1
	defptr bench_vsextvf8m_m1

	defptr bench_vfmvfs_m1
	defptr bench_vfmvsf_m1

	defptr bench_vfcvtxufv_m1
	defptr bench_vfcvtxufvm_m1
	defptr bench_vfcvtxfv_m1
	defptr bench_vfcvtxfvm_m1
	defptr bench_vfcvtfxuv_m1
	defptr bench_vfcvtfxuvm_m1
	defptr bench_vfcvtfxv_m1
	defptr bench_vfcvtfxvm_m1
	defptr bench_vfcvtrtzxfv_m1
	defptr bench_vfcvtrtzxfvm_m1
	defptr bench_vfcvtrtzxufv_m1
	defptr bench_vfcvtrtzxufvm_m1

	defptr bench_vfwcvtxufv_m1
	defptr bench_vfwcvtxufvm_m1
	defptr bench_vfwcvtxfv_m1
	defptr bench_vfwcvtxfvm_m1
	defptr bench_vfwcvtfxuv_m1
	defptr bench_vfwcvtfxuvm_m1
	defptr bench_vfwcvtfxv_m1
	defptr bench_vfwcvtfxvm_m1
	defptr bench_vfwcvtffv_m1
	defptr bench_vfwcvtffvm_m1
	defptr bench_vfwcvtrtzxufv_m1
	defptr bench_vfwcvtrtzxufvm_m1
	defptr bench_vfwcvtrtzxfv_m1
	defptr bench_vfwcvtrtzxfvm_m1

	defptr bench_vfncvtxufw_m1
	defptr bench_vfncvtxufwm_m1
	defptr bench_vfncvtxfw_m1
	defptr bench_vfncvtxfwm_m1
	defptr bench_vfncvtfxuw_m1
	defptr bench_vfncvtfxuwm_m1
	defptr bench_vfncvtfxw_m1
	defptr bench_vfncvtfxwm_m1
	defptr bench_vfncvtffw_m1
	defptr bench_vfncvtffwm_m1
	defptr bench_vfncvtrtzxfw_m1
	defptr bench_vfncvtrtzxfwm_m1
	defptr bench_vfncvtrtzxufw_m1
	defptr bench_vfncvtrtzxufwm_m1
	defptr bench_vfncvt.rod.f.f.w_m1
	defptr bench_vfncvt.rod.f.f.wm_m1

	defptr bench_vfsqrtv_m1
	defptr bench_vfsqrtvm_m1
	defptr bench_vfrsqrt7v_m1
	defptr bench_vfrsqrt7vm_m1
	defptr bench_vfrec7v_m1
	defptr bench_vfrec7vm_m1
	defptr bench_vfclassv_m1
	defptr bench_vfclassvm_m1

	defptr bench_vmsbfm_m1
	defptr bench_vmsbfmm_m1
	defptr bench_vmsofm_m1
	defptr bench_vmsofmm_m1
	defptr bench_vmsifm_m1
	defptr bench_vmsifmm_m1
	defptr bench_viotam_m1
	defptr bench_viotamm_m1
	defptr bench_vidv_m1
	defptr bench_vidvm_m1



.balign 8


.global bench_m1
	bench_m1:

	defptr bench_add_m1
	defptr bench_mul_m1
	defptr bench_vaddvv_m1
	defptr bench_vaddvvm_m1
	defptr bench_vaddvx_m1
	defptr bench_vaddvxm_m1
	defptr bench_vaddvi_m1
	defptr bench_vaddvim_m1
	defptr bench_vsubvv_m1
	defptr bench_vsubvvm_m1
	defptr bench_vsubvx_m1
	defptr bench_vsubvxm_m1
	defptr bench_vrsubvx_m1
	defptr bench_vrsubvxm_m1
	defptr bench_vrsubvi_m1
	defptr bench_vrsubvim_m1
	defptr bench_vminuvv_m1
	defptr bench_vminuvvm_m1
	defptr bench_vminuvx_m1
	defptr bench_vminuvxm_m1
	defptr bench_vminvv_m1
	defptr bench_vminvvm_m1
	defptr bench_vminvx_m1
	defptr bench_vminvxm_m1
	defptr bench_vmaxuvv_m1
	defptr bench_vmaxuvvm_m1
	defptr bench_vmaxuvx_m1
	defptr bench_vmaxuvxm_m1
	defptr bench_vmaxvv_m1
	defptr bench_vmaxvvm_m1
	defptr bench_vmaxvx_m1
	defptr bench_vmaxvxm_m1
	defptr bench_vandvv_m1
	defptr bench_vandvvm_m1
	defptr bench_vandvx_m1
	defptr bench_vandvxm_m1
	defptr bench_vandvi_m1
	defptr bench_vandvim_m1
	defptr bench_vorvv_m1
	defptr bench_vorvvm_m1
	defptr bench_vorvx_m1
	defptr bench_vorvxm_m1
	defptr bench_vorvi_m1
	defptr bench_vorvim_m1
	defptr bench_vxorvv_m1
	defptr bench_vxorvvm_m1
	defptr bench_vxorvx_m1
	defptr bench_vxorvxm_m1
	defptr bench_vxorvi_m1
	defptr bench_vxorvim_m1

	defptr bench_vrgathervv_m1
	defptr bench_vrgathervvm_m1
	defptr bench_vrgathervx_m1
	defptr bench_vrgathervxm_m1
	defptr bench_vrgathervi_m1
	defptr bench_vrgathervim_m1
	defptr bench_vslideupvx_m1
	defptr bench_vslideupvxm_m1
	defptr bench_vslideupvi_m1
	defptr bench_vslideupvim_m1
	defptr bench_vrgatherei16vv_m1
	defptr bench_vrgatherei16vvm_m1

	defptr bench_vslidedownvx_m1
	defptr bench_vslidedownvxm_m1
	defptr bench_vslidedownvi_m1
	defptr bench_vslidedownvim_m1

	defptr bench_vredsumvs_m1
	defptr bench_vredsumvsm_m1
	defptr bench_vredandvs_m1
	defptr bench_vredandvsm_m1
	defptr bench_vredorvs_m1
	defptr bench_vredorvsm_m1
	defptr bench_vredxorvs_m1
	defptr bench_vredxorvsm_m1
	defptr bench_vredminuvs_m1
	defptr bench_vredminuvsm_m1
	defptr bench_vredminvs_m1
	defptr bench_vredminvsm_m1
	defptr bench_vredmaxuvs_m1
	defptr bench_vredmaxuvsm_m1
	defptr bench_vredmaxvs_m1
	defptr bench_vredmaxvsm_m1

	defptr bench_vaadduvv_m1
	defptr bench_vaadduvvm_m1
	defptr bench_vaadduvx_m1
	defptr bench_vaadduvxm_m1
	defptr bench_vaaddvv_m1
	defptr bench_vaaddvvm_m1
	defptr bench_vaaddvx_m1
	defptr bench_vaaddvxm_m1
	defptr bench_vasubuvv_m1
	defptr bench_vasubuvvm_m1
	defptr bench_vasubuvx_m1
	defptr bench_vasubuvxm_m1
	defptr bench_vasubvv_m1
	defptr bench_vasubvvm_m1
	defptr bench_vasubvx_m1
	defptr bench_vasubvxm_m1

	defptr bench_vslide1upvx_m1
	defptr bench_vslide1upvxm_m1
	defptr bench_vslide1downvx_m1
	defptr bench_vslide1downvxm_m1

	defptr bench_vadcvvm_m1
	 defptr bench_vadcvxm_m1
	 defptr bench_vadcvim_m1
	defptr bench_vmadcvvm_m1
	 defptr bench_vmadcvxm_m1
	 defptr bench_vmadcvim_m1
	defptr bench_vmadcvv_m1
	defptr bench_vmadcvx_m1
	defptr bench_vmadcvi_m1
	defptr bench_vsbcvvm_m1
	 defptr bench_vsbcvxm_m1
	defptr bench_vmsbcvvm_m1
	 defptr bench_vmsbcvxm_m1
	defptr bench_vmsbcvv_m1
	defptr bench_vmsbcvx_m1

	defptr bench_vmergevvm_m1
	 defptr bench_vmergevxm_m1
	 defptr bench_vmergevim_m1
	defptr bench_vmvvv_m1
	defptr bench_vmvvx_m1
	defptr bench_vmvvi_m1
	defptr bench_vmseqvv_m1
	defptr bench_vmseqvvm_m1
	defptr bench_vmseqvx_m1
	defptr bench_vmseqvxm_m1
	defptr bench_vmseqvi_m1
	defptr bench_vmseqvim_m1
	defptr bench_vmsnevv_m1
	defptr bench_vmsnevvm_m1
	defptr bench_vmsnevx_m1
	defptr bench_vmsnevxm_m1
	defptr bench_vmsnevi_m1
	defptr bench_vmsnevim_m1
	defptr bench_vmsltuvv_m1
	defptr bench_vmsltuvvm_m1
	defptr bench_vmsltuvx_m1
	defptr bench_vmsltuvxm_m1
	defptr bench_vmsltvv_m1
	defptr bench_vmsltvvm_m1
	defptr bench_vmsltvx_m1
	defptr bench_vmsltvxm_m1
	defptr bench_vmsleuvv_m1
	defptr bench_vmsleuvvm_m1
	defptr bench_vmsleuvx_m1
	defptr bench_vmsleuvxm_m1
	defptr bench_vmsleuvi_m1
	defptr bench_vmsleuvim_m1
	defptr bench_vmslevv_m1
	defptr bench_vmslevvm_m1
	defptr bench_vmslevx_m1
	defptr bench_vmslevxm_m1
	defptr bench_vmslevi_m1
	defptr bench_vmslevim_m1
	defptr bench_vmsgtuvx_m1
	defptr bench_vmsgtuvxm_m1
	defptr bench_vmsgtuvi_m1
	defptr bench_vmsgtuvim_m1
	defptr bench_vmsgtvx_m1
	defptr bench_vmsgtvxm_m1
	defptr bench_vmsgtvi_m1
	defptr bench_vmsgtvim_m1

	defptr bench_vcompressvm_m1

	defptr bench_vmandnmm_m1
	defptr bench_vmandmm_m1
	defptr bench_vmormm_m1
	defptr bench_vmxormm_m1
	defptr bench_vmornmm_m1
	defptr bench_vmnandmm_m1
	defptr bench_vmnormm_m1
	defptr bench_vmxnormm_m1

	defptr bench_vsadduvv_m1
	defptr bench_vsadduvvm_m1
	defptr bench_vsadduvx_m1
	defptr bench_vsadduvxm_m1
	defptr bench_vsadduvi_m1
	defptr bench_vsadduvim_m1
	defptr bench_vsaddvv_m1
	defptr bench_vsaddvvm_m1
	defptr bench_vsaddvx_m1
	defptr bench_vsaddvxm_m1
	defptr bench_vsaddvi_m1
	defptr bench_vsaddvim_m1
	defptr bench_vssubuvv_m1
	defptr bench_vssubuvvm_m1
	defptr bench_vssubuvx_m1
	defptr bench_vssubuvxm_m1
	defptr bench_vssubvv_m1
	defptr bench_vssubvvm_m1
	defptr bench_vssubvx_m1
	defptr bench_vssubvxm_m1
	defptr bench_vsllvv_m1
	defptr bench_vsllvvm_m1
	defptr bench_vsllvx_m1
	defptr bench_vsllvxm_m1
	defptr bench_vsllvi_m1
	defptr bench_vsllvim_m1
	defptr bench_vsmulvv_m1
	defptr bench_vsmulvvm_m1
	defptr bench_vsmulvx_m1
	defptr bench_vsmulvxm_m1
	defptr bench_vmv1rv_m1
	defptr bench_vmv2rv_m1
	defptr bench_vmv4rv_m1
	defptr bench_vmv8rv_m1
	defptr bench_vsrlvv_m1
	defptr bench_vsrlvvm_m1
	defptr bench_vsrlvx_m1
	defptr bench_vsrlvxm_m1
	defptr bench_vsrlvi_m1
	defptr bench_vsrlvim_m1
	defptr bench_vsravv_m1
	defptr bench_vsravvm_m1
	defptr bench_vsravx_m1
	defptr bench_vsravxm_m1
	defptr bench_vsravi_m1
	defptr bench_vsravim_m1
	defptr bench_vssrlvv_m1
	defptr bench_vssrlvvm_m1
	defptr bench_vssrlvx_m1
	defptr bench_vssrlvxm_m1
	defptr bench_vssrlvi_m1
	defptr bench_vssrlvim_m1

	defptr bench_vdivuvv_m1
	defptr bench_vdivuvvm_m1
	defptr bench_vdivuvx_m1
	defptr bench_vdivuvxm_m1
	defptr bench_vdivvv_m1
	defptr bench_vdivvvm_m1
	defptr bench_vdivvx_m1
	defptr bench_vdivvxm_m1
	defptr bench_vremuvv_m1
	defptr bench_vremuvvm_m1
	defptr bench_vremuvx_m1
	defptr bench_vremuvxm_m1
	defptr bench_vremvv_m1
	defptr bench_vremvvm_m1
	defptr bench_vremvx_m1
	defptr bench_vremvxm_m1
	defptr bench_vmulhuvv_m1
	defptr bench_vmulhuvvm_m1
	defptr bench_vmulhuvx_m1
	defptr bench_vmulhuvxm_m1
	defptr bench_vmulvv_m1
	defptr bench_vmulvvm_m1
	defptr bench_vmulvx_m1
	defptr bench_vmulvxm_m1
	defptr bench_vmulhsuvv_m1
	defptr bench_vmulhsuvvm_m1
	defptr bench_vmulhsuvx_m1
	defptr bench_vmulhsuvxm_m1
	defptr bench_vmulhvv_m1
	defptr bench_vmulhvvm_m1
	defptr bench_vmulhvx_m1
	defptr bench_vmulhvxm_m1
	defptr bench_vmaddvv_m1
	defptr bench_vmaddvvm_m1
	defptr bench_vmaddvx_m1
	defptr bench_vmaddvxm_m1
	defptr bench_vmaccvv_m1
	defptr bench_vmaccvvm_m1
	defptr bench_vmaccvx_m1
	defptr bench_vmaccvxm_m1

	defptr bench_vnsrlwv_m1
	defptr bench_vnsrlwvm_m1
	defptr bench_vnsrlwx_m1
	defptr bench_vnsrlwxm_m1
	defptr bench_vnsrlwi_m1
	defptr bench_vnsrlwim_m1
	defptr bench_vnsrawv_m1
	defptr bench_vnsrawvm_m1
	defptr bench_vnsrawx_m1
	defptr bench_vnsrawxm_m1
	defptr bench_vnsrawi_m1
	defptr bench_vnsrawim_m1
	defptr bench_vnclipuwv_m1
	defptr bench_vnclipuwvm_m1
	defptr bench_vnclipuwx_m1
	defptr bench_vnclipuwxm_m1
	defptr bench_vnclipuwi_m1
	defptr bench_vnclipuwim_m1
	defptr bench_vnclipwv_m1
	defptr bench_vnclipwvm_m1
	defptr bench_vnclipwx_m1
	defptr bench_vnclipwxm_m1
	defptr bench_vnclipwi_m1
	defptr bench_vnclipwim_m1
	defptr bench_vnmsubvv_m1
	defptr bench_vnmsubvvm_m1
	defptr bench_vnmsubvx_m1
	defptr bench_vnmsubvxm_m1
	defptr bench_vnmsacvv_m1
	defptr bench_vnmsacvvm_m1
	defptr bench_vnmsacvx_m1
	defptr bench_vnmsacvxm_m1

	defptr bench_vwadduvv_m1
	defptr bench_vwadduvvm_m1
	defptr bench_vwadduvx_m1
	defptr bench_vwadduvxm_m1
	defptr bench_vwaddvv_m1
	defptr bench_vwaddvvm_m1
	defptr bench_vwaddvx_m1
	defptr bench_vwaddvxm_m1
	defptr bench_vwsubuvv_m1
	defptr bench_vwsubuvvm_m1
	defptr bench_vwsubuvx_m1
	defptr bench_vwsubuvxm_m1
	defptr bench_vwsubvv_m1
	defptr bench_vwsubvvm_m1
	defptr bench_vwsubvx_m1
	defptr bench_vwsubvxm_m1
	defptr bench_vwadduwv_m1
	defptr bench_vwadduwvm_m1
	defptr bench_vwadduwx_m1
	defptr bench_vwadduwxm_m1
	defptr bench_vwaddwv_m1
	defptr bench_vwaddwvm_m1
	defptr bench_vwaddwx_m1
	defptr bench_vwaddwxm_m1
	defptr bench_vwsubuwv_m1
	defptr bench_vwsubuwvm_m1
	defptr bench_vwsubuwx_m1
	defptr bench_vwsubuwxm_m1
	defptr bench_vwsubwv_m1
	defptr bench_vwsubwvm_m1
	defptr bench_vwsubwx_m1
	defptr bench_vwsubwxm_m1
	defptr bench_vwmuluvv_m1
	defptr bench_vwmuluvvm_m1
	defptr bench_vwmuluvx_m1
	defptr bench_vwmuluvxm_m1
	defptr bench_vwmulsuvv_m1
	defptr bench_vwmulsuvvm_m1
	defptr bench_vwmulsuvx_m1
	defptr bench_vwmulsuvxm_m1
	defptr bench_vwmulvv_m1
	defptr bench_vwmulvvm_m1
	defptr bench_vwmulvx_m1
	defptr bench_vwmulvxm_m1
	defptr bench_vwmaccuvv_m1
	defptr bench_vwmaccuvvm_m1
	defptr bench_vwmaccuvx_m1
	defptr bench_vwmaccuvxm_m1
	defptr bench_vwmaccvv_m1
	defptr bench_vwmaccvvm_m1
	defptr bench_vwmaccvx_m1
	defptr bench_vwmaccvxm_m1
	defptr bench_vwmaccsuvv_m1
	defptr bench_vwmaccsuvvm_m1
	defptr bench_vwmaccsuvx_m1
	defptr bench_vwmaccsuvxm_m1
	defptr bench_vwmaccusvx_m1
	defptr bench_vwmaccusvxm_m1

	defptr bench_vfaddvv_m1
	defptr bench_vfaddvvm_m1
	defptr bench_vfaddvf_m1
	defptr bench_vfaddvfm_m1
	defptr bench_vfsubvv_m1
	defptr bench_vfsubvvm_m1
	defptr bench_vfsubvf_m1
	defptr bench_vfsubvfm_m1
	defptr bench_vfminvv_m1
	defptr bench_vfminvvm_m1
	defptr bench_vfminvf_m1
	defptr bench_vfminvfm_m1
	defptr bench_vfmaxvv_m1
	defptr bench_vfmaxvvm_m1
	defptr bench_vfmaxvf_m1
	defptr bench_vfmaxvfm_m1
	defptr bench_vfsgnjvv_m1
	defptr bench_vfsgnjvvm_m1
	defptr bench_vfsgnjvf_m1
	defptr bench_vfsgnjvfm_m1
	defptr bench_vfsgnjnvv_m1
	defptr bench_vfsgnjnvvm_m1
	defptr bench_vfsgnjnvf_m1
	defptr bench_vfsgnjnvfm_m1
	defptr bench_vfsgnjxvv_m1
	defptr bench_vfsgnjxvvm_m1
	defptr bench_vfsgnjxvf_m1
	defptr bench_vfsgnjxvfm_m1
	defptr bench_vfslide1upvf_m1
	defptr bench_vfslide1upvfm_m1
	defptr bench_vfslide1downvf_m1
	defptr bench_vfslide1downvfm_m1

	defptr bench_vfredusumvs_m1
	defptr bench_vfredusumvsm_m1
	defptr bench_vfredosumvs_m1
	defptr bench_vfredosumvsm_m1
	defptr bench_vfredminvs_m1
	defptr bench_vfredminvsm_m1
	defptr bench_vfredmaxvs_m1
	defptr bench_vfredmaxvsm_m1

	defptr bench_vfmergevfm_m1
	defptr bench_vfmvvf_m1

	defptr bench_vmfeqvv_m1
	defptr bench_vmfeqvvm_m1
	defptr bench_vmfeqvf_m1
	defptr bench_vmfeqvfm_m1
	defptr bench_vmflevv_m1
	defptr bench_vmflevvm_m1
	defptr bench_vmflevf_m1
	defptr bench_vmflevfm_m1
	defptr bench_vmfltvv_m1
	defptr bench_vmfltvvm_m1
	defptr bench_vmfltvf_m1
	defptr bench_vmfltvfm_m1
	defptr bench_vmfnevv_m1
	defptr bench_vmfnevvm_m1
	defptr bench_vmfnevf_m1
	defptr bench_vmfnevfm_m1
	defptr bench_vmfgtvv_m1
	defptr bench_vmfgtvvm_m1
	defptr bench_vmfgtvf_m1
	defptr bench_vmfgtvfm_m1
	defptr bench_vmfgevv_m1
	defptr bench_vmfgevvm_m1
	defptr bench_vmfgevf_m1
	defptr bench_vmfgevfm_m1

	defptr bench_vfdivvv_m1
	defptr bench_vfdivvvm_m1
	defptr bench_vfdivvf_m1
	defptr bench_vfdivvfm_m1
	defptr bench_vfrdivvf_m1
	defptr bench_vfrdivvfm_m1
	defptr bench_vfmulvv_m1
	defptr bench_vfmulvvm_m1
	defptr bench_vfmulvf_m1
	defptr bench_vfmulvfm_m1
	defptr bench_vfrsubvf_m1
	defptr bench_vfrsubvfm_m1
	defptr bench_vfmaddvv_m1
	defptr bench_vfmaddvvm_m1
	defptr bench_vfmaddvf_m1
	defptr bench_vfmaddvfm_m1
	defptr bench_vfmsubvv_m1
	defptr bench_vfmsubvvm_m1
	defptr bench_vfmsubvf_m1
	defptr bench_vfmsubvfm_m1
	defptr bench_vfmaccvv_m1
	defptr bench_vfmaccvvm_m1
	defptr bench_vfmaccvf_m1
	defptr bench_vfmaccvfm_m1
	defptr bench_vfmsacvv_m1
	defptr bench_vfmsacvvm_m1
	defptr bench_vfmsacvf_m1
	defptr bench_vfmsacvfm_m1

	defptr bench_vfnmsacvv_m1
	defptr bench_vfnmsacvvm_m1
	defptr bench_vfnmsacvf_m1
	defptr bench_vfnmsacvfm_m1
	defptr bench_vfnmaccvv_m1
	defptr bench_vfnmaccvvm_m1
	defptr bench_vfnmaccvf_m1
	defptr bench_vfnmaccvfm_m1
	defptr bench_vfnmsubvv_m1
	defptr bench_vfnmsubvvm_m1
	defptr bench_vfnmsubvf_m1
	defptr bench_vfnmsubvfm_m1
	defptr bench_vfnmaddvv_m1
	defptr bench_vfnmaddvvm_m1
	defptr bench_vfnmaddvf_m1
	defptr bench_vfnmaddvfm_m1

	defptr bench_vwredsumuvs_m1
	defptr bench_vwredsumuvsm_m1
	defptr bench_vwredsumvs_m1
	defptr bench_vwredsumvsm_m1

	defptr bench_vfwaddvv_m1
	defptr bench_vfwaddvvm_m1
	defptr bench_vfwaddvf_m1
	defptr bench_vfwaddvfm_m1
	defptr bench_vfwsubvv_m1
	defptr bench_vfwsubvvm_m1
	defptr bench_vfwsubvf_m1
	defptr bench_vfwsubvfm_m1
	defptr bench_vfwaddwv_m1
	defptr bench_vfwaddwvm_m1
	defptr bench_vfwaddwf_m1
	defptr bench_vfwaddwfm_m1
	defptr bench_vfwsubwv_m1
	defptr bench_vfwsubwvm_m1
	defptr bench_vfwsubwf_m1
	defptr bench_vfwsubwfm_m1
	defptr bench_vfwmulvv_m1
	defptr bench_vfwmulvvm_m1
	defptr bench_vfwmulvf_m1
	defptr bench_vfwmulvfm_m1
	defptr bench_vfwmaccvv_m1
	defptr bench_vfwmaccvvm_m1
	defptr bench_vfwmaccvf_m1
	defptr bench_vfwmaccvfm_m1
	defptr bench_vfwnmaccvv_m1
	defptr bench_vfwnmaccvvm_m1
	defptr bench_vfwnmaccvf_m1
	defptr bench_vfwnmaccvfm_m1
	defptr bench_vfwmsacvv_m1
	defptr bench_vfwmsacvvm_m1
	defptr bench_vfwmsacvf_m1
	defptr bench_vfwmsacvfm_m1
	defptr bench_vfwnmsacvv_m1
	defptr bench_vfwnmsacvvm_m1
	defptr bench_vfwnmsacvf_m1
	defptr bench_vfwnmsacvfm_m1

	defptr bench_vfwredosumvs_m1
	defptr bench_vfwredosumvsm_m1
	defptr bench_vfwredusumvs_m1
	defptr bench_vfwredusumvsm_m1

	defptr bench_vmvsx_m1
	defptr bench_vmvxs_m1

	defptr bench_vcpopm_m1
	defptr bench_vcpopmm_m1
	defptr bench_vfirstm_m1
	defptr bench_vfirstmm_m1
	defptr bench_vzextvf2_m1
	defptr bench_vzextvf2m_m1
	defptr bench_vsextvf2_m1
	defptr bench_vsextvf2m_m1
	defptr bench_vzextvf4_m1
	defptr bench_vzextvf4m_m1
	defptr bench_vsextvf4_m1
	defptr bench_vsextvf4m_m1
	defptr bench_vzextvf8_m1
	defptr bench_vzextvf8m_m1
	defptr bench_vsextvf8_m1
	defptr bench_vsextvf8m_m1

	defptr bench_vfmvfs_m1
	defptr bench_vfmvsf_m1

	defptr bench_vfcvtxufv_m1
	defptr bench_vfcvtxufvm_m1
	defptr bench_vfcvtxfv_m1
	defptr bench_vfcvtxfvm_m1
	defptr bench_vfcvtfxuv_m1
	defptr bench_vfcvtfxuvm_m1
	defptr bench_vfcvtfxv_m1
	defptr bench_vfcvtfxvm_m1
	defptr bench_vfcvtrtzxfv_m1
	defptr bench_vfcvtrtzxfvm_m1
	defptr bench_vfcvtrtzxufv_m1
	defptr bench_vfcvtrtzxufvm_m1

	defptr bench_vfwcvtxufv_m1
	defptr bench_vfwcvtxufvm_m1
	defptr bench_vfwcvtxfv_m1
	defptr bench_vfwcvtxfvm_m1
	defptr bench_vfwcvtfxuv_m1
	defptr bench_vfwcvtfxuvm_m1
	defptr bench_vfwcvtfxv_m1
	defptr bench_vfwcvtfxvm_m1
	defptr bench_vfwcvtffv_m1
	defptr bench_vfwcvtffvm_m1
	defptr bench_vfwcvtrtzxufv_m1
	defptr bench_vfwcvtrtzxufvm_m1
	defptr bench_vfwcvtrtzxfv_m1
	defptr bench_vfwcvtrtzxfvm_m1

	defptr bench_vfncvtxufw_m1
	defptr bench_vfncvtxufwm_m1
	defptr bench_vfncvtxfw_m1
	defptr bench_vfncvtxfwm_m1
	defptr bench_vfncvtfxuw_m1
	defptr bench_vfncvtfxuwm_m1
	defptr bench_vfncvtfxw_m1
	defptr bench_vfncvtfxwm_m1
	defptr bench_vfncvtffw_m1
	defptr bench_vfncvtffwm_m1
	defptr bench_vfncvtrtzxfw_m1
	defptr bench_vfncvtrtzxfwm_m1
	defptr bench_vfncvtrtzxufw_m1
	defptr bench_vfncvtrtzxufwm_m1
	defptr bench_vfncvt.rod.f.f.w_m1
	defptr bench_vfncvt.rod.f.f.wm_m1

	defptr bench_vfsqrtv_m1
	defptr bench_vfsqrtvm_m1
	defptr bench_vfrsqrt7v_m1
	defptr bench_vfrsqrt7vm_m1
	defptr bench_vfrec7v_m1
	defptr bench_vfrec7vm_m1
	defptr bench_vfclassv_m1
	defptr bench_vfclassvm_m1

	defptr bench_vmsbfm_m1
	defptr bench_vmsbfmm_m1
	defptr bench_vmsofm_m1
	defptr bench_vmsofmm_m1
	defptr bench_vmsifm_m1
	defptr bench_vmsifmm_m1
	defptr bench_viotam_m1
	defptr bench_viotamm_m1
	defptr bench_vidv_m1
	defptr bench_vidvm_m1



.balign 8


.global bench_m2
	bench_m2:

	defptr bench_add_m2
	defptr bench_mul_m2
	defptr bench_vaddvv_m2
	defptr bench_vaddvvm_m2
	defptr bench_vaddvx_m2
	defptr bench_vaddvxm_m2
	defptr bench_vaddvi_m2
	defptr bench_vaddvim_m2
	defptr bench_vsubvv_m2
	defptr bench_vsubvvm_m2
	defptr bench_vsubvx_m2
	defptr bench_vsubvxm_m2
	defptr bench_vrsubvx_m2
	defptr bench_vrsubvxm_m2
	defptr bench_vrsubvi_m2
	defptr bench_vrsubvim_m2
	defptr bench_vminuvv_m2
	defptr bench_vminuvvm_m2
	defptr bench_vminuvx_m2
	defptr bench_vminuvxm_m2
	defptr bench_vminvv_m2
	defptr bench_vminvvm_m2
	defptr bench_vminvx_m2
	defptr bench_vminvxm_m2
	defptr bench_vmaxuvv_m2
	defptr bench_vmaxuvvm_m2
	defptr bench_vmaxuvx_m2
	defptr bench_vmaxuvxm_m2
	defptr bench_vmaxvv_m2
	defptr bench_vmaxvvm_m2
	defptr bench_vmaxvx_m2
	defptr bench_vmaxvxm_m2
	defptr bench_vandvv_m2
	defptr bench_vandvvm_m2
	defptr bench_vandvx_m2
	defptr bench_vandvxm_m2
	defptr bench_vandvi_m2
	defptr bench_vandvim_m2
	defptr bench_vorvv_m2
	defptr bench_vorvvm_m2
	defptr bench_vorvx_m2
	defptr bench_vorvxm_m2
	defptr bench_vorvi_m2
	defptr bench_vorvim_m2
	defptr bench_vxorvv_m2
	defptr bench_vxorvvm_m2
	defptr bench_vxorvx_m2
	defptr bench_vxorvxm_m2
	defptr bench_vxorvi_m2
	defptr bench_vxorvim_m2

	defptr bench_vrgathervv_m2
	defptr bench_vrgathervvm_m2
	defptr bench_vrgathervx_m2
	defptr bench_vrgathervxm_m2
	defptr bench_vrgathervi_m2
	defptr bench_vrgathervim_m2
	defptr bench_vslideupvx_m2
	defptr bench_vslideupvxm_m2
	defptr bench_vslideupvi_m2
	defptr bench_vslideupvim_m2
	defptr bench_vrgatherei16vv_m2
	defptr bench_vrgatherei16vvm_m2

	defptr bench_vslidedownvx_m2
	defptr bench_vslidedownvxm_m2
	defptr bench_vslidedownvi_m2
	defptr bench_vslidedownvim_m2

	defptr bench_vredsumvs_m2
	defptr bench_vredsumvsm_m2
	defptr bench_vredandvs_m2
	defptr bench_vredandvsm_m2
	defptr bench_vredorvs_m2
	defptr bench_vredorvsm_m2
	defptr bench_vredxorvs_m2
	defptr bench_vredxorvsm_m2
	defptr bench_vredminuvs_m2
	defptr bench_vredminuvsm_m2
	defptr bench_vredminvs_m2
	defptr bench_vredminvsm_m2
	defptr bench_vredmaxuvs_m2
	defptr bench_vredmaxuvsm_m2
	defptr bench_vredmaxvs_m2
	defptr bench_vredmaxvsm_m2

	defptr bench_vaadduvv_m2
	defptr bench_vaadduvvm_m2
	defptr bench_vaadduvx_m2
	defptr bench_vaadduvxm_m2
	defptr bench_vaaddvv_m2
	defptr bench_vaaddvvm_m2
	defptr bench_vaaddvx_m2
	defptr bench_vaaddvxm_m2
	defptr bench_vasubuvv_m2
	defptr bench_vasubuvvm_m2
	defptr bench_vasubuvx_m2
	defptr bench_vasubuvxm_m2
	defptr bench_vasubvv_m2
	defptr bench_vasubvvm_m2
	defptr bench_vasubvx_m2
	defptr bench_vasubvxm_m2

	defptr bench_vslide1upvx_m2
	defptr bench_vslide1upvxm_m2
	defptr bench_vslide1downvx_m2
	defptr bench_vslide1downvxm_m2

	defptr bench_vadcvvm_m2
	 defptr bench_vadcvxm_m2
	 defptr bench_vadcvim_m2
	defptr bench_vmadcvvm_m2
	 defptr bench_vmadcvxm_m2
	 defptr bench_vmadcvim_m2
	defptr bench_vmadcvv_m2
	defptr bench_vmadcvx_m2
	defptr bench_vmadcvi_m2
	defptr bench_vsbcvvm_m2
	 defptr bench_vsbcvxm_m2
	defptr bench_vmsbcvvm_m2
	 defptr bench_vmsbcvxm_m2
	defptr bench_vmsbcvv_m2
	defptr bench_vmsbcvx_m2

	defptr bench_vmergevvm_m2
	 defptr bench_vmergevxm_m2
	 defptr bench_vmergevim_m2
	defptr bench_vmvvv_m2
	defptr bench_vmvvx_m2
	defptr bench_vmvvi_m2
	defptr bench_vmseqvv_m2
	defptr bench_vmseqvvm_m2
	defptr bench_vmseqvx_m2
	defptr bench_vmseqvxm_m2
	defptr bench_vmseqvi_m2
	defptr bench_vmseqvim_m2
	defptr bench_vmsnevv_m2
	defptr bench_vmsnevvm_m2
	defptr bench_vmsnevx_m2
	defptr bench_vmsnevxm_m2
	defptr bench_vmsnevi_m2
	defptr bench_vmsnevim_m2
	defptr bench_vmsltuvv_m2
	defptr bench_vmsltuvvm_m2
	defptr bench_vmsltuvx_m2
	defptr bench_vmsltuvxm_m2
	defptr bench_vmsltvv_m2
	defptr bench_vmsltvvm_m2
	defptr bench_vmsltvx_m2
	defptr bench_vmsltvxm_m2
	defptr bench_vmsleuvv_m2
	defptr bench_vmsleuvvm_m2
	defptr bench_vmsleuvx_m2
	defptr bench_vmsleuvxm_m2
	defptr bench_vmsleuvi_m2
	defptr bench_vmsleuvim_m2
	defptr bench_vmslevv_m2
	defptr bench_vmslevvm_m2
	defptr bench_vmslevx_m2
	defptr bench_vmslevxm_m2
	defptr bench_vmslevi_m2
	defptr bench_vmslevim_m2
	defptr bench_vmsgtuvx_m2
	defptr bench_vmsgtuvxm_m2
	defptr bench_vmsgtuvi_m2
	defptr bench_vmsgtuvim_m2
	defptr bench_vmsgtvx_m2
	defptr bench_vmsgtvxm_m2
	defptr bench_vmsgtvi_m2
	defptr bench_vmsgtvim_m2

	defptr bench_vcompressvm_m2

	defptr bench_vmandnmm_m2
	defptr bench_vmandmm_m2
	defptr bench_vmormm_m2
	defptr bench_vmxormm_m2
	defptr bench_vmornmm_m2
	defptr bench_vmnandmm_m2
	defptr bench_vmnormm_m2
	defptr bench_vmxnormm_m2

	defptr bench_vsadduvv_m2
	defptr bench_vsadduvvm_m2
	defptr bench_vsadduvx_m2
	defptr bench_vsadduvxm_m2
	defptr bench_vsadduvi_m2
	defptr bench_vsadduvim_m2
	defptr bench_vsaddvv_m2
	defptr bench_vsaddvvm_m2
	defptr bench_vsaddvx_m2
	defptr bench_vsaddvxm_m2
	defptr bench_vsaddvi_m2
	defptr bench_vsaddvim_m2
	defptr bench_vssubuvv_m2
	defptr bench_vssubuvvm_m2
	defptr bench_vssubuvx_m2
	defptr bench_vssubuvxm_m2
	defptr bench_vssubvv_m2
	defptr bench_vssubvvm_m2
	defptr bench_vssubvx_m2
	defptr bench_vssubvxm_m2
	defptr bench_vsllvv_m2
	defptr bench_vsllvvm_m2
	defptr bench_vsllvx_m2
	defptr bench_vsllvxm_m2
	defptr bench_vsllvi_m2
	defptr bench_vsllvim_m2
	defptr bench_vsmulvv_m2
	defptr bench_vsmulvvm_m2
	defptr bench_vsmulvx_m2
	defptr bench_vsmulvxm_m2
	defptr bench_vmv1rv_m2
	defptr bench_vmv2rv_m2
	defptr bench_vmv4rv_m2
	defptr bench_vmv8rv_m2
	defptr bench_vsrlvv_m2
	defptr bench_vsrlvvm_m2
	defptr bench_vsrlvx_m2
	defptr bench_vsrlvxm_m2
	defptr bench_vsrlvi_m2
	defptr bench_vsrlvim_m2
	defptr bench_vsravv_m2
	defptr bench_vsravvm_m2
	defptr bench_vsravx_m2
	defptr bench_vsravxm_m2
	defptr bench_vsravi_m2
	defptr bench_vsravim_m2
	defptr bench_vssrlvv_m2
	defptr bench_vssrlvvm_m2
	defptr bench_vssrlvx_m2
	defptr bench_vssrlvxm_m2
	defptr bench_vssrlvi_m2
	defptr bench_vssrlvim_m2

	defptr bench_vdivuvv_m2
	defptr bench_vdivuvvm_m2
	defptr bench_vdivuvx_m2
	defptr bench_vdivuvxm_m2
	defptr bench_vdivvv_m2
	defptr bench_vdivvvm_m2
	defptr bench_vdivvx_m2
	defptr bench_vdivvxm_m2
	defptr bench_vremuvv_m2
	defptr bench_vremuvvm_m2
	defptr bench_vremuvx_m2
	defptr bench_vremuvxm_m2
	defptr bench_vremvv_m2
	defptr bench_vremvvm_m2
	defptr bench_vremvx_m2
	defptr bench_vremvxm_m2
	defptr bench_vmulhuvv_m2
	defptr bench_vmulhuvvm_m2
	defptr bench_vmulhuvx_m2
	defptr bench_vmulhuvxm_m2
	defptr bench_vmulvv_m2
	defptr bench_vmulvvm_m2
	defptr bench_vmulvx_m2
	defptr bench_vmulvxm_m2
	defptr bench_vmulhsuvv_m2
	defptr bench_vmulhsuvvm_m2
	defptr bench_vmulhsuvx_m2
	defptr bench_vmulhsuvxm_m2
	defptr bench_vmulhvv_m2
	defptr bench_vmulhvvm_m2
	defptr bench_vmulhvx_m2
	defptr bench_vmulhvxm_m2
	defptr bench_vmaddvv_m2
	defptr bench_vmaddvvm_m2
	defptr bench_vmaddvx_m2
	defptr bench_vmaddvxm_m2
	defptr bench_vmaccvv_m2
	defptr bench_vmaccvvm_m2
	defptr bench_vmaccvx_m2
	defptr bench_vmaccvxm_m2

	defptr bench_vnsrlwv_m2
	defptr bench_vnsrlwvm_m2
	defptr bench_vnsrlwx_m2
	defptr bench_vnsrlwxm_m2
	defptr bench_vnsrlwi_m2
	defptr bench_vnsrlwim_m2
	defptr bench_vnsrawv_m2
	defptr bench_vnsrawvm_m2
	defptr bench_vnsrawx_m2
	defptr bench_vnsrawxm_m2
	defptr bench_vnsrawi_m2
	defptr bench_vnsrawim_m2
	defptr bench_vnclipuwv_m2
	defptr bench_vnclipuwvm_m2
	defptr bench_vnclipuwx_m2
	defptr bench_vnclipuwxm_m2
	defptr bench_vnclipuwi_m2
	defptr bench_vnclipuwim_m2
	defptr bench_vnclipwv_m2
	defptr bench_vnclipwvm_m2
	defptr bench_vnclipwx_m2
	defptr bench_vnclipwxm_m2
	defptr bench_vnclipwi_m2
	defptr bench_vnclipwim_m2
	defptr bench_vnmsubvv_m2
	defptr bench_vnmsubvvm_m2
	defptr bench_vnmsubvx_m2
	defptr bench_vnmsubvxm_m2
	defptr bench_vnmsacvv_m2
	defptr bench_vnmsacvvm_m2
	defptr bench_vnmsacvx_m2
	defptr bench_vnmsacvxm_m2

	defptr bench_vwadduvv_m2
	defptr bench_vwadduvvm_m2
	defptr bench_vwadduvx_m2
	defptr bench_vwadduvxm_m2
	defptr bench_vwaddvv_m2
	defptr bench_vwaddvvm_m2
	defptr bench_vwaddvx_m2
	defptr bench_vwaddvxm_m2
	defptr bench_vwsubuvv_m2
	defptr bench_vwsubuvvm_m2
	defptr bench_vwsubuvx_m2
	defptr bench_vwsubuvxm_m2
	defptr bench_vwsubvv_m2
	defptr bench_vwsubvvm_m2
	defptr bench_vwsubvx_m2
	defptr bench_vwsubvxm_m2
	defptr bench_vwadduwv_m2
	defptr bench_vwadduwvm_m2
	defptr bench_vwadduwx_m2
	defptr bench_vwadduwxm_m2
	defptr bench_vwaddwv_m2
	defptr bench_vwaddwvm_m2
	defptr bench_vwaddwx_m2
	defptr bench_vwaddwxm_m2
	defptr bench_vwsubuwv_m2
	defptr bench_vwsubuwvm_m2
	defptr bench_vwsubuwx_m2
	defptr bench_vwsubuwxm_m2
	defptr bench_vwsubwv_m2
	defptr bench_vwsubwvm_m2
	defptr bench_vwsubwx_m2
	defptr bench_vwsubwxm_m2
	defptr bench_vwmuluvv_m2
	defptr bench_vwmuluvvm_m2
	defptr bench_vwmuluvx_m2
	defptr bench_vwmuluvxm_m2
	defptr bench_vwmulsuvv_m2
	defptr bench_vwmulsuvvm_m2
	defptr bench_vwmulsuvx_m2
	defptr bench_vwmulsuvxm_m2
	defptr bench_vwmulvv_m2
	defptr bench_vwmulvvm_m2
	defptr bench_vwmulvx_m2
	defptr bench_vwmulvxm_m2
	defptr bench_vwmaccuvv_m2
	defptr bench_vwmaccuvvm_m2
	defptr bench_vwmaccuvx_m2
	defptr bench_vwmaccuvxm_m2
	defptr bench_vwmaccvv_m2
	defptr bench_vwmaccvvm_m2
	defptr bench_vwmaccvx_m2
	defptr bench_vwmaccvxm_m2
	defptr bench_vwmaccsuvv_m2
	defptr bench_vwmaccsuvvm_m2
	defptr bench_vwmaccsuvx_m2
	defptr bench_vwmaccsuvxm_m2
	defptr bench_vwmaccusvx_m2
	defptr bench_vwmaccusvxm_m2

	defptr bench_vfaddvv_m2
	defptr bench_vfaddvvm_m2
	defptr bench_vfaddvf_m2
	defptr bench_vfaddvfm_m2
	defptr bench_vfsubvv_m2
	defptr bench_vfsubvvm_m2
	defptr bench_vfsubvf_m2
	defptr bench_vfsubvfm_m2
	defptr bench_vfminvv_m2
	defptr bench_vfminvvm_m2
	defptr bench_vfminvf_m2
	defptr bench_vfminvfm_m2
	defptr bench_vfmaxvv_m2
	defptr bench_vfmaxvvm_m2
	defptr bench_vfmaxvf_m2
	defptr bench_vfmaxvfm_m2
	defptr bench_vfsgnjvv_m2
	defptr bench_vfsgnjvvm_m2
	defptr bench_vfsgnjvf_m2
	defptr bench_vfsgnjvfm_m2
	defptr bench_vfsgnjnvv_m2
	defptr bench_vfsgnjnvvm_m2
	defptr bench_vfsgnjnvf_m2
	defptr bench_vfsgnjnvfm_m2
	defptr bench_vfsgnjxvv_m2
	defptr bench_vfsgnjxvvm_m2
	defptr bench_vfsgnjxvf_m2
	defptr bench_vfsgnjxvfm_m2
	defptr bench_vfslide1upvf_m2
	defptr bench_vfslide1upvfm_m2
	defptr bench_vfslide1downvf_m2
	defptr bench_vfslide1downvfm_m2

	defptr bench_vfredusumvs_m2
	defptr bench_vfredusumvsm_m2
	defptr bench_vfredosumvs_m2
	defptr bench_vfredosumvsm_m2
	defptr bench_vfredminvs_m2
	defptr bench_vfredminvsm_m2
	defptr bench_vfredmaxvs_m2
	defptr bench_vfredmaxvsm_m2

	defptr bench_vfmergevfm_m2
	defptr bench_vfmvvf_m2

	defptr bench_vmfeqvv_m2
	defptr bench_vmfeqvvm_m2
	defptr bench_vmfeqvf_m2
	defptr bench_vmfeqvfm_m2
	defptr bench_vmflevv_m2
	defptr bench_vmflevvm_m2
	defptr bench_vmflevf_m2
	defptr bench_vmflevfm_m2
	defptr bench_vmfltvv_m2
	defptr bench_vmfltvvm_m2
	defptr bench_vmfltvf_m2
	defptr bench_vmfltvfm_m2
	defptr bench_vmfnevv_m2
	defptr bench_vmfnevvm_m2
	defptr bench_vmfnevf_m2
	defptr bench_vmfnevfm_m2
	defptr bench_vmfgtvv_m2
	defptr bench_vmfgtvvm_m2
	defptr bench_vmfgtvf_m2
	defptr bench_vmfgtvfm_m2
	defptr bench_vmfgevv_m2
	defptr bench_vmfgevvm_m2
	defptr bench_vmfgevf_m2
	defptr bench_vmfgevfm_m2

	defptr bench_vfdivvv_m2
	defptr bench_vfdivvvm_m2
	defptr bench_vfdivvf_m2
	defptr bench_vfdivvfm_m2
	defptr bench_vfrdivvf_m2
	defptr bench_vfrdivvfm_m2
	defptr bench_vfmulvv_m2
	defptr bench_vfmulvvm_m2
	defptr bench_vfmulvf_m2
	defptr bench_vfmulvfm_m2
	defptr bench_vfrsubvf_m2
	defptr bench_vfrsubvfm_m2
	defptr bench_vfmaddvv_m2
	defptr bench_vfmaddvvm_m2
	defptr bench_vfmaddvf_m2
	defptr bench_vfmaddvfm_m2
	defptr bench_vfmsubvv_m2
	defptr bench_vfmsubvvm_m2
	defptr bench_vfmsubvf_m2
	defptr bench_vfmsubvfm_m2
	defptr bench_vfmaccvv_m2
	defptr bench_vfmaccvvm_m2
	defptr bench_vfmaccvf_m2
	defptr bench_vfmaccvfm_m2
	defptr bench_vfmsacvv_m2
	defptr bench_vfmsacvvm_m2
	defptr bench_vfmsacvf_m2
	defptr bench_vfmsacvfm_m2

	defptr bench_vfnmsacvv_m2
	defptr bench_vfnmsacvvm_m2
	defptr bench_vfnmsacvf_m2
	defptr bench_vfnmsacvfm_m2
	defptr bench_vfnmaccvv_m2
	defptr bench_vfnmaccvvm_m2
	defptr bench_vfnmaccvf_m2
	defptr bench_vfnmaccvfm_m2
	defptr bench_vfnmsubvv_m2
	defptr bench_vfnmsubvvm_m2
	defptr bench_vfnmsubvf_m2
	defptr bench_vfnmsubvfm_m2
	defptr bench_vfnmaddvv_m2
	defptr bench_vfnmaddvvm_m2
	defptr bench_vfnmaddvf_m2
	defptr bench_vfnmaddvfm_m2

	defptr bench_vwredsumuvs_m2
	defptr bench_vwredsumuvsm_m2
	defptr bench_vwredsumvs_m2
	defptr bench_vwredsumvsm_m2

	defptr bench_vfwaddvv_m2
	defptr bench_vfwaddvvm_m2
	defptr bench_vfwaddvf_m2
	defptr bench_vfwaddvfm_m2
	defptr bench_vfwsubvv_m2
	defptr bench_vfwsubvvm_m2
	defptr bench_vfwsubvf_m2
	defptr bench_vfwsubvfm_m2
	defptr bench_vfwaddwv_m2
	defptr bench_vfwaddwvm_m2
	defptr bench_vfwaddwf_m2
	defptr bench_vfwaddwfm_m2
	defptr bench_vfwsubwv_m2
	defptr bench_vfwsubwvm_m2
	defptr bench_vfwsubwf_m2
	defptr bench_vfwsubwfm_m2
	defptr bench_vfwmulvv_m2
	defptr bench_vfwmulvvm_m2
	defptr bench_vfwmulvf_m2
	defptr bench_vfwmulvfm_m2
	defptr bench_vfwmaccvv_m2
	defptr bench_vfwmaccvvm_m2
	defptr bench_vfwmaccvf_m2
	defptr bench_vfwmaccvfm_m2
	defptr bench_vfwnmaccvv_m2
	defptr bench_vfwnmaccvvm_m2
	defptr bench_vfwnmaccvf_m2
	defptr bench_vfwnmaccvfm_m2
	defptr bench_vfwmsacvv_m2
	defptr bench_vfwmsacvvm_m2
	defptr bench_vfwmsacvf_m2
	defptr bench_vfwmsacvfm_m2
	defptr bench_vfwnmsacvv_m2
	defptr bench_vfwnmsacvvm_m2
	defptr bench_vfwnmsacvf_m2
	defptr bench_vfwnmsacvfm_m2

	defptr bench_vfwredosumvs_m2
	defptr bench_vfwredosumvsm_m2
	defptr bench_vfwredusumvs_m2
	defptr bench_vfwredusumvsm_m2

	defptr bench_vmvsx_m2
	defptr bench_vmvxs_m2

	defptr bench_vcpopm_m2
	defptr bench_vcpopmm_m2
	defptr bench_vfirstm_m2
	defptr bench_vfirstmm_m2
	defptr bench_vzextvf2_m2
	defptr bench_vzextvf2m_m2
	defptr bench_vsextvf2_m2
	defptr bench_vsextvf2m_m2
	defptr bench_vzextvf4_m2
	defptr bench_vzextvf4m_m2
	defptr bench_vsextvf4_m2
	defptr bench_vsextvf4m_m2
	defptr bench_vzextvf8_m2
	defptr bench_vzextvf8m_m2
	defptr bench_vsextvf8_m2
	defptr bench_vsextvf8m_m2

	defptr bench_vfmvfs_m2
	defptr bench_vfmvsf_m2

	defptr bench_vfcvtxufv_m2
	defptr bench_vfcvtxufvm_m2
	defptr bench_vfcvtxfv_m2
	defptr bench_vfcvtxfvm_m2
	defptr bench_vfcvtfxuv_m2
	defptr bench_vfcvtfxuvm_m2
	defptr bench_vfcvtfxv_m2
	defptr bench_vfcvtfxvm_m2
	defptr bench_vfcvtrtzxfv_m2
	defptr bench_vfcvtrtzxfvm_m2
	defptr bench_vfcvtrtzxufv_m2
	defptr bench_vfcvtrtzxufvm_m2

	defptr bench_vfwcvtxufv_m2
	defptr bench_vfwcvtxufvm_m2
	defptr bench_vfwcvtxfv_m2
	defptr bench_vfwcvtxfvm_m2
	defptr bench_vfwcvtfxuv_m2
	defptr bench_vfwcvtfxuvm_m2
	defptr bench_vfwcvtfxv_m2
	defptr bench_vfwcvtfxvm_m2
	defptr bench_vfwcvtffv_m2
	defptr bench_vfwcvtffvm_m2
	defptr bench_vfwcvtrtzxufv_m2
	defptr bench_vfwcvtrtzxufvm_m2
	defptr bench_vfwcvtrtzxfv_m2
	defptr bench_vfwcvtrtzxfvm_m2

	defptr bench_vfncvtxufw_m2
	defptr bench_vfncvtxufwm_m2
	defptr bench_vfncvtxfw_m2
	defptr bench_vfncvtxfwm_m2
	defptr bench_vfncvtfxuw_m2
	defptr bench_vfncvtfxuwm_m2
	defptr bench_vfncvtfxw_m2
	defptr bench_vfncvtfxwm_m2
	defptr bench_vfncvtffw_m2
	defptr bench_vfncvtffwm_m2
	defptr bench_vfncvtrtzxfw_m2
	defptr bench_vfncvtrtzxfwm_m2
	defptr bench_vfncvtrtzxufw_m2
	defptr bench_vfncvtrtzxufwm_m2
	defptr bench_vfncvt.rod.f.f.w_m2
	defptr bench_vfncvt.rod.f.f.wm_m2

	defptr bench_vfsqrtv_m2
	defptr bench_vfsqrtvm_m2
	defptr bench_vfrsqrt7v_m2
	defptr bench_vfrsqrt7vm_m2
	defptr bench_vfrec7v_m2
	defptr bench_vfrec7vm_m2
	defptr bench_vfclassv_m2
	defptr bench_vfclassvm_m2

	defptr bench_vmsbfm_m2
	defptr bench_vmsbfmm_m2
	defptr bench_vmsofm_m2
	defptr bench_vmsofmm_m2
	defptr bench_vmsifm_m2
	defptr bench_vmsifmm_m2
	defptr bench_viotam_m2
	defptr bench_viotamm_m2
	defptr bench_vidv_m2
	defptr bench_vidvm_m2



.balign 8


.global bench_m4
	bench_m4:

	defptr bench_add_m4
	defptr bench_mul_m4
	defptr bench_vaddvv_m4
	defptr bench_vaddvvm_m4
	defptr bench_vaddvx_m4
	defptr bench_vaddvxm_m4
	defptr bench_vaddvi_m4
	defptr bench_vaddvim_m4
	defptr bench_vsubvv_m4
	defptr bench_vsubvvm_m4
	defptr bench_vsubvx_m4
	defptr bench_vsubvxm_m4
	defptr bench_vrsubvx_m4
	defptr bench_vrsubvxm_m4
	defptr bench_vrsubvi_m4
	defptr bench_vrsubvim_m4
	defptr bench_vminuvv_m4
	defptr bench_vminuvvm_m4
	defptr bench_vminuvx_m4
	defptr bench_vminuvxm_m4
	defptr bench_vminvv_m4
	defptr bench_vminvvm_m4
	defptr bench_vminvx_m4
	defptr bench_vminvxm_m4
	defptr bench_vmaxuvv_m4
	defptr bench_vmaxuvvm_m4
	defptr bench_vmaxuvx_m4
	defptr bench_vmaxuvxm_m4
	defptr bench_vmaxvv_m4
	defptr bench_vmaxvvm_m4
	defptr bench_vmaxvx_m4
	defptr bench_vmaxvxm_m4
	defptr bench_vandvv_m4
	defptr bench_vandvvm_m4
	defptr bench_vandvx_m4
	defptr bench_vandvxm_m4
	defptr bench_vandvi_m4
	defptr bench_vandvim_m4
	defptr bench_vorvv_m4
	defptr bench_vorvvm_m4
	defptr bench_vorvx_m4
	defptr bench_vorvxm_m4
	defptr bench_vorvi_m4
	defptr bench_vorvim_m4
	defptr bench_vxorvv_m4
	defptr bench_vxorvvm_m4
	defptr bench_vxorvx_m4
	defptr bench_vxorvxm_m4
	defptr bench_vxorvi_m4
	defptr bench_vxorvim_m4

	defptr bench_vrgathervv_m4
	defptr bench_vrgathervvm_m4
	defptr bench_vrgathervx_m4
	defptr bench_vrgathervxm_m4
	defptr bench_vrgathervi_m4
	defptr bench_vrgathervim_m4
	defptr bench_vslideupvx_m4
	defptr bench_vslideupvxm_m4
	defptr bench_vslideupvi_m4
	defptr bench_vslideupvim_m4
	defptr bench_vrgatherei16vv_m4
	defptr bench_vrgatherei16vvm_m4

	defptr bench_vslidedownvx_m4
	defptr bench_vslidedownvxm_m4
	defptr bench_vslidedownvi_m4
	defptr bench_vslidedownvim_m4

	defptr bench_vredsumvs_m4
	defptr bench_vredsumvsm_m4
	defptr bench_vredandvs_m4
	defptr bench_vredandvsm_m4
	defptr bench_vredorvs_m4
	defptr bench_vredorvsm_m4
	defptr bench_vredxorvs_m4
	defptr bench_vredxorvsm_m4
	defptr bench_vredminuvs_m4
	defptr bench_vredminuvsm_m4
	defptr bench_vredminvs_m4
	defptr bench_vredminvsm_m4
	defptr bench_vredmaxuvs_m4
	defptr bench_vredmaxuvsm_m4
	defptr bench_vredmaxvs_m4
	defptr bench_vredmaxvsm_m4

	defptr bench_vaadduvv_m4
	defptr bench_vaadduvvm_m4
	defptr bench_vaadduvx_m4
	defptr bench_vaadduvxm_m4
	defptr bench_vaaddvv_m4
	defptr bench_vaaddvvm_m4
	defptr bench_vaaddvx_m4
	defptr bench_vaaddvxm_m4
	defptr bench_vasubuvv_m4
	defptr bench_vasubuvvm_m4
	defptr bench_vasubuvx_m4
	defptr bench_vasubuvxm_m4
	defptr bench_vasubvv_m4
	defptr bench_vasubvvm_m4
	defptr bench_vasubvx_m4
	defptr bench_vasubvxm_m4

	defptr bench_vslide1upvx_m4
	defptr bench_vslide1upvxm_m4
	defptr bench_vslide1downvx_m4
	defptr bench_vslide1downvxm_m4

	defptr bench_vadcvvm_m4
	 defptr bench_vadcvxm_m4
	 defptr bench_vadcvim_m4
	defptr bench_vmadcvvm_m4
	 defptr bench_vmadcvxm_m4
	 defptr bench_vmadcvim_m4
	defptr bench_vmadcvv_m4
	defptr bench_vmadcvx_m4
	defptr bench_vmadcvi_m4
	defptr bench_vsbcvvm_m4
	 defptr bench_vsbcvxm_m4
	defptr bench_vmsbcvvm_m4
	 defptr bench_vmsbcvxm_m4
	defptr bench_vmsbcvv_m4
	defptr bench_vmsbcvx_m4

	defptr bench_vmergevvm_m4
	 defptr bench_vmergevxm_m4
	 defptr bench_vmergevim_m4
	defptr bench_vmvvv_m4
	defptr bench_vmvvx_m4
	defptr bench_vmvvi_m4
	defptr bench_vmseqvv_m4
	defptr bench_vmseqvvm_m4
	defptr bench_vmseqvx_m4
	defptr bench_vmseqvxm_m4
	defptr bench_vmseqvi_m4
	defptr bench_vmseqvim_m4
	defptr bench_vmsnevv_m4
	defptr bench_vmsnevvm_m4
	defptr bench_vmsnevx_m4
	defptr bench_vmsnevxm_m4
	defptr bench_vmsnevi_m4
	defptr bench_vmsnevim_m4
	defptr bench_vmsltuvv_m4
	defptr bench_vmsltuvvm_m4
	defptr bench_vmsltuvx_m4
	defptr bench_vmsltuvxm_m4
	defptr bench_vmsltvv_m4
	defptr bench_vmsltvvm_m4
	defptr bench_vmsltvx_m4
	defptr bench_vmsltvxm_m4
	defptr bench_vmsleuvv_m4
	defptr bench_vmsleuvvm_m4
	defptr bench_vmsleuvx_m4
	defptr bench_vmsleuvxm_m4
	defptr bench_vmsleuvi_m4
	defptr bench_vmsleuvim_m4
	defptr bench_vmslevv_m4
	defptr bench_vmslevvm_m4
	defptr bench_vmslevx_m4
	defptr bench_vmslevxm_m4
	defptr bench_vmslevi_m4
	defptr bench_vmslevim_m4
	defptr bench_vmsgtuvx_m4
	defptr bench_vmsgtuvxm_m4
	defptr bench_vmsgtuvi_m4
	defptr bench_vmsgtuvim_m4
	defptr bench_vmsgtvx_m4
	defptr bench_vmsgtvxm_m4
	defptr bench_vmsgtvi_m4
	defptr bench_vmsgtvim_m4

	defptr bench_vcompressvm_m4

	defptr bench_vmandnmm_m4
	defptr bench_vmandmm_m4
	defptr bench_vmormm_m4
	defptr bench_vmxormm_m4
	defptr bench_vmornmm_m4
	defptr bench_vmnandmm_m4
	defptr bench_vmnormm_m4
	defptr bench_vmxnormm_m4

	defptr bench_vsadduvv_m4
	defptr bench_vsadduvvm_m4
	defptr bench_vsadduvx_m4
	defptr bench_vsadduvxm_m4
	defptr bench_vsadduvi_m4
	defptr bench_vsadduvim_m4
	defptr bench_vsaddvv_m4
	defptr bench_vsaddvvm_m4
	defptr bench_vsaddvx_m4
	defptr bench_vsaddvxm_m4
	defptr bench_vsaddvi_m4
	defptr bench_vsaddvim_m4
	defptr bench_vssubuvv_m4
	defptr bench_vssubuvvm_m4
	defptr bench_vssubuvx_m4
	defptr bench_vssubuvxm_m4
	defptr bench_vssubvv_m4
	defptr bench_vssubvvm_m4
	defptr bench_vssubvx_m4
	defptr bench_vssubvxm_m4
	defptr bench_vsllvv_m4
	defptr bench_vsllvvm_m4
	defptr bench_vsllvx_m4
	defptr bench_vsllvxm_m4
	defptr bench_vsllvi_m4
	defptr bench_vsllvim_m4
	defptr bench_vsmulvv_m4
	defptr bench_vsmulvvm_m4
	defptr bench_vsmulvx_m4
	defptr bench_vsmulvxm_m4
	defptr bench_vmv1rv_m4
	defptr bench_vmv2rv_m4
	defptr bench_vmv4rv_m4
	defptr bench_vmv8rv_m4
	defptr bench_vsrlvv_m4
	defptr bench_vsrlvvm_m4
	defptr bench_vsrlvx_m4
	defptr bench_vsrlvxm_m4
	defptr bench_vsrlvi_m4
	defptr bench_vsrlvim_m4
	defptr bench_vsravv_m4
	defptr bench_vsravvm_m4
	defptr bench_vsravx_m4
	defptr bench_vsravxm_m4
	defptr bench_vsravi_m4
	defptr bench_vsravim_m4
	defptr bench_vssrlvv_m4
	defptr bench_vssrlvvm_m4
	defptr bench_vssrlvx_m4
	defptr bench_vssrlvxm_m4
	defptr bench_vssrlvi_m4
	defptr bench_vssrlvim_m4

	defptr bench_vdivuvv_m4
	defptr bench_vdivuvvm_m4
	defptr bench_vdivuvx_m4
	defptr bench_vdivuvxm_m4
	defptr bench_vdivvv_m4
	defptr bench_vdivvvm_m4
	defptr bench_vdivvx_m4
	defptr bench_vdivvxm_m4
	defptr bench_vremuvv_m4
	defptr bench_vremuvvm_m4
	defptr bench_vremuvx_m4
	defptr bench_vremuvxm_m4
	defptr bench_vremvv_m4
	defptr bench_vremvvm_m4
	defptr bench_vremvx_m4
	defptr bench_vremvxm_m4
	defptr bench_vmulhuvv_m4
	defptr bench_vmulhuvvm_m4
	defptr bench_vmulhuvx_m4
	defptr bench_vmulhuvxm_m4
	defptr bench_vmulvv_m4
	defptr bench_vmulvvm_m4
	defptr bench_vmulvx_m4
	defptr bench_vmulvxm_m4
	defptr bench_vmulhsuvv_m4
	defptr bench_vmulhsuvvm_m4
	defptr bench_vmulhsuvx_m4
	defptr bench_vmulhsuvxm_m4
	defptr bench_vmulhvv_m4
	defptr bench_vmulhvvm_m4
	defptr bench_vmulhvx_m4
	defptr bench_vmulhvxm_m4
	defptr bench_vmaddvv_m4
	defptr bench_vmaddvvm_m4
	defptr bench_vmaddvx_m4
	defptr bench_vmaddvxm_m4
	defptr bench_vmaccvv_m4
	defptr bench_vmaccvvm_m4
	defptr bench_vmaccvx_m4
	defptr bench_vmaccvxm_m4

	defptr bench_vnsrlwv_m4
	defptr bench_vnsrlwvm_m4
	defptr bench_vnsrlwx_m4
	defptr bench_vnsrlwxm_m4
	defptr bench_vnsrlwi_m4
	defptr bench_vnsrlwim_m4
	defptr bench_vnsrawv_m4
	defptr bench_vnsrawvm_m4
	defptr bench_vnsrawx_m4
	defptr bench_vnsrawxm_m4
	defptr bench_vnsrawi_m4
	defptr bench_vnsrawim_m4
	defptr bench_vnclipuwv_m4
	defptr bench_vnclipuwvm_m4
	defptr bench_vnclipuwx_m4
	defptr bench_vnclipuwxm_m4
	defptr bench_vnclipuwi_m4
	defptr bench_vnclipuwim_m4
	defptr bench_vnclipwv_m4
	defptr bench_vnclipwvm_m4
	defptr bench_vnclipwx_m4
	defptr bench_vnclipwxm_m4
	defptr bench_vnclipwi_m4
	defptr bench_vnclipwim_m4
	defptr bench_vnmsubvv_m4
	defptr bench_vnmsubvvm_m4
	defptr bench_vnmsubvx_m4
	defptr bench_vnmsubvxm_m4
	defptr bench_vnmsacvv_m4
	defptr bench_vnmsacvvm_m4
	defptr bench_vnmsacvx_m4
	defptr bench_vnmsacvxm_m4

	defptr bench_vwadduvv_m4
	defptr bench_vwadduvvm_m4
	defptr bench_vwadduvx_m4
	defptr bench_vwadduvxm_m4
	defptr bench_vwaddvv_m4
	defptr bench_vwaddvvm_m4
	defptr bench_vwaddvx_m4
	defptr bench_vwaddvxm_m4
	defptr bench_vwsubuvv_m4
	defptr bench_vwsubuvvm_m4
	defptr bench_vwsubuvx_m4
	defptr bench_vwsubuvxm_m4
	defptr bench_vwsubvv_m4
	defptr bench_vwsubvvm_m4
	defptr bench_vwsubvx_m4
	defptr bench_vwsubvxm_m4
	defptr bench_vwadduwv_m4
	defptr bench_vwadduwvm_m4
	defptr bench_vwadduwx_m4
	defptr bench_vwadduwxm_m4
	defptr bench_vwaddwv_m4
	defptr bench_vwaddwvm_m4
	defptr bench_vwaddwx_m4
	defptr bench_vwaddwxm_m4
	defptr bench_vwsubuwv_m4
	defptr bench_vwsubuwvm_m4
	defptr bench_vwsubuwx_m4
	defptr bench_vwsubuwxm_m4
	defptr bench_vwsubwv_m4
	defptr bench_vwsubwvm_m4
	defptr bench_vwsubwx_m4
	defptr bench_vwsubwxm_m4
	defptr bench_vwmuluvv_m4
	defptr bench_vwmuluvvm_m4
	defptr bench_vwmuluvx_m4
	defptr bench_vwmuluvxm_m4
	defptr bench_vwmulsuvv_m4
	defptr bench_vwmulsuvvm_m4
	defptr bench_vwmulsuvx_m4
	defptr bench_vwmulsuvxm_m4
	defptr bench_vwmulvv_m4
	defptr bench_vwmulvvm_m4
	defptr bench_vwmulvx_m4
	defptr bench_vwmulvxm_m4
	defptr bench_vwmaccuvv_m4
	defptr bench_vwmaccuvvm_m4
	defptr bench_vwmaccuvx_m4
	defptr bench_vwmaccuvxm_m4
	defptr bench_vwmaccvv_m4
	defptr bench_vwmaccvvm_m4
	defptr bench_vwmaccvx_m4
	defptr bench_vwmaccvxm_m4
	defptr bench_vwmaccsuvv_m4
	defptr bench_vwmaccsuvvm_m4
	defptr bench_vwmaccsuvx_m4
	defptr bench_vwmaccsuvxm_m4
	defptr bench_vwmaccusvx_m4
	defptr bench_vwmaccusvxm_m4

	defptr bench_vfaddvv_m4
	defptr bench_vfaddvvm_m4
	defptr bench_vfaddvf_m4
	defptr bench_vfaddvfm_m4
	defptr bench_vfsubvv_m4
	defptr bench_vfsubvvm_m4
	defptr bench_vfsubvf_m4
	defptr bench_vfsubvfm_m4
	defptr bench_vfminvv_m4
	defptr bench_vfminvvm_m4
	defptr bench_vfminvf_m4
	defptr bench_vfminvfm_m4
	defptr bench_vfmaxvv_m4
	defptr bench_vfmaxvvm_m4
	defptr bench_vfmaxvf_m4
	defptr bench_vfmaxvfm_m4
	defptr bench_vfsgnjvv_m4
	defptr bench_vfsgnjvvm_m4
	defptr bench_vfsgnjvf_m4
	defptr bench_vfsgnjvfm_m4
	defptr bench_vfsgnjnvv_m4
	defptr bench_vfsgnjnvvm_m4
	defptr bench_vfsgnjnvf_m4
	defptr bench_vfsgnjnvfm_m4
	defptr bench_vfsgnjxvv_m4
	defptr bench_vfsgnjxvvm_m4
	defptr bench_vfsgnjxvf_m4
	defptr bench_vfsgnjxvfm_m4
	defptr bench_vfslide1upvf_m4
	defptr bench_vfslide1upvfm_m4
	defptr bench_vfslide1downvf_m4
	defptr bench_vfslide1downvfm_m4

	defptr bench_vfredusumvs_m4
	defptr bench_vfredusumvsm_m4
	defptr bench_vfredosumvs_m4
	defptr bench_vfredosumvsm_m4
	defptr bench_vfredminvs_m4
	defptr bench_vfredminvsm_m4
	defptr bench_vfredmaxvs_m4
	defptr bench_vfredmaxvsm_m4

	defptr bench_vfmergevfm_m4
	defptr bench_vfmvvf_m4

	defptr bench_vmfeqvv_m4
	defptr bench_vmfeqvvm_m4
	defptr bench_vmfeqvf_m4
	defptr bench_vmfeqvfm_m4
	defptr bench_vmflevv_m4
	defptr bench_vmflevvm_m4
	defptr bench_vmflevf_m4
	defptr bench_vmflevfm_m4
	defptr bench_vmfltvv_m4
	defptr bench_vmfltvvm_m4
	defptr bench_vmfltvf_m4
	defptr bench_vmfltvfm_m4
	defptr bench_vmfnevv_m4
	defptr bench_vmfnevvm_m4
	defptr bench_vmfnevf_m4
	defptr bench_vmfnevfm_m4
	defptr bench_vmfgtvv_m4
	defptr bench_vmfgtvvm_m4
	defptr bench_vmfgtvf_m4
	defptr bench_vmfgtvfm_m4
	defptr bench_vmfgevv_m4
	defptr bench_vmfgevvm_m4
	defptr bench_vmfgevf_m4
	defptr bench_vmfgevfm_m4

	defptr bench_vfdivvv_m4
	defptr bench_vfdivvvm_m4
	defptr bench_vfdivvf_m4
	defptr bench_vfdivvfm_m4
	defptr bench_vfrdivvf_m4
	defptr bench_vfrdivvfm_m4
	defptr bench_vfmulvv_m4
	defptr bench_vfmulvvm_m4
	defptr bench_vfmulvf_m4
	defptr bench_vfmulvfm_m4
	defptr bench_vfrsubvf_m4
	defptr bench_vfrsubvfm_m4
	defptr bench_vfmaddvv_m4
	defptr bench_vfmaddvvm_m4
	defptr bench_vfmaddvf_m4
	defptr bench_vfmaddvfm_m4
	defptr bench_vfmsubvv_m4
	defptr bench_vfmsubvvm_m4
	defptr bench_vfmsubvf_m4
	defptr bench_vfmsubvfm_m4
	defptr bench_vfmaccvv_m4
	defptr bench_vfmaccvvm_m4
	defptr bench_vfmaccvf_m4
	defptr bench_vfmaccvfm_m4
	defptr bench_vfmsacvv_m4
	defptr bench_vfmsacvvm_m4
	defptr bench_vfmsacvf_m4
	defptr bench_vfmsacvfm_m4

	defptr bench_vfnmsacvv_m4
	defptr bench_vfnmsacvvm_m4
	defptr bench_vfnmsacvf_m4
	defptr bench_vfnmsacvfm_m4
	defptr bench_vfnmaccvv_m4
	defptr bench_vfnmaccvvm_m4
	defptr bench_vfnmaccvf_m4
	defptr bench_vfnmaccvfm_m4
	defptr bench_vfnmsubvv_m4
	defptr bench_vfnmsubvvm_m4
	defptr bench_vfnmsubvf_m4
	defptr bench_vfnmsubvfm_m4
	defptr bench_vfnmaddvv_m4
	defptr bench_vfnmaddvvm_m4
	defptr bench_vfnmaddvf_m4
	defptr bench_vfnmaddvfm_m4

	defptr bench_vwredsumuvs_m4
	defptr bench_vwredsumuvsm_m4
	defptr bench_vwredsumvs_m4
	defptr bench_vwredsumvsm_m4

	defptr bench_vfwaddvv_m4
	defptr bench_vfwaddvvm_m4
	defptr bench_vfwaddvf_m4
	defptr bench_vfwaddvfm_m4
	defptr bench_vfwsubvv_m4
	defptr bench_vfwsubvvm_m4
	defptr bench_vfwsubvf_m4
	defptr bench_vfwsubvfm_m4
	defptr bench_vfwaddwv_m4
	defptr bench_vfwaddwvm_m4
	defptr bench_vfwaddwf_m4
	defptr bench_vfwaddwfm_m4
	defptr bench_vfwsubwv_m4
	defptr bench_vfwsubwvm_m4
	defptr bench_vfwsubwf_m4
	defptr bench_vfwsubwfm_m4
	defptr bench_vfwmulvv_m4
	defptr bench_vfwmulvvm_m4
	defptr bench_vfwmulvf_m4
	defptr bench_vfwmulvfm_m4
	defptr bench_vfwmaccvv_m4
	defptr bench_vfwmaccvvm_m4
	defptr bench_vfwmaccvf_m4
	defptr bench_vfwmaccvfm_m4
	defptr bench_vfwnmaccvv_m4
	defptr bench_vfwnmaccvvm_m4
	defptr bench_vfwnmaccvf_m4
	defptr bench_vfwnmaccvfm_m4
	defptr bench_vfwmsacvv_m4
	defptr bench_vfwmsacvvm_m4
	defptr bench_vfwmsacvf_m4
	defptr bench_vfwmsacvfm_m4
	defptr bench_vfwnmsacvv_m4
	defptr bench_vfwnmsacvvm_m4
	defptr bench_vfwnmsacvf_m4
	defptr bench_vfwnmsacvfm_m4

	defptr bench_vfwredosumvs_m4
	defptr bench_vfwredosumvsm_m4
	defptr bench_vfwredusumvs_m4
	defptr bench_vfwredusumvsm_m4

	defptr bench_vmvsx_m4
	defptr bench_vmvxs_m4

	defptr bench_vcpopm_m4
	defptr bench_vcpopmm_m4
	defptr bench_vfirstm_m4
	defptr bench_vfirstmm_m4
	defptr bench_vzextvf2_m4
	defptr bench_vzextvf2m_m4
	defptr bench_vsextvf2_m4
	defptr bench_vsextvf2m_m4
	defptr bench_vzextvf4_m4
	defptr bench_vzextvf4m_m4
	defptr bench_vsextvf4_m4
	defptr bench_vsextvf4m_m4
	defptr bench_vzextvf8_m4
	defptr bench_vzextvf8m_m4
	defptr bench_vsextvf8_m4
	defptr bench_vsextvf8m_m4

	defptr bench_vfmvfs_m4
	defptr bench_vfmvsf_m4

	defptr bench_vfcvtxufv_m4
	defptr bench_vfcvtxufvm_m4
	defptr bench_vfcvtxfv_m4
	defptr bench_vfcvtxfvm_m4
	defptr bench_vfcvtfxuv_m4
	defptr bench_vfcvtfxuvm_m4
	defptr bench_vfcvtfxv_m4
	defptr bench_vfcvtfxvm_m4
	defptr bench_vfcvtrtzxfv_m4
	defptr bench_vfcvtrtzxfvm_m4
	defptr bench_vfcvtrtzxufv_m4
	defptr bench_vfcvtrtzxufvm_m4

	defptr bench_vfwcvtxufv_m4
	defptr bench_vfwcvtxufvm_m4
	defptr bench_vfwcvtxfv_m4
	defptr bench_vfwcvtxfvm_m4
	defptr bench_vfwcvtfxuv_m4
	defptr bench_vfwcvtfxuvm_m4
	defptr bench_vfwcvtfxv_m4
	defptr bench_vfwcvtfxvm_m4
	defptr bench_vfwcvtffv_m4
	defptr bench_vfwcvtffvm_m4
	defptr bench_vfwcvtrtzxufv_m4
	defptr bench_vfwcvtrtzxufvm_m4
	defptr bench_vfwcvtrtzxfv_m4
	defptr bench_vfwcvtrtzxfvm_m4

	defptr bench_vfncvtxufw_m4
	defptr bench_vfncvtxufwm_m4
	defptr bench_vfncvtxfw_m4
	defptr bench_vfncvtxfwm_m4
	defptr bench_vfncvtfxuw_m4
	defptr bench_vfncvtfxuwm_m4
	defptr bench_vfncvtfxw_m4
	defptr bench_vfncvtfxwm_m4
	defptr bench_vfncvtffw_m4
	defptr bench_vfncvtffwm_m4
	defptr bench_vfncvtrtzxfw_m4
	defptr bench_vfncvtrtzxfwm_m4
	defptr bench_vfncvtrtzxufw_m4
	defptr bench_vfncvtrtzxufwm_m4
	defptr bench_vfncvt.rod.f.f.w_m4
	defptr bench_vfncvt.rod.f.f.wm_m4

	defptr bench_vfsqrtv_m4
	defptr bench_vfsqrtvm_m4
	defptr bench_vfrsqrt7v_m4
	defptr bench_vfrsqrt7vm_m4
	defptr bench_vfrec7v_m4
	defptr bench_vfrec7vm_m4
	defptr bench_vfclassv_m4
	defptr bench_vfclassvm_m4

	defptr bench_vmsbfm_m4
	defptr bench_vmsbfmm_m4
	defptr bench_vmsofm_m4
	defptr bench_vmsofmm_m4
	defptr bench_vmsifm_m4
	defptr bench_vmsifmm_m4
	defptr bench_viotam_m4
	defptr bench_viotamm_m4
	defptr bench_vidv_m4
	defptr bench_vidvm_m4



.balign 8


.global bench_m8
	bench_m8:

	defptr bench_add_m8
	defptr bench_mul_m8
	defptr bench_vaddvv_m8
	defptr bench_vaddvvm_m8
	defptr bench_vaddvx_m8
	defptr bench_vaddvxm_m8
	defptr bench_vaddvi_m8
	defptr bench_vaddvim_m8
	defptr bench_vsubvv_m8
	defptr bench_vsubvvm_m8
	defptr bench_vsubvx_m8
	defptr bench_vsubvxm_m8
	defptr bench_vrsubvx_m8
	defptr bench_vrsubvxm_m8
	defptr bench_vrsubvi_m8
	defptr bench_vrsubvim_m8
	defptr bench_vminuvv_m8
	defptr bench_vminuvvm_m8
	defptr bench_vminuvx_m8
	defptr bench_vminuvxm_m8
	defptr bench_vminvv_m8
	defptr bench_vminvvm_m8
	defptr bench_vminvx_m8
	defptr bench_vminvxm_m8
	defptr bench_vmaxuvv_m8
	defptr bench_vmaxuvvm_m8
	defptr bench_vmaxuvx_m8
	defptr bench_vmaxuvxm_m8
	defptr bench_vmaxvv_m8
	defptr bench_vmaxvvm_m8
	defptr bench_vmaxvx_m8
	defptr bench_vmaxvxm_m8
	defptr bench_vandvv_m8
	defptr bench_vandvvm_m8
	defptr bench_vandvx_m8
	defptr bench_vandvxm_m8
	defptr bench_vandvi_m8
	defptr bench_vandvim_m8
	defptr bench_vorvv_m8
	defptr bench_vorvvm_m8
	defptr bench_vorvx_m8
	defptr bench_vorvxm_m8
	defptr bench_vorvi_m8
	defptr bench_vorvim_m8
	defptr bench_vxorvv_m8
	defptr bench_vxorvvm_m8
	defptr bench_vxorvx_m8
	defptr bench_vxorvxm_m8
	defptr bench_vxorvi_m8
	defptr bench_vxorvim_m8

	defptr bench_vrgathervv_m8
	defptr bench_vrgathervvm_m8
	defptr bench_vrgathervx_m8
	defptr bench_vrgathervxm_m8
	defptr bench_vrgathervi_m8
	defptr bench_vrgathervim_m8
	defptr bench_vslideupvx_m8
	defptr bench_vslideupvxm_m8
	defptr bench_vslideupvi_m8
	defptr bench_vslideupvim_m8
	defptr bench_vrgatherei16vv_m8
	defptr bench_vrgatherei16vvm_m8

	defptr bench_vslidedownvx_m8
	defptr bench_vslidedownvxm_m8
	defptr bench_vslidedownvi_m8
	defptr bench_vslidedownvim_m8

	defptr bench_vredsumvs_m8
	defptr bench_vredsumvsm_m8
	defptr bench_vredandvs_m8
	defptr bench_vredandvsm_m8
	defptr bench_vredorvs_m8
	defptr bench_vredorvsm_m8
	defptr bench_vredxorvs_m8
	defptr bench_vredxorvsm_m8
	defptr bench_vredminuvs_m8
	defptr bench_vredminuvsm_m8
	defptr bench_vredminvs_m8
	defptr bench_vredminvsm_m8
	defptr bench_vredmaxuvs_m8
	defptr bench_vredmaxuvsm_m8
	defptr bench_vredmaxvs_m8
	defptr bench_vredmaxvsm_m8

	defptr bench_vaadduvv_m8
	defptr bench_vaadduvvm_m8
	defptr bench_vaadduvx_m8
	defptr bench_vaadduvxm_m8
	defptr bench_vaaddvv_m8
	defptr bench_vaaddvvm_m8
	defptr bench_vaaddvx_m8
	defptr bench_vaaddvxm_m8
	defptr bench_vasubuvv_m8
	defptr bench_vasubuvvm_m8
	defptr bench_vasubuvx_m8
	defptr bench_vasubuvxm_m8
	defptr bench_vasubvv_m8
	defptr bench_vasubvvm_m8
	defptr bench_vasubvx_m8
	defptr bench_vasubvxm_m8

	defptr bench_vslide1upvx_m8
	defptr bench_vslide1upvxm_m8
	defptr bench_vslide1downvx_m8
	defptr bench_vslide1downvxm_m8

	defptr bench_vadcvvm_m8
	 defptr bench_vadcvxm_m8
	 defptr bench_vadcvim_m8
	defptr bench_vmadcvvm_m8
	 defptr bench_vmadcvxm_m8
	 defptr bench_vmadcvim_m8
	defptr bench_vmadcvv_m8
	defptr bench_vmadcvx_m8
	defptr bench_vmadcvi_m8
	defptr bench_vsbcvvm_m8
	 defptr bench_vsbcvxm_m8
	defptr bench_vmsbcvvm_m8
	 defptr bench_vmsbcvxm_m8
	defptr bench_vmsbcvv_m8
	defptr bench_vmsbcvx_m8

	defptr bench_vmergevvm_m8
	 defptr bench_vmergevxm_m8
	 defptr bench_vmergevim_m8
	defptr bench_vmvvv_m8
	defptr bench_vmvvx_m8
	defptr bench_vmvvi_m8
	defptr bench_vmseqvv_m8
	defptr bench_vmseqvvm_m8
	defptr bench_vmseqvx_m8
	defptr bench_vmseqvxm_m8
	defptr bench_vmseqvi_m8
	defptr bench_vmseqvim_m8
	defptr bench_vmsnevv_m8
	defptr bench_vmsnevvm_m8
	defptr bench_vmsnevx_m8
	defptr bench_vmsnevxm_m8
	defptr bench_vmsnevi_m8
	defptr bench_vmsnevim_m8
	defptr bench_vmsltuvv_m8
	defptr bench_vmsltuvvm_m8
	defptr bench_vmsltuvx_m8
	defptr bench_vmsltuvxm_m8
	defptr bench_vmsltvv_m8
	defptr bench_vmsltvvm_m8
	defptr bench_vmsltvx_m8
	defptr bench_vmsltvxm_m8
	defptr bench_vmsleuvv_m8
	defptr bench_vmsleuvvm_m8
	defptr bench_vmsleuvx_m8
	defptr bench_vmsleuvxm_m8
	defptr bench_vmsleuvi_m8
	defptr bench_vmsleuvim_m8
	defptr bench_vmslevv_m8
	defptr bench_vmslevvm_m8
	defptr bench_vmslevx_m8
	defptr bench_vmslevxm_m8
	defptr bench_vmslevi_m8
	defptr bench_vmslevim_m8
	defptr bench_vmsgtuvx_m8
	defptr bench_vmsgtuvxm_m8
	defptr bench_vmsgtuvi_m8
	defptr bench_vmsgtuvim_m8
	defptr bench_vmsgtvx_m8
	defptr bench_vmsgtvxm_m8
	defptr bench_vmsgtvi_m8
	defptr bench_vmsgtvim_m8

	defptr bench_vcompressvm_m8

	defptr bench_vmandnmm_m8
	defptr bench_vmandmm_m8
	defptr bench_vmormm_m8
	defptr bench_vmxormm_m8
	defptr bench_vmornmm_m8
	defptr bench_vmnandmm_m8
	defptr bench_vmnormm_m8
	defptr bench_vmxnormm_m8

	defptr bench_vsadduvv_m8
	defptr bench_vsadduvvm_m8
	defptr bench_vsadduvx_m8
	defptr bench_vsadduvxm_m8
	defptr bench_vsadduvi_m8
	defptr bench_vsadduvim_m8
	defptr bench_vsaddvv_m8
	defptr bench_vsaddvvm_m8
	defptr bench_vsaddvx_m8
	defptr bench_vsaddvxm_m8
	defptr bench_vsaddvi_m8
	defptr bench_vsaddvim_m8
	defptr bench_vssubuvv_m8
	defptr bench_vssubuvvm_m8
	defptr bench_vssubuvx_m8
	defptr bench_vssubuvxm_m8
	defptr bench_vssubvv_m8
	defptr bench_vssubvvm_m8
	defptr bench_vssubvx_m8
	defptr bench_vssubvxm_m8
	defptr bench_vsllvv_m8
	defptr bench_vsllvvm_m8
	defptr bench_vsllvx_m8
	defptr bench_vsllvxm_m8
	defptr bench_vsllvi_m8
	defptr bench_vsllvim_m8
	defptr bench_vsmulvv_m8
	defptr bench_vsmulvvm_m8
	defptr bench_vsmulvx_m8
	defptr bench_vsmulvxm_m8
	defptr bench_vmv1rv_m8
	defptr bench_vmv2rv_m8
	defptr bench_vmv4rv_m8
	defptr bench_vmv8rv_m8
	defptr bench_vsrlvv_m8
	defptr bench_vsrlvvm_m8
	defptr bench_vsrlvx_m8
	defptr bench_vsrlvxm_m8
	defptr bench_vsrlvi_m8
	defptr bench_vsrlvim_m8
	defptr bench_vsravv_m8
	defptr bench_vsravvm_m8
	defptr bench_vsravx_m8
	defptr bench_vsravxm_m8
	defptr bench_vsravi_m8
	defptr bench_vsravim_m8
	defptr bench_vssrlvv_m8
	defptr bench_vssrlvvm_m8
	defptr bench_vssrlvx_m8
	defptr bench_vssrlvxm_m8
	defptr bench_vssrlvi_m8
	defptr bench_vssrlvim_m8

	defptr bench_vdivuvv_m8
	defptr bench_vdivuvvm_m8
	defptr bench_vdivuvx_m8
	defptr bench_vdivuvxm_m8
	defptr bench_vdivvv_m8
	defptr bench_vdivvvm_m8
	defptr bench_vdivvx_m8
	defptr bench_vdivvxm_m8
	defptr bench_vremuvv_m8
	defptr bench_vremuvvm_m8
	defptr bench_vremuvx_m8
	defptr bench_vremuvxm_m8
	defptr bench_vremvv_m8
	defptr bench_vremvvm_m8
	defptr bench_vremvx_m8
	defptr bench_vremvxm_m8
	defptr bench_vmulhuvv_m8
	defptr bench_vmulhuvvm_m8
	defptr bench_vmulhuvx_m8
	defptr bench_vmulhuvxm_m8
	defptr bench_vmulvv_m8
	defptr bench_vmulvvm_m8
	defptr bench_vmulvx_m8
	defptr bench_vmulvxm_m8
	defptr bench_vmulhsuvv_m8
	defptr bench_vmulhsuvvm_m8
	defptr bench_vmulhsuvx_m8
	defptr bench_vmulhsuvxm_m8
	defptr bench_vmulhvv_m8
	defptr bench_vmulhvvm_m8
	defptr bench_vmulhvx_m8
	defptr bench_vmulhvxm_m8
	defptr bench_vmaddvv_m8
	defptr bench_vmaddvvm_m8
	defptr bench_vmaddvx_m8
	defptr bench_vmaddvxm_m8
	defptr bench_vmaccvv_m8
	defptr bench_vmaccvvm_m8
	defptr bench_vmaccvx_m8
	defptr bench_vmaccvxm_m8

	defptr bench_vnsrlwv_m8
	defptr bench_vnsrlwvm_m8
	defptr bench_vnsrlwx_m8
	defptr bench_vnsrlwxm_m8
	defptr bench_vnsrlwi_m8
	defptr bench_vnsrlwim_m8
	defptr bench_vnsrawv_m8
	defptr bench_vnsrawvm_m8
	defptr bench_vnsrawx_m8
	defptr bench_vnsrawxm_m8
	defptr bench_vnsrawi_m8
	defptr bench_vnsrawim_m8
	defptr bench_vnclipuwv_m8
	defptr bench_vnclipuwvm_m8
	defptr bench_vnclipuwx_m8
	defptr bench_vnclipuwxm_m8
	defptr bench_vnclipuwi_m8
	defptr bench_vnclipuwim_m8
	defptr bench_vnclipwv_m8
	defptr bench_vnclipwvm_m8
	defptr bench_vnclipwx_m8
	defptr bench_vnclipwxm_m8
	defptr bench_vnclipwi_m8
	defptr bench_vnclipwim_m8
	defptr bench_vnmsubvv_m8
	defptr bench_vnmsubvvm_m8
	defptr bench_vnmsubvx_m8
	defptr bench_vnmsubvxm_m8
	defptr bench_vnmsacvv_m8
	defptr bench_vnmsacvvm_m8
	defptr bench_vnmsacvx_m8
	defptr bench_vnmsacvxm_m8

	defptr bench_vwadduvv_m8
	defptr bench_vwadduvvm_m8
	defptr bench_vwadduvx_m8
	defptr bench_vwadduvxm_m8
	defptr bench_vwaddvv_m8
	defptr bench_vwaddvvm_m8
	defptr bench_vwaddvx_m8
	defptr bench_vwaddvxm_m8
	defptr bench_vwsubuvv_m8
	defptr bench_vwsubuvvm_m8
	defptr bench_vwsubuvx_m8
	defptr bench_vwsubuvxm_m8
	defptr bench_vwsubvv_m8
	defptr bench_vwsubvvm_m8
	defptr bench_vwsubvx_m8
	defptr bench_vwsubvxm_m8
	defptr bench_vwadduwv_m8
	defptr bench_vwadduwvm_m8
	defptr bench_vwadduwx_m8
	defptr bench_vwadduwxm_m8
	defptr bench_vwaddwv_m8
	defptr bench_vwaddwvm_m8
	defptr bench_vwaddwx_m8
	defptr bench_vwaddwxm_m8
	defptr bench_vwsubuwv_m8
	defptr bench_vwsubuwvm_m8
	defptr bench_vwsubuwx_m8
	defptr bench_vwsubuwxm_m8
	defptr bench_vwsubwv_m8
	defptr bench_vwsubwvm_m8
	defptr bench_vwsubwx_m8
	defptr bench_vwsubwxm_m8
	defptr bench_vwmuluvv_m8
	defptr bench_vwmuluvvm_m8
	defptr bench_vwmuluvx_m8
	defptr bench_vwmuluvxm_m8
	defptr bench_vwmulsuvv_m8
	defptr bench_vwmulsuvvm_m8
	defptr bench_vwmulsuvx_m8
	defptr bench_vwmulsuvxm_m8
	defptr bench_vwmulvv_m8
	defptr bench_vwmulvvm_m8
	defptr bench_vwmulvx_m8
	defptr bench_vwmulvxm_m8
	defptr bench_vwmaccuvv_m8
	defptr bench_vwmaccuvvm_m8
	defptr bench_vwmaccuvx_m8
	defptr bench_vwmaccuvxm_m8
	defptr bench_vwmaccvv_m8
	defptr bench_vwmaccvvm_m8
	defptr bench_vwmaccvx_m8
	defptr bench_vwmaccvxm_m8
	defptr bench_vwmaccsuvv_m8
	defptr bench_vwmaccsuvvm_m8
	defptr bench_vwmaccsuvx_m8
	defptr bench_vwmaccsuvxm_m8
	defptr bench_vwmaccusvx_m8
	defptr bench_vwmaccusvxm_m8

	defptr bench_vfaddvv_m8
	defptr bench_vfaddvvm_m8
	defptr bench_vfaddvf_m8
	defptr bench_vfaddvfm_m8
	defptr bench_vfsubvv_m8
	defptr bench_vfsubvvm_m8
	defptr bench_vfsubvf_m8
	defptr bench_vfsubvfm_m8
	defptr bench_vfminvv_m8
	defptr bench_vfminvvm_m8
	defptr bench_vfminvf_m8
	defptr bench_vfminvfm_m8
	defptr bench_vfmaxvv_m8
	defptr bench_vfmaxvvm_m8
	defptr bench_vfmaxvf_m8
	defptr bench_vfmaxvfm_m8
	defptr bench_vfsgnjvv_m8
	defptr bench_vfsgnjvvm_m8
	defptr bench_vfsgnjvf_m8
	defptr bench_vfsgnjvfm_m8
	defptr bench_vfsgnjnvv_m8
	defptr bench_vfsgnjnvvm_m8
	defptr bench_vfsgnjnvf_m8
	defptr bench_vfsgnjnvfm_m8
	defptr bench_vfsgnjxvv_m8
	defptr bench_vfsgnjxvvm_m8
	defptr bench_vfsgnjxvf_m8
	defptr bench_vfsgnjxvfm_m8
	defptr bench_vfslide1upvf_m8
	defptr bench_vfslide1upvfm_m8
	defptr bench_vfslide1downvf_m8
	defptr bench_vfslide1downvfm_m8

	defptr bench_vfredusumvs_m8
	defptr bench_vfredusumvsm_m8
	defptr bench_vfredosumvs_m8
	defptr bench_vfredosumvsm_m8
	defptr bench_vfredminvs_m8
	defptr bench_vfredminvsm_m8
	defptr bench_vfredmaxvs_m8
	defptr bench_vfredmaxvsm_m8

	defptr bench_vfmergevfm_m8
	defptr bench_vfmvvf_m8

	defptr bench_vmfeqvv_m8
	defptr bench_vmfeqvvm_m8
	defptr bench_vmfeqvf_m8
	defptr bench_vmfeqvfm_m8
	defptr bench_vmflevv_m8
	defptr bench_vmflevvm_m8
	defptr bench_vmflevf_m8
	defptr bench_vmflevfm_m8
	defptr bench_vmfltvv_m8
	defptr bench_vmfltvvm_m8
	defptr bench_vmfltvf_m8
	defptr bench_vmfltvfm_m8
	defptr bench_vmfnevv_m8
	defptr bench_vmfnevvm_m8
	defptr bench_vmfnevf_m8
	defptr bench_vmfnevfm_m8
	defptr bench_vmfgtvv_m8
	defptr bench_vmfgtvvm_m8
	defptr bench_vmfgtvf_m8
	defptr bench_vmfgtvfm_m8
	defptr bench_vmfgevv_m8
	defptr bench_vmfgevvm_m8
	defptr bench_vmfgevf_m8
	defptr bench_vmfgevfm_m8

	defptr bench_vfdivvv_m8
	defptr bench_vfdivvvm_m8
	defptr bench_vfdivvf_m8
	defptr bench_vfdivvfm_m8
	defptr bench_vfrdivvf_m8
	defptr bench_vfrdivvfm_m8
	defptr bench_vfmulvv_m8
	defptr bench_vfmulvvm_m8
	defptr bench_vfmulvf_m8
	defptr bench_vfmulvfm_m8
	defptr bench_vfrsubvf_m8
	defptr bench_vfrsubvfm_m8
	defptr bench_vfmaddvv_m8
	defptr bench_vfmaddvvm_m8
	defptr bench_vfmaddvf_m8
	defptr bench_vfmaddvfm_m8
	defptr bench_vfmsubvv_m8
	defptr bench_vfmsubvvm_m8
	defptr bench_vfmsubvf_m8
	defptr bench_vfmsubvfm_m8
	defptr bench_vfmaccvv_m8
	defptr bench_vfmaccvvm_m8
	defptr bench_vfmaccvf_m8
	defptr bench_vfmaccvfm_m8
	defptr bench_vfmsacvv_m8
	defptr bench_vfmsacvvm_m8
	defptr bench_vfmsacvf_m8
	defptr bench_vfmsacvfm_m8

	defptr bench_vfnmsacvv_m8
	defptr bench_vfnmsacvvm_m8
	defptr bench_vfnmsacvf_m8
	defptr bench_vfnmsacvfm_m8
	defptr bench_vfnmaccvv_m8
	defptr bench_vfnmaccvvm_m8
	defptr bench_vfnmaccvf_m8
	defptr bench_vfnmaccvfm_m8
	defptr bench_vfnmsubvv_m8
	defptr bench_vfnmsubvvm_m8
	defptr bench_vfnmsubvf_m8
	defptr bench_vfnmsubvfm_m8
	defptr bench_vfnmaddvv_m8
	defptr bench_vfnmaddvvm_m8
	defptr bench_vfnmaddvf_m8
	defptr bench_vfnmaddvfm_m8

	defptr bench_vwredsumuvs_m8
	defptr bench_vwredsumuvsm_m8
	defptr bench_vwredsumvs_m8
	defptr bench_vwredsumvsm_m8

	defptr bench_vfwaddvv_m8
	defptr bench_vfwaddvvm_m8
	defptr bench_vfwaddvf_m8
	defptr bench_vfwaddvfm_m8
	defptr bench_vfwsubvv_m8
	defptr bench_vfwsubvvm_m8
	defptr bench_vfwsubvf_m8
	defptr bench_vfwsubvfm_m8
	defptr bench_vfwaddwv_m8
	defptr bench_vfwaddwvm_m8
	defptr bench_vfwaddwf_m8
	defptr bench_vfwaddwfm_m8
	defptr bench_vfwsubwv_m8
	defptr bench_vfwsubwvm_m8
	defptr bench_vfwsubwf_m8
	defptr bench_vfwsubwfm_m8
	defptr bench_vfwmulvv_m8
	defptr bench_vfwmulvvm_m8
	defptr bench_vfwmulvf_m8
	defptr bench_vfwmulvfm_m8
	defptr bench_vfwmaccvv_m8
	defptr bench_vfwmaccvvm_m8
	defptr bench_vfwmaccvf_m8
	defptr bench_vfwmaccvfm_m8
	defptr bench_vfwnmaccvv_m8
	defptr bench_vfwnmaccvvm_m8
	defptr bench_vfwnmaccvf_m8
	defptr bench_vfwnmaccvfm_m8
	defptr bench_vfwmsacvv_m8
	defptr bench_vfwmsacvvm_m8
	defptr bench_vfwmsacvf_m8
	defptr bench_vfwmsacvfm_m8
	defptr bench_vfwnmsacvv_m8
	defptr bench_vfwnmsacvvm_m8
	defptr bench_vfwnmsacvf_m8
	defptr bench_vfwnmsacvfm_m8

	defptr bench_vfwredosumvs_m8
	defptr bench_vfwredosumvsm_m8
	defptr bench_vfwredusumvs_m8
	defptr bench_vfwredusumvsm_m8

	defptr bench_vmvsx_m8
	defptr bench_vmvxs_m8

	defptr bench_vcpopm_m8
	defptr bench_vcpopmm_m8
	defptr bench_vfirstm_m8
	defptr bench_vfirstmm_m8
	defptr bench_vzextvf2_m8
	defptr bench_vzextvf2m_m8
	defptr bench_vsextvf2_m8
	defptr bench_vsextvf2m_m8
	defptr bench_vzextvf4_m8
	defptr bench_vzextvf4m_m8
	defptr bench_vsextvf4_m8
	defptr bench_vsextvf4m_m8
	defptr bench_vzextvf8_m8
	defptr bench_vzextvf8m_m8
	defptr bench_vsextvf8_m8
	defptr bench_vsextvf8m_m8

	defptr bench_vfmvfs_m8
	defptr bench_vfmvsf_m8

	defptr bench_vfcvtxufv_m8
	defptr bench_vfcvtxufvm_m8
	defptr bench_vfcvtxfv_m8
	defptr bench_vfcvtxfvm_m8
	defptr bench_vfcvtfxuv_m8
	defptr bench_vfcvtfxuvm_m8
	defptr bench_vfcvtfxv_m8
	defptr bench_vfcvtfxvm_m8
	defptr bench_vfcvtrtzxfv_m8
	defptr bench_vfcvtrtzxfvm_m8
	defptr bench_vfcvtrtzxufv_m8
	defptr bench_vfcvtrtzxufvm_m8

	defptr bench_vfwcvtxufv_m8
	defptr bench_vfwcvtxufvm_m8
	defptr bench_vfwcvtxfv_m8
	defptr bench_vfwcvtxfvm_m8
	defptr bench_vfwcvtfxuv_m8
	defptr bench_vfwcvtfxuvm_m8
	defptr bench_vfwcvtfxv_m8
	defptr bench_vfwcvtfxvm_m8
	defptr bench_vfwcvtffv_m8
	defptr bench_vfwcvtffvm_m8
	defptr bench_vfwcvtrtzxufv_m8
	defptr bench_vfwcvtrtzxufvm_m8
	defptr bench_vfwcvtrtzxfv_m8
	defptr bench_vfwcvtrtzxfvm_m8

	defptr bench_vfncvtxufw_m8
	defptr bench_vfncvtxufwm_m8
	defptr bench_vfncvtxfw_m8
	defptr bench_vfncvtxfwm_m8
	defptr bench_vfncvtfxuw_m8
	defptr bench_vfncvtfxuwm_m8
	defptr bench_vfncvtfxw_m8
	defptr bench_vfncvtfxwm_m8
	defptr bench_vfncvtffw_m8
	defptr bench_vfncvtffwm_m8
	defptr bench_vfncvtrtzxfw_m8
	defptr bench_vfncvtrtzxfwm_m8
	defptr bench_vfncvtrtzxufw_m8
	defptr bench_vfncvtrtzxufwm_m8
	defptr bench_vfncvt.rod.f.f.w_m8
	defptr bench_vfncvt.rod.f.f.wm_m8

	defptr bench_vfsqrtv_m8
	defptr bench_vfsqrtvm_m8
	defptr bench_vfrsqrt7v_m8
	defptr bench_vfrsqrt7vm_m8
	defptr bench_vfrec7v_m8
	defptr bench_vfrec7vm_m8
	defptr bench_vfclassv_m8
	defptr bench_vfclassvm_m8

	defptr bench_vmsbfm_m8
	defptr bench_vmsbfmm_m8
	defptr bench_vmsofm_m8
	defptr bench_vmsofmm_m8
	defptr bench_vmsifm_m8
	defptr bench_vmsifmm_m8
	defptr bench_viotam_m8
	defptr bench_viotamm_m8
	defptr bench_vidv_m8
	defptr bench_vidvm_m8





.balign 8
.global bench_types
	bench_types:

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_ei16
	defptr T_ei16

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	 defptr T_A
	 defptr T_A
	defptr T_A
	 defptr T_A
	 defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	 defptr T_A
	defptr T_A
	 defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	 defptr T_A
	 defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A

	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A

	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N
	defptr T_N

	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W
	defptr T_W

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_F
	defptr T_F

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN

	defptr T_WR
	defptr T_WR
	defptr T_WR
	defptr T_WR

	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW

	defptr T_FWR
	defptr T_FWR
	defptr T_FWR
	defptr T_FWR

	defptr T_A
	defptr T_A

	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_m1
	defptr T_E2
	defptr T_E2
	defptr T_E2
	defptr T_E2
	defptr T_E4
	defptr T_E4
	defptr T_E4
	defptr T_E4
	defptr T_E8
	defptr T_E8
	defptr T_E8
	defptr T_E8

	defptr T_F
	defptr T_F

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW
	defptr T_FW

	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN
	defptr T_FN

	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F
	defptr T_F

	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A
	defptr T_A



.balign 8
.global bench_names
	bench_names:

	.string "add t0,t1,t2"
	.string "mul t0,t1,t2"
	.string "vadd.vv v8,v16,v24"
	.string "vadd.vv v8,v16,v24,v0.t"
	.string "vadd.vx v8,v16,t0"
	.string "vadd.vx v8,v16,t0,v0.t"
	.string "vadd.vi v8,v16,13"
	.string "vadd.vi v8,v16,13,v0.t"
	.string "vsub.vv v8,v16,v24"
	.string "vsub.vv v8,v16,v24,v0.t"
	.string "vsub.vx v8,v16,t0"
	.string "vsub.vx v8,v16,t0,v0.t"
	.string "vrsub.vx v8,v16,t0"
	.string "vrsub.vx v8,v16,t0,v0.t"
	.string "vrsub.vi v8,v16,13"
	.string "vrsub.vi v8,v16,13,v0.t"
	.string "vminu.vv v8,v16,v24"
	.string "vminu.vv v8,v16,v24,v0.t"
	.string "vminu.vx v8,v16,t0"
	.string "vminu.vx v8,v16,t0,v0.t"
	.string "vmin.vv v8,v16,v24"
	.string "vmin.vv v8,v16,v24,v0.t"
	.string "vmin.vx v8,v16,t0"
	.string "vmin.vx v8,v16,t0,v0.t"
	.string "vmaxu.vv v8,v16,v24"
	.string "vmaxu.vv v8,v16,v24,v0.t"
	.string "vmaxu.vx v8,v16,t0"
	.string "vmaxu.vx v8,v16,t0,v0.t"
	.string "vmax.vv v8,v16,v24"
	.string "vmax.vv v8,v16,v24,v0.t"
	.string "vmax.vx v8,v16,t0"
	.string "vmax.vx v8,v16,t0,v0.t"
	.string "vand.vv v8,v16,v24"
	.string "vand.vv v8,v16,v24,v0.t"
	.string "vand.vx v8,v16,t0"
	.string "vand.vx v8,v16,t0,v0.t"
	.string "vand.vi v8,v16,13"
	.string "vand.vi v8,v16,13,v0.t"
	.string "vor.vv v8,v16,v24"
	.string "vor.vv v8,v16,v24,v0.t"
	.string "vor.vx v8,v16,t0"
	.string "vor.vx v8,v16,t0,v0.t"
	.string "vor.vi v8,v16,13"
	.string "vor.vi v8,v16,13,v0.t"
	.string "vxor.vv v8,v16,v24"
	.string "vxor.vv v8,v16,v24,v0.t"
	.string "vxor.vx v8,v16,t0"
	.string "vxor.vx v8,v16,t0,v0.t"
	.string "vxor.vi v8,v16,13"
	.string "vxor.vi v8,v16,13,v0.t"

	.string "vrgather.vv v8,v16,v24"
	.string "vrgather.vv v8,v16,v24,v0.t"
	.string "vrgather.vx v8,v16,t0"
	.string "vrgather.vx v8,v16,t0,v0.t"
	.string "vrgather.vi v8,v16,3"
	.string "vrgather.vi v8,v16,3,v0.t"
	.string "vslideup.vx v8,v16,t0"
	.string "vslideup.vx v8,v16,t0,v0.t"
	.string "vslideup.vi v8,v16,3"
	.string "vslideup.vi v8,v16,3,v0.t"
	.string "vrgatherei16.vv v8,v16,v24"
	.string "vrgatherei16.vv v8,v16,v24,v0.t"

	.string "vslidedown.vx v8,v16,t0"
	.string "vslidedown.vx v8,v16,t0,v0.t"
	.string "vslidedown.vi v8,v16,3"
	.string "vslidedown.vi v8,v16,3,v0.t"

	.string "vredsum.vs v8,v16,v24"
	.string "vredsum.vs v8,v16,v24,v0.t"
	.string "vredand.vs v8,v16,v24"
	.string "vredand.vs v8,v16,v24,v0.t"
	.string "vredor.vs v8,v16,v24"
	.string "vredor.vs v8,v16,v24,v0.t"
	.string "vredxor.vs v8,v16,v24"
	.string "vredxor.vs v8,v16,v24,v0.t"
	.string "vredminu.vs v8,v16,v24"
	.string "vredminu.vs v8,v16,v24,v0.t"
	.string "vredmin.vs v8,v16,v24"
	.string "vredmin.vs v8,v16,v24,v0.t"
	.string "vredmaxu.vs v8,v16,v24"
	.string "vredmaxu.vs v8,v16,v24,v0.t"
	.string "vredmax.vs v8,v16,v24"
	.string "vredmax.vs v8,v16,v24,v0.t"

	.string "vaaddu.vv v8,v16,v24"
	.string "vaaddu.vv v8,v16,v24,v0.t"
	.string "vaaddu.vx v8,v16,t0"
	.string "vaaddu.vx v8,v16,t0,v0.t"
	.string "vaadd.vv v8,v16,v24"
	.string "vaadd.vv v8,v16,v24,v0.t"
	.string "vaadd.vx v8,v16,t0"
	.string "vaadd.vx v8,v16,t0,v0.t"
	.string "vasubu.vv v8,v16,v24"
	.string "vasubu.vv v8,v16,v24,v0.t"
	.string "vasubu.vx v8,v16,t0"
	.string "vasubu.vx v8,v16,t0,v0.t"
	.string "vasub.vv v8,v16,v24"
	.string "vasub.vv v8,v16,v24,v0.t"
	.string "vasub.vx v8,v16,t0"
	.string "vasub.vx v8,v16,t0,v0.t"

	.string "vslide1up.vx v8,v16,t0"
	.string "vslide1up.vx v8,v16,t0,v0.t"
	.string "vslide1down.vx v8,v16,t0"
	.string "vslide1down.vx v8,v16,t0,v0.t"

	.string "vadc.vvm v8,v16,v24,v0"
	 .string "vadc.vxm v8,v16,t0,v0"
	 .string "vadc.vim v8,v16,13,v0"
	.string "vmadc.vvm v8,v16,v24,v0"
	 .string "vmadc.vxm v8,v16,t0,v0"
	 .string "vmadc.vim v8,v16,13,v0"
	.string "vmadc.vv v8,v16,v24"
	.string "vmadc.vx v8,v16,t0"
	.string "vmadc.vi v8,v16,13"
	.string "vsbc.vvm v8,v16,v24,v0"
	 .string "vsbc.vxm v8,v16,t0,v0"
	.string "vmsbc.vvm v8,v16,v24,v0"
	 .string "vmsbc.vxm v8,v16,t0,v0"
	.string "vmsbc.vv v8,v16,v24"
	.string "vmsbc.vx v8,v16,t0"

	.string "vmerge.vvm v8,v16,v24,v0"
	 .string "vmerge.vxm v8,v16,t0,v0"
	 .string "vmerge.vim v8,v16,13,v0"
	.string "vmv.v.v v8,v16"
	.string "vmv.v.x v8,t0"
	.string "vmv.v.i v8,13"
	.string "vmseq.vv v8,v16,v24"
	.string "vmseq.vv v8,v16,v24,v0.t"
	.string "vmseq.vx v8,v16,t0"
	.string "vmseq.vx v8,v16,t0,v0.t"
	.string "vmseq.vi v8,v16,13"
	.string "vmseq.vi v8,v16,13,v0.t"
	.string "vmsne.vv v8,v16,v24"
	.string "vmsne.vv v8,v16,v24,v0.t"
	.string "vmsne.vx v8,v16,t0"
	.string "vmsne.vx v8,v16,t0,v0.t"
	.string "vmsne.vi v8,v16,13"
	.string "vmsne.vi v8,v16,13,v0.t"
	.string "vmsltu.vv v8,v16,v24"
	.string "vmsltu.vv v8,v16,v24,v0.t"
	.string "vmsltu.vx v8,v16,t0"
	.string "vmsltu.vx v8,v16,t0,v0.t"
	.string "vmslt.vv v8,v16,v24"
	.string "vmslt.vv v8,v16,v24,v0.t"
	.string "vmslt.vx v8,v16,t0"
	.string "vmslt.vx v8,v16,t0,v0.t"
	.string "vmsleu.vv v8,v16,v24"
	.string "vmsleu.vv v8,v16,v24,v0.t"
	.string "vmsleu.vx v8,v16,t0"
	.string "vmsleu.vx v8,v16,t0,v0.t"
	.string "vmsleu.vi v8,v16,13"
	.string "vmsleu.vi v8,v16,13,v0.t"
	.string "vmsle.vv v8,v16,v24"
	.string "vmsle.vv v8,v16,v24,v0.t"
	.string "vmsle.vx v8,v16,t0"
	.string "vmsle.vx v8,v16,t0,v0.t"
	.string "vmsle.vi v8,v16,13"
	.string "vmsle.vi v8,v16,13,v0.t"
	.string "vmsgtu.vx v8,v16,t0"
	.string "vmsgtu.vx v8,v16,t0,v0.t"
	.string "vmsgtu.vi v8,v16,13"
	.string "vmsgtu.vi v8,v16,13,v0.t"
	.string "vmsgt.vx v8,v16,t0"
	.string "vmsgt.vx v8,v16,t0,v0.t"
	.string "vmsgt.vi v8,v16,13"
	.string "vmsgt.vi v8,v16,13,v0.t"

	.string "vcompress.vm v8,v16,v24"

	.string "vmandn.mm v8,v16,v24"
	.string "vmand.mm v8,v16,v24"
	.string "vmor.mm v8,v16,v24"
	.string "vmxor.mm v8,v16,v24"
	.string "vmorn.mm v8,v16,v24"
	.string "vmnand.mm v8,v16,v24"
	.string "vmnor.mm v8,v16,v24"
	.string "vmxnor.mm v8,v16,v24"

	.string "vsaddu.vv v8,v16,v24"
	.string "vsaddu.vv v8,v16,v24,v0.t"
	.string "vsaddu.vx v8,v16,t0"
	.string "vsaddu.vx v8,v16,t0,v0.t"
	.string "vsaddu.vi v8,v16,13"
	.string "vsaddu.vi v8,v16,13,v0.t"
	.string "vsadd.vv v8,v16,v24"
	.string "vsadd.vv v8,v16,v24,v0.t"
	.string "vsadd.vx v8,v16,t0"
	.string "vsadd.vx v8,v16,t0,v0.t"
	.string "vsadd.vi v8,v16,13"
	.string "vsadd.vi v8,v16,13,v0.t"
	.string "vssubu.vv v8,v16,v24"
	.string "vssubu.vv v8,v16,v24,v0.t"
	.string "vssubu.vx v8,v16,t0"
	.string "vssubu.vx v8,v16,t0,v0.t"
	.string "vssub.vv v8,v16,v24"
	.string "vssub.vv v8,v16,v24,v0.t"
	.string "vssub.vx v8,v16,t0"
	.string "vssub.vx v8,v16,t0,v0.t"
	.string "vsll.vv v8,v16,v24"
	.string "vsll.vv v8,v16,v24,v0.t"
	.string "vsll.vx v8,v16,t0"
	.string "vsll.vx v8,v16,t0,v0.t"
	.string "vsll.vi v8,v16,13"
	.string "vsll.vi v8,v16,13,v0.t"
	.string "vsmul.vv v8,v16,v24"
	.string "vsmul.vv v8,v16,v24,v0.t"
	.string "vsmul.vx v8,v16,t0"
	.string "vsmul.vx v8,v16,t0,v0.t"
	.string "vmv1r.v v8,v16"
	.string "vmv2r.v v8,v16"
	.string "vmv4r.v v8,v16"
	.string "vmv8r.v v8,v16"
	.string "vsrl.vv v8,v16,v24"
	.string "vsrl.vv v8,v16,v24,v0.t"
	.string "vsrl.vx v8,v16,t0"
	.string "vsrl.vx v8,v16,t0,v0.t"
	.string "vsrl.vi v8,v16,13"
	.string "vsrl.vi v8,v16,13,v0.t"
	.string "vsra.vv v8,v16,v24"
	.string "vsra.vv v8,v16,v24,v0.t"
	.string "vsra.vx v8,v16,t0"
	.string "vsra.vx v8,v16,t0,v0.t"
	.string "vsra.vi v8,v16,13"
	.string "vsra.vi v8,v16,13,v0.t"
	.string "vssrl.vv v8,v16,v24"
	.string "vssrl.vv v8,v16,v24,v0.t"
	.string "vssrl.vx v8,v16,t0"
	.string "vssrl.vx v8,v16,t0,v0.t"
	.string "vssrl.vi v8,v16,13"
	.string "vssrl.vi v8,v16,13,v0.t"

	.string "vdivu.vv v8,v16,v24"
	.string "vdivu.vv v8,v16,v24,v0.t"
	.string "vdivu.vx v8,v16,t0"
	.string "vdivu.vx v8,v16,t0,v0.t"
	.string "vdiv.vv v8,v16,v24"
	.string "vdiv.vv v8,v16,v24,v0.t"
	.string "vdiv.vx v8,v16,t0"
	.string "vdiv.vx v8,v16,t0,v0.t"
	.string "vremu.vv v8,v16,v24"
	.string "vremu.vv v8,v16,v24,v0.t"
	.string "vremu.vx v8,v16,t0"
	.string "vremu.vx v8,v16,t0,v0.t"
	.string "vrem.vv v8,v16,v24"
	.string "vrem.vv v8,v16,v24,v0.t"
	.string "vrem.vx v8,v16,t0"
	.string "vrem.vx v8,v16,t0,v0.t"
	.string "vmulhu.vv v8,v16,v24"
	.string "vmulhu.vv v8,v16,v24,v0.t"
	.string "vmulhu.vx v8,v16,t0"
	.string "vmulhu.vx v8,v16,t0,v0.t"
	.string "vmul.vv v8,v16,v24"
	.string "vmul.vv v8,v16,v24,v0.t"
	.string "vmul.vx v8,v16,t0"
	.string "vmul.vx v8,v16,t0,v0.t"
	.string "vmulhsu.vv v8,v16,v24"
	.string "vmulhsu.vv v8,v16,v24,v0.t"
	.string "vmulhsu.vx v8,v16,t0"
	.string "vmulhsu.vx v8,v16,t0,v0.t"
	.string "vmulh.vv v8,v16,v24"
	.string "vmulh.vv v8,v16,v24,v0.t"
	.string "vmulh.vx v8,v16,t0"
	.string "vmulh.vx v8,v16,t0,v0.t"
	.string "vmadd.vv v8,v16,v24"
	.string "vmadd.vv v8,v16,v24,v0.t"
	.string "vmadd.vx v8,t0,v16"
	.string "vmadd.vx v8,t0,v16,v0.t"
	.string "vmacc.vv v8,v16,v24"
	.string "vmacc.vv v8,v16,v24,v0.t"
	.string "vmacc.vx v8,t0,v16"
	.string "vmacc.vx v8,t0,v16,v0.t"

	.string "vnsrl.wv v8,v16,v24"
	.string "vnsrl.wv v8,v16,v24,v0.t"
	.string "vnsrl.wx v8,v16,t0"
	.string "vnsrl.wx v8,v16,t0,v0.t"
	.string "vnsrl.wi v8,v16,13"
	.string "vnsrl.wi v8,v16,13,v0.t"
	.string "vnsra.wv v8,v16,v24"
	.string "vnsra.wv v8,v16,v24,v0.t"
	.string "vnsra.wx v8,v16,t0"
	.string "vnsra.wx v8,v16,t0,v0.t"
	.string "vnsra.wi v8,v16,13"
	.string "vnsra.wi v8,v16,13,v0.t"
	.string "vnclipu.wv v8,v16,v24"
	.string "vnclipu.wv v8,v16,v24,v0.t"
	.string "vnclipu.wx v8,v16,t0"
	.string "vnclipu.wx v8,v16,t0,v0.t"
	.string "vnclipu.wi v8,v16,13"
	.string "vnclipu.wi v8,v16,13,v0.t"
	.string "vnclip.wv v8,v16,v24"
	.string "vnclip.wv v8,v16,v24,v0.t"
	.string "vnclip.wx v8,v16,t0"
	.string "vnclip.wx v8,v16,t0,v0.t"
	.string "vnclip.wi v8,v16,13"
	.string "vnclip.wi v8,v16,13,v0.t"
	.string "vnmsub.vv v8,v16,v24"
	.string "vnmsub.vv v8,v16,v24,v0.t"
	.string "vnmsub.vx v8,t0,v16"
	.string "vnmsub.vx v8,t0,v16,v0.t"
	.string "vnmsac.vv v8,v16,v24"
	.string "vnmsac.vv v8,v16,v24,v0.t"
	.string "vnmsac.vx v8,t0,v16"
	.string "vnmsac.vx v8,t0,v16,v0.t"

	.string "vwaddu.vv v8,v16,v24"
	.string "vwaddu.vv v8,v16,v24,v0.t"
	.string "vwaddu.vx v8,v16,t0"
	.string "vwaddu.vx v8,v16,t0,v0.t"
	.string "vwadd.vv v8,v16,v24"
	.string "vwadd.vv v8,v16,v24,v0.t"
	.string "vwadd.vx v8,v16,t0"
	.string "vwadd.vx v8,v16,t0,v0.t"
	.string "vwsubu.vv v8,v16,v24"
	.string "vwsubu.vv v8,v16,v24,v0.t"
	.string "vwsubu.vx v8,v16,t0"
	.string "vwsubu.vx v8,v16,t0,v0.t"
	.string "vwsub.vv v8,v16,v24"
	.string "vwsub.vv v8,v16,v24,v0.t"
	.string "vwsub.vx v8,v16,t0"
	.string "vwsub.vx v8,v16,t0,v0.t"
	.string "vwaddu.wv v8,v16,v24"
	.string "vwaddu.wv v8,v16,v24,v0.t"
	.string "vwaddu.wx v8,v16,t0"
	.string "vwaddu.wx v8,v16,t0,v0.t"
	.string "vwadd.wv v8,v16,v24"
	.string "vwadd.wv v8,v16,v24,v0.t"
	.string "vwadd.wx v8,v16,t0"
	.string "vwadd.wx v8,v16,t0,v0.t"
	.string "vwsubu.wv v8,v16,v24"
	.string "vwsubu.wv v8,v16,v24,v0.t"
	.string "vwsubu.wx v8,v16,t0"
	.string "vwsubu.wx v8,v16,t0,v0.t"
	.string "vwsub.wv v8,v16,v24"
	.string "vwsub.wv v8,v16,v24,v0.t"
	.string "vwsub.wx v8,v16,t0"
	.string "vwsub.wx v8,v16,t0,v0.t"
	.string "vwmulu.vv v8,v16,v24"
	.string "vwmulu.vv v8,v16,v24,v0.t"
	.string "vwmulu.vx v8,v16,t0"
	.string "vwmulu.vx v8,v16,t0,v0.t"
	.string "vwmulsu.vv v8,v16,v24"
	.string "vwmulsu.vv v8,v16,v24,v0.t"
	.string "vwmulsu.vx v8,v16,t0"
	.string "vwmulsu.vx v8,v16,t0,v0.t"
	.string "vwmul.vv v8,v16,v24"
	.string "vwmul.vv v8,v16,v24,v0.t"
	.string "vwmul.vx v8,v16,t0"
	.string "vwmul.vx v8,v16,t0,v0.t"
	.string "vwmaccu.vv v8,v16,v24"
	.string "vwmaccu.vv v8,v16,v24,v0.t"
	.string "vwmaccu.vx v8,t0,v16"
	.string "vwmaccu.vx v8,t0,v16,v0.t"
	.string "vwmacc.vv v8,v16,v24"
	.string "vwmacc.vv v8,v16,v24,v0.t"
	.string "vwmacc.vx v8,t0,v16"
	.string "vwmacc.vx v8,t0,v16,v0.t"
	.string "vwmaccsu.vv v8,v16,v24"
	.string "vwmaccsu.vv v8,v16,v24,v0.t"
	.string "vwmaccsu.vx v8,t0,v16"
	.string "vwmaccsu.vx v8,t0,v16,v0.t"
	.string "vwmaccus.vx v8,t0,v16"
	.string "vwmaccus.vx v8,t0,v16,v0.t"

	.string "vfadd.vv v8,v16,v24"
	.string "vfadd.vv v8,v16,v24,v0.t"
	.string "vfadd.vf v8,v16,ft0"
	.string "vfadd.vf v8,v16,ft0,v0.t"
	.string "vfsub.vv v8,v16,v24"
	.string "vfsub.vv v8,v16,v24,v0.t"
	.string "vfsub.vf v8,v16,ft0"
	.string "vfsub.vf v8,v16,ft0,v0.t"
	.string "vfmin.vv v8,v16,v24"
	.string "vfmin.vv v8,v16,v24,v0.t"
	.string "vfmin.vf v8,v16,ft0"
	.string "vfmin.vf v8,v16,ft0,v0.t"
	.string "vfmax.vv v8,v16,v24"
	.string "vfmax.vv v8,v16,v24,v0.t"
	.string "vfmax.vf v8,v16,ft0"
	.string "vfmax.vf v8,v16,ft0,v0.t"
	.string "vfsgnj.vv v8,v16,v24"
	.string "vfsgnj.vv v8,v16,v24,v0.t"
	.string "vfsgnj.vf v8,v16,ft0"
	.string "vfsgnj.vf v8,v16,ft0,v0.t"
	.string "vfsgnjn.vv v8,v16,v24"
	.string "vfsgnjn.vv v8,v16,v24,v0.t"
	.string "vfsgnjn.vf v8,v16,ft0"
	.string "vfsgnjn.vf v8,v16,ft0,v0.t"
	.string "vfsgnjx.vv v8,v16,v24"
	.string "vfsgnjx.vv v8,v16,v24,v0.t"
	.string "vfsgnjx.vf v8,v16,ft0"
	.string "vfsgnjx.vf v8,v16,ft0,v0.t"
	.string "vfslide1up.vf v8,v16,ft0"
	.string "vfslide1up.vf v8,v16,ft0,v0.t"
	.string "vfslide1down.vf v8,v16,ft0"
	.string "vfslide1down.vf v8,v16,ft0,v0.t"

	.string "vfredusum.vs v8,v16,v24"
	.string "vfredusum.vs v8,v16,v24,v0.t"
	.string "vfredosum.vs v8,v16,v24"
	.string "vfredosum.vs v8,v16,v24,v0.t"
	.string "vfredmin.vs v8,v16,v24"
	.string "vfredmin.vs v8,v16,v24,v0.t"
	.string "vfredmax.vs v8,v16,v24"
	.string "vfredmax.vs v8,v16,v24,v0.t"

	.string "vfmerge.vfm v8,v16,ft0,v0"
	.string "vfmv.v.f v8,ft0"

	.string "vmfeq.vv v8,v16,v24"
	.string "vmfeq.vv v8,v16,v24,v0.t"
	.string "vmfeq.vf v8,v16,ft0"
	.string "vmfeq.vf v8,v16,ft0,v0.t"
	.string "vmfle.vv v8,v16,v24"
	.string "vmfle.vv v8,v16,v24,v0.t"
	.string "vmfle.vf v8,v16,ft0"
	.string "vmfle.vf v8,v16,ft0,v0.t"
	.string "vmflt.vv v8,v16,v24"
	.string "vmflt.vv v8,v16,v24,v0.t"
	.string "vmflt.vf v8,v16,ft0"
	.string "vmflt.vf v8,v16,ft0,v0.t"
	.string "vmfne.vv v8,v16,v24"
	.string "vmfne.vv v8,v16,v24,v0.t"
	.string "vmfne.vf v8,v16,ft0"
	.string "vmfne.vf v8,v16,ft0,v0.t"
	.string "vmfgt.vv v8,v16,v24"
	.string "vmfgt.vv v8,v16,v24,v0.t"
	.string "vmfgt.vf v8,v16,ft0"
	.string "vmfgt.vf v8,v16,ft0,v0.t"
	.string "vmfge.vv v8,v16,v24"
	.string "vmfge.vv v8,v16,v24,v0.t"
	.string "vmfge.vf v8,v16,ft0"
	.string "vmfge.vf v8,v16,ft0,v0.t"

	.string "vfdiv.vv v8,v16,v24"
	.string "vfdiv.vv v8,v16,v24,v0.t"
	.string "vfdiv.vf v8,v16,ft0"
	.string "vfdiv.vf v8,v16,ft0,v0.t"
	.string "vfrdiv.vf v8,v16,ft0"
	.string "vfrdiv.vf v8,v16,ft0,v0.t"
	.string "vfmul.vv v8,v16,v24"
	.string "vfmul.vv v8,v16,v24,v0.t"
	.string "vfmul.vf v8,v16,ft0"
	.string "vfmul.vf v8,v16,ft0,v0.t"
	.string "vfrsub.vf v8,v16,ft0"
	.string "vfrsub.vf v8,v16,ft0,v0.t"
	.string "vfmadd.vv v8,v16,v24"
	.string "vfmadd.vv v8,v16,v24,v0.t"
	.string "vfmadd.vf v8,ft0,v16"
	.string "vfmadd.vf v8,ft0,v16,v0.t"
	.string "vfmsub.vv v8,v16,v24"
	.string "vfmsub.vv v8,v16,v24,v0.t"
	.string "vfmsub.vf v8,ft0,v16"
	.string "vfmsub.vf v8,ft0,v16,v0.t"
	.string "vfmacc.vv v8,v16,v24"
	.string "vfmacc.vv v8,v16,v24,v0.t"
	.string "vfmacc.vf v8,ft0,v16"
	.string "vfmacc.vf v8,ft0,v16,v0.t"
	.string "vfmsac.vv v8,v16,v24"
	.string "vfmsac.vv v8,v16,v24,v0.t"
	.string "vfmsac.vf v8,ft0,v16"
	.string "vfmsac.vf v8,ft0,v16,v0.t"

	.string "vfnmsac.vv v8,v16,v24"
	.string "vfnmsac.vv v8,v16,v24,v0.t"
	.string "vfnmsac.vf v8,ft0,v16"
	.string "vfnmsac.vf v8,ft0,v16,v0.t"
	.string "vfnmacc.vv v8,v16,v24"
	.string "vfnmacc.vv v8,v16,v24,v0.t"
	.string "vfnmacc.vf v8,ft0,v16"
	.string "vfnmacc.vf v8,ft0,v16,v0.t"
	.string "vfnmsub.vv v8,v16,v24"
	.string "vfnmsub.vv v8,v16,v24,v0.t"
	.string "vfnmsub.vf v8,ft0,v16"
	.string "vfnmsub.vf v8,ft0,v16,v0.t"
	.string "vfnmadd.vv v8,v16,v24"
	.string "vfnmadd.vv v8,v16,v24,v0.t"
	.string "vfnmadd.vf v8,ft0,v16"
	.string "vfnmadd.vf v8,ft0,v16,v0.t"

	.string "vwredsumu.vs v8,v16,v24"
	.string "vwredsumu.vs v8,v16,v24,v0.t"
	.string "vwredsum.vs v8,v16,v24"
	.string "vwredsum.vs v8,v16,v24,v0.t"

	.string "vfwadd.vv v8,v16,v24"
	.string "vfwadd.vv v8,v16,v24,v0.t"
	.string "vfwadd.vf v8,v16,ft0"
	.string "vfwadd.vf v8,v16,ft0,v0.t"
	.string "vfwsub.vv v8,v16,v24"
	.string "vfwsub.vv v8,v16,v24,v0.t"
	.string "vfwsub.vf v8,v16,ft0"
	.string "vfwsub.vf v8,v16,ft0,v0.t"
	.string "vfwadd.wv v8,v16,v24"
	.string "vfwadd.wv v8,v16,v24,v0.t"
	.string "vfwadd.wf v8,v16,ft0"
	.string "vfwadd.wf v8,v16,ft0,v0.t"
	.string "vfwsub.wv v8,v16,v24"
	.string "vfwsub.wv v8,v16,v24,v0.t"
	.string "vfwsub.wf v8,v16,ft0"
	.string "vfwsub.wf v8,v16,ft0,v0.t"
	.string "vfwmul.vv v8,v16,v24"
	.string "vfwmul.vv v8,v16,v24,v0.t"
	.string "vfwmul.vf v8,v16,ft0"
	.string "vfwmul.vf v8,v16,ft0,v0.t"
	.string "vfwmacc.vv v8,v16,v24"
	.string "vfwmacc.vv v8,v16,v24,v0.t"
	.string "vfwmacc.vf v8,ft0,v16"
	.string "vfwmacc.vf v8,ft0,v16,v0.t"
	.string "vfwnmacc.vv v8,v16,v24"
	.string "vfwnmacc.vv v8,v16,v24,v0.t"
	.string "vfwnmacc.vf v8,ft0,v16"
	.string "vfwnmacc.vf v8,ft0,v16,v0.t"
	.string "vfwmsac.vv v8,v16,v24"
	.string "vfwmsac.vv v8,v16,v24,v0.t"
	.string "vfwmsac.vf v8,ft0,v16"
	.string "vfwmsac.vf v8,ft0,v16,v0.t"
	.string "vfwnmsac.vv v8,v16,v24"
	.string "vfwnmsac.vv v8,v16,v24,v0.t"
	.string "vfwnmsac.vf v8,ft0,v16"
	.string "vfwnmsac.vf v8,ft0,v16,v0.t"

	.string "vfwredosum.vs v8,v16,v24"
	.string "vfwredosum.vs v8,v16,v24,v0.t"
	.string "vfwredusum.vs v8,v16,v24"
	.string "vfwredusum.vs v8,v16,v24,v0.t"

	.string "vmv.s.x v8,t0"
	.string "vmv.x.s t0,v8"

	.string "vcpop.m t0,v8"
	.string "vcpop.m t0,v8,v0.t"
	.string "vfirst.m t0,v8"
	.string "vfirst.m t0,v8,v0.t"
	.string "vzext.vf2 v8,v16"
	.string "vzext.vf2 v8,v16,v0.t"
	.string "vsext.vf2 v8,v16"
	.string "vsext.vf2 v8,v16,v0.t"
	.string "vzext.vf4 v8,v16"
	.string "vzext.vf4 v8,v16,v0.t"
	.string "vsext.vf4 v8,v16"
	.string "vsext.vf4 v8,v16,v0.t"
	.string "vzext.vf8 v8,v16"
	.string "vzext.vf8 v8,v16,v0.t"
	.string "vsext.vf8 v8,v16"
	.string "vsext.vf8 v8,v16,v0.t"

	.string "vfmv.f.s ft0,v8"
	.string "vfmv.s.f v8,ft0"

	.string "vfcvt.xu.f.v v8,v16"
	.string "vfcvt.xu.f.v v8,v16,v0.t"
	.string "vfcvt.x.f.v v8,v16"
	.string "vfcvt.x.f.v v8,v16,v0.t"
	.string "vfcvt.f.xu.v v8,v16"
	.string "vfcvt.f.xu.v v8,v16,v0.t"
	.string "vfcvt.f.x.v v8,v16"
	.string "vfcvt.f.x.v v8,v16,v0.t"
	.string "vfcvt.rtz.x.f.v v8,v16"
	.string "vfcvt.rtz.x.f.v v8,v16,v0.t"
	.string "vfcvt.rtz.xu.f.v v8,v16"
	.string "vfcvt.rtz.xu.f.v v8,v16,v0.t"

	.string "vfwcvt.xu.f.v v8,v16"
	.string "vfwcvt.xu.f.v v8,v16,v0.t"
	.string "vfwcvt.x.f.v v8,v16"
	.string "vfwcvt.x.f.v v8,v16,v0.t"
	.string "vfwcvt.f.xu.v v8,v16"
	.string "vfwcvt.f.xu.v v8,v16,v0.t"
	.string "vfwcvt.f.x.v v8,v16"
	.string "vfwcvt.f.x.v v8,v16,v0.t"
	.string "vfwcvt.f.f.v v8,v16"
	.string "vfwcvt.f.f.v v8,v16,v0.t"
	.string "vfwcvt.rtz.xu.f.v v8,v16"
	.string "vfwcvt.rtz.xu.f.v v8,v16,v0.t"
	.string "vfwcvt.rtz.x.f.v v8,v16"
	.string "vfwcvt.rtz.x.f.v v8,v16,v0.t"

	.string "vfncvt.xu.f.w v8,v16"
	.string "vfncvt.xu.f.w v8,v16,v0.t"
	.string "vfncvt.x.f.w v8,v16"
	.string "vfncvt.x.f.w v8,v16,v0.t"
	.string "vfncvt.f.xu.w v8,v16"
	.string "vfncvt.f.xu.w v8,v16,v0.t"
	.string "vfncvt.f.x.w v8,v16"
	.string "vfncvt.f.x.w v8,v16,v0.t"
	.string "vfncvt.f.f.w v8,v16"
	.string "vfncvt.f.f.w v8,v16,v0.t"
	.string "vfncvt.rtz.x.f.w v8,v16"
	.string "vfncvt.rtz.x.f.w v8,v16,v0.t"
	.string "vfncvt.rtz.xu.f.w v8,v16"
	.string "vfncvt.rtz.xu.f.w v8,v16,v0.t"
	.string "vfncvt.rod.f.f.w v8,v16"
	.string "vfncvt.rod.f.f.w v8,v16,v0.t"

	.string "vfsqrt.v v8,v16"
	.string "vfsqrt.v v8,v16,v0.t"
	.string "vfrsqrt7.v v8,v16"
	.string "vfrsqrt7.v v8,v16,v0.t"
	.string "vfrec7.v v8,v16"
	.string "vfrec7.v v8,v16,v0.t"
	.string "vfclass.v v8,v16"
	.string "vfclass.v v8,v16,v0.t"

	.string "vmsbf.m v8,v16"
	.string "vmsbf.m v8,v16,v0.t"
	.string "vmsof.m v8,v16"
	.string "vmsof.m v8,v16,v0.t"
	.string "vmsif.m v8,v16"
	.string "vmsif.m v8,v16,v0.t"
	.string "viota.m v8,v16"
	.string "viota.m v8,v16,v0.t"
	.string "vid.v v8"
	.string "vid.v v8,v0.t"



.balign 8

.global bench_count
bench_count:
	defptr 588



.balign 8
u64_cycle:
.dword 0

.text
.balign 8


# x off

# acc off code:vararg











# name type setup code:vararg





bench_add_m1:
	m_nop
	li a0, WARMUP
1:
add t0,t1,t2
add t1,t1,t2
add t2,t1,t2
add t3,t1,t2
add t4,t1,t2
add t5,t1,t2
add t6,t1,t2
add t7,t1,t2



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
add t0,t1,t2
add t1,t1,t2
add t2,t1,t2
add t3,t1,t2
add t4,t1,t2
add t5,t1,t2
add t6,t1,t2
add t7,t1,t2



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_mul_m1:
	m_nop
	li a0, WARMUP
1:
mul t0,t1,t2
mul t1,t1,t2
mul t2,t1,t2
mul t3,t1,t2
mul t4,t1,t2
mul t5,t1,t2
mul t6,t1,t2
mul t7,t1,t2



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
mul t0,t1,t2
mul t1,t1,t2
mul t2,t1,t2
mul t3,t1,t2
mul t4,t1,t2
mul t5,t1,t2
mul t6,t1,t2
mul t7,t1,t2



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vv v8,v16,v24
vadd.vv v9,v17,v25
vadd.vv v10,v18,v26
vadd.vv v11,v19,v27
vadd.vv v12,v20,v28
vadd.vv v13,v21,v29
vadd.vv v14,v22,v30
vadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vv v8,v16,v24
vadd.vv v9,v17,v25
vadd.vv v10,v18,v26
vadd.vv v11,v19,v27
vadd.vv v12,v20,v28
vadd.vv v13,v21,v29
vadd.vv v14,v22,v30
vadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vv v8,v16,v24,v0.t
vadd.vv v9,v17,v25,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v11,v19,v27,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v13,v21,v29,v0.t
vadd.vv v14,v22,v30,v0.t
vadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vv v8,v16,v24,v0.t
vadd.vv v9,v17,v25,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v11,v19,v27,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v13,v21,v29,v0.t
vadd.vv v14,v22,v30,v0.t
vadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvx_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vx v8,v16,t0
vadd.vx v9,v17,t1
vadd.vx v10,v18,t2
vadd.vx v11,v19,t3
vadd.vx v12,v20,t4
vadd.vx v13,v21,t5
vadd.vx v14,v22,t6
vadd.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vx v8,v16,t0
vadd.vx v9,v17,t1
vadd.vx v10,v18,t2
vadd.vx v11,v19,t3
vadd.vx v12,v20,t4
vadd.vx v13,v21,t5
vadd.vx v14,v22,t6
vadd.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvxm_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vx v8,v16,t0,v0.t
vadd.vx v9,v17,t1,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v11,v19,t3,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v13,v21,t5,v0.t
vadd.vx v14,v22,t6,v0.t
vadd.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vx v8,v16,t0,v0.t
vadd.vx v9,v17,t1,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v11,v19,t3,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v13,v21,t5,v0.t
vadd.vx v14,v22,t6,v0.t
vadd.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvi_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vi v8,v16,13
vadd.vi v9,v17,13
vadd.vi v10,v18,13
vadd.vi v11,v19,13
vadd.vi v12,v20,13
vadd.vi v13,v21,13
vadd.vi v14,v22,13
vadd.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vi v8,v16,13
vadd.vi v9,v17,13
vadd.vi v10,v18,13
vadd.vi v11,v19,13
vadd.vi v12,v20,13
vadd.vi v13,v21,13
vadd.vi v14,v22,13
vadd.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvim_m1:
	m_nop
	li a0, WARMUP
1:
vadd.vi v8,v16,13,v0.t
vadd.vi v9,v17,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v11,v19,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v13,v21,13,v0.t
vadd.vi v14,v22,13,v0.t
vadd.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadd.vi v8,v16,13,v0.t
vadd.vi v9,v17,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v11,v19,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v13,v21,13,v0.t
vadd.vi v14,v22,13,v0.t
vadd.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vsub.vv v8,v16,v24
vsub.vv v9,v17,v25
vsub.vv v10,v18,v26
vsub.vv v11,v19,v27
vsub.vv v12,v20,v28
vsub.vv v13,v21,v29
vsub.vv v14,v22,v30
vsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsub.vv v8,v16,v24
vsub.vv v9,v17,v25
vsub.vv v10,v18,v26
vsub.vv v11,v19,v27
vsub.vv v12,v20,v28
vsub.vv v13,v21,v29
vsub.vv v14,v22,v30
vsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsub.vv v8,v16,v24,v0.t
vsub.vv v9,v17,v25,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v11,v19,v27,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v13,v21,v29,v0.t
vsub.vv v14,v22,v30,v0.t
vsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsub.vv v8,v16,v24,v0.t
vsub.vv v9,v17,v25,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v11,v19,v27,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v13,v21,v29,v0.t
vsub.vv v14,v22,v30,v0.t
vsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvx_m1:
	m_nop
	li a0, WARMUP
1:
vsub.vx v8,v16,t0
vsub.vx v9,v17,t1
vsub.vx v10,v18,t2
vsub.vx v11,v19,t3
vsub.vx v12,v20,t4
vsub.vx v13,v21,t5
vsub.vx v14,v22,t6
vsub.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsub.vx v8,v16,t0
vsub.vx v9,v17,t1
vsub.vx v10,v18,t2
vsub.vx v11,v19,t3
vsub.vx v12,v20,t4
vsub.vx v13,v21,t5
vsub.vx v14,v22,t6
vsub.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsub.vx v8,v16,t0,v0.t
vsub.vx v9,v17,t1,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v11,v19,t3,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v13,v21,t5,v0.t
vsub.vx v14,v22,t6,v0.t
vsub.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsub.vx v8,v16,t0,v0.t
vsub.vx v9,v17,t1,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v11,v19,t3,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v13,v21,t5,v0.t
vsub.vx v14,v22,t6,v0.t
vsub.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvx_m1:
	m_nop
	li a0, WARMUP
1:
vrsub.vx v8,v16,t0
vrsub.vx v9,v17,t1
vrsub.vx v10,v18,t2
vrsub.vx v11,v19,t3
vrsub.vx v12,v20,t4
vrsub.vx v13,v21,t5
vrsub.vx v14,v22,t6
vrsub.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrsub.vx v8,v16,t0
vrsub.vx v9,v17,t1
vrsub.vx v10,v18,t2
vrsub.vx v11,v19,t3
vrsub.vx v12,v20,t4
vrsub.vx v13,v21,t5
vrsub.vx v14,v22,t6
vrsub.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v9,v17,t1,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v11,v19,t3,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v13,v21,t5,v0.t
vrsub.vx v14,v22,t6,v0.t
vrsub.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v9,v17,t1,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v11,v19,t3,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v13,v21,t5,v0.t
vrsub.vx v14,v22,t6,v0.t
vrsub.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvi_m1:
	m_nop
	li a0, WARMUP
1:
vrsub.vi v8,v16,13
vrsub.vi v9,v17,13
vrsub.vi v10,v18,13
vrsub.vi v11,v19,13
vrsub.vi v12,v20,13
vrsub.vi v13,v21,13
vrsub.vi v14,v22,13
vrsub.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrsub.vi v8,v16,13
vrsub.vi v9,v17,13
vrsub.vi v10,v18,13
vrsub.vi v11,v19,13
vrsub.vi v12,v20,13
vrsub.vi v13,v21,13
vrsub.vi v14,v22,13
vrsub.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvim_m1:
	m_nop
	li a0, WARMUP
1:
vrsub.vi v8,v16,13,v0.t
vrsub.vi v9,v17,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v11,v19,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v13,v21,13,v0.t
vrsub.vi v14,v22,13,v0.t
vrsub.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrsub.vi v8,v16,13,v0.t
vrsub.vi v9,v17,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v11,v19,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v13,v21,13,v0.t
vrsub.vi v14,v22,13,v0.t
vrsub.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvv_m1:
	m_nop
	li a0, WARMUP
1:
vminu.vv v8,v16,v24
vminu.vv v9,v17,v25
vminu.vv v10,v18,v26
vminu.vv v11,v19,v27
vminu.vv v12,v20,v28
vminu.vv v13,v21,v29
vminu.vv v14,v22,v30
vminu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vminu.vv v8,v16,v24
vminu.vv v9,v17,v25
vminu.vv v10,v18,v26
vminu.vv v11,v19,v27
vminu.vv v12,v20,v28
vminu.vv v13,v21,v29
vminu.vv v14,v22,v30
vminu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vminu.vv v8,v16,v24,v0.t
vminu.vv v9,v17,v25,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v11,v19,v27,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v13,v21,v29,v0.t
vminu.vv v14,v22,v30,v0.t
vminu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vminu.vv v8,v16,v24,v0.t
vminu.vv v9,v17,v25,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v11,v19,v27,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v13,v21,v29,v0.t
vminu.vv v14,v22,v30,v0.t
vminu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvx_m1:
	m_nop
	li a0, WARMUP
1:
vminu.vx v8,v16,t0
vminu.vx v9,v17,t1
vminu.vx v10,v18,t2
vminu.vx v11,v19,t3
vminu.vx v12,v20,t4
vminu.vx v13,v21,t5
vminu.vx v14,v22,t6
vminu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vminu.vx v8,v16,t0
vminu.vx v9,v17,t1
vminu.vx v10,v18,t2
vminu.vx v11,v19,t3
vminu.vx v12,v20,t4
vminu.vx v13,v21,t5
vminu.vx v14,v22,t6
vminu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vminu.vx v8,v16,t0,v0.t
vminu.vx v9,v17,t1,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v11,v19,t3,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v13,v21,t5,v0.t
vminu.vx v14,v22,t6,v0.t
vminu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vminu.vx v8,v16,t0,v0.t
vminu.vx v9,v17,t1,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v11,v19,t3,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v13,v21,t5,v0.t
vminu.vx v14,v22,t6,v0.t
vminu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvv_m1:
	m_nop
	li a0, WARMUP
1:
vmin.vv v8,v16,v24
vmin.vv v9,v17,v25
vmin.vv v10,v18,v26
vmin.vv v11,v19,v27
vmin.vv v12,v20,v28
vmin.vv v13,v21,v29
vmin.vv v14,v22,v30
vmin.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmin.vv v8,v16,v24
vmin.vv v9,v17,v25
vmin.vv v10,v18,v26
vmin.vv v11,v19,v27
vmin.vv v12,v20,v28
vmin.vv v13,v21,v29
vmin.vv v14,v22,v30
vmin.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmin.vv v8,v16,v24,v0.t
vmin.vv v9,v17,v25,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v11,v19,v27,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v13,v21,v29,v0.t
vmin.vv v14,v22,v30,v0.t
vmin.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmin.vv v8,v16,v24,v0.t
vmin.vv v9,v17,v25,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v11,v19,v27,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v13,v21,v29,v0.t
vmin.vv v14,v22,v30,v0.t
vmin.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvx_m1:
	m_nop
	li a0, WARMUP
1:
vmin.vx v8,v16,t0
vmin.vx v9,v17,t1
vmin.vx v10,v18,t2
vmin.vx v11,v19,t3
vmin.vx v12,v20,t4
vmin.vx v13,v21,t5
vmin.vx v14,v22,t6
vmin.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmin.vx v8,v16,t0
vmin.vx v9,v17,t1
vmin.vx v10,v18,t2
vmin.vx v11,v19,t3
vmin.vx v12,v20,t4
vmin.vx v13,v21,t5
vmin.vx v14,v22,t6
vmin.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmin.vx v8,v16,t0,v0.t
vmin.vx v9,v17,t1,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v11,v19,t3,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v13,v21,t5,v0.t
vmin.vx v14,v22,t6,v0.t
vmin.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmin.vx v8,v16,t0,v0.t
vmin.vx v9,v17,t1,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v11,v19,t3,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v13,v21,t5,v0.t
vmin.vx v14,v22,t6,v0.t
vmin.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvv_m1:
	m_nop
	li a0, WARMUP
1:
vmaxu.vv v8,v16,v24
vmaxu.vv v9,v17,v25
vmaxu.vv v10,v18,v26
vmaxu.vv v11,v19,v27
vmaxu.vv v12,v20,v28
vmaxu.vv v13,v21,v29
vmaxu.vv v14,v22,v30
vmaxu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmaxu.vv v8,v16,v24
vmaxu.vv v9,v17,v25
vmaxu.vv v10,v18,v26
vmaxu.vv v11,v19,v27
vmaxu.vv v12,v20,v28
vmaxu.vv v13,v21,v29
vmaxu.vv v14,v22,v30
vmaxu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v9,v17,v25,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v11,v19,v27,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v13,v21,v29,v0.t
vmaxu.vv v14,v22,v30,v0.t
vmaxu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v9,v17,v25,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v11,v19,v27,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v13,v21,v29,v0.t
vmaxu.vv v14,v22,v30,v0.t
vmaxu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmaxu.vx v8,v16,t0
vmaxu.vx v9,v17,t1
vmaxu.vx v10,v18,t2
vmaxu.vx v11,v19,t3
vmaxu.vx v12,v20,t4
vmaxu.vx v13,v21,t5
vmaxu.vx v14,v22,t6
vmaxu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmaxu.vx v8,v16,t0
vmaxu.vx v9,v17,t1
vmaxu.vx v10,v18,t2
vmaxu.vx v11,v19,t3
vmaxu.vx v12,v20,t4
vmaxu.vx v13,v21,t5
vmaxu.vx v14,v22,t6
vmaxu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v9,v17,t1,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v11,v19,t3,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v13,v21,t5,v0.t
vmaxu.vx v14,v22,t6,v0.t
vmaxu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v9,v17,t1,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v11,v19,t3,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v13,v21,t5,v0.t
vmaxu.vx v14,v22,t6,v0.t
vmaxu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvv_m1:
	m_nop
	li a0, WARMUP
1:
vmax.vv v8,v16,v24
vmax.vv v9,v17,v25
vmax.vv v10,v18,v26
vmax.vv v11,v19,v27
vmax.vv v12,v20,v28
vmax.vv v13,v21,v29
vmax.vv v14,v22,v30
vmax.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmax.vv v8,v16,v24
vmax.vv v9,v17,v25
vmax.vv v10,v18,v26
vmax.vv v11,v19,v27
vmax.vv v12,v20,v28
vmax.vv v13,v21,v29
vmax.vv v14,v22,v30
vmax.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmax.vv v8,v16,v24,v0.t
vmax.vv v9,v17,v25,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v11,v19,v27,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v13,v21,v29,v0.t
vmax.vv v14,v22,v30,v0.t
vmax.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmax.vv v8,v16,v24,v0.t
vmax.vv v9,v17,v25,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v11,v19,v27,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v13,v21,v29,v0.t
vmax.vv v14,v22,v30,v0.t
vmax.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvx_m1:
	m_nop
	li a0, WARMUP
1:
vmax.vx v8,v16,t0
vmax.vx v9,v17,t1
vmax.vx v10,v18,t2
vmax.vx v11,v19,t3
vmax.vx v12,v20,t4
vmax.vx v13,v21,t5
vmax.vx v14,v22,t6
vmax.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmax.vx v8,v16,t0
vmax.vx v9,v17,t1
vmax.vx v10,v18,t2
vmax.vx v11,v19,t3
vmax.vx v12,v20,t4
vmax.vx v13,v21,t5
vmax.vx v14,v22,t6
vmax.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmax.vx v8,v16,t0,v0.t
vmax.vx v9,v17,t1,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v11,v19,t3,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v13,v21,t5,v0.t
vmax.vx v14,v22,t6,v0.t
vmax.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmax.vx v8,v16,t0,v0.t
vmax.vx v9,v17,t1,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v11,v19,t3,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v13,v21,t5,v0.t
vmax.vx v14,v22,t6,v0.t
vmax.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvv_m1:
	m_nop
	li a0, WARMUP
1:
vand.vv v8,v16,v24
vand.vv v9,v17,v25
vand.vv v10,v18,v26
vand.vv v11,v19,v27
vand.vv v12,v20,v28
vand.vv v13,v21,v29
vand.vv v14,v22,v30
vand.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vv v8,v16,v24
vand.vv v9,v17,v25
vand.vv v10,v18,v26
vand.vv v11,v19,v27
vand.vv v12,v20,v28
vand.vv v13,v21,v29
vand.vv v14,v22,v30
vand.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvvm_m1:
	m_nop
	li a0, WARMUP
1:
vand.vv v8,v16,v24,v0.t
vand.vv v9,v17,v25,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v11,v19,v27,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v13,v21,v29,v0.t
vand.vv v14,v22,v30,v0.t
vand.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vv v8,v16,v24,v0.t
vand.vv v9,v17,v25,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v11,v19,v27,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v13,v21,v29,v0.t
vand.vv v14,v22,v30,v0.t
vand.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvx_m1:
	m_nop
	li a0, WARMUP
1:
vand.vx v8,v16,t0
vand.vx v9,v17,t1
vand.vx v10,v18,t2
vand.vx v11,v19,t3
vand.vx v12,v20,t4
vand.vx v13,v21,t5
vand.vx v14,v22,t6
vand.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vx v8,v16,t0
vand.vx v9,v17,t1
vand.vx v10,v18,t2
vand.vx v11,v19,t3
vand.vx v12,v20,t4
vand.vx v13,v21,t5
vand.vx v14,v22,t6
vand.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvxm_m1:
	m_nop
	li a0, WARMUP
1:
vand.vx v8,v16,t0,v0.t
vand.vx v9,v17,t1,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v11,v19,t3,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v13,v21,t5,v0.t
vand.vx v14,v22,t6,v0.t
vand.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vx v8,v16,t0,v0.t
vand.vx v9,v17,t1,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v11,v19,t3,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v13,v21,t5,v0.t
vand.vx v14,v22,t6,v0.t
vand.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvi_m1:
	m_nop
	li a0, WARMUP
1:
vand.vi v8,v16,13
vand.vi v9,v17,13
vand.vi v10,v18,13
vand.vi v11,v19,13
vand.vi v12,v20,13
vand.vi v13,v21,13
vand.vi v14,v22,13
vand.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vi v8,v16,13
vand.vi v9,v17,13
vand.vi v10,v18,13
vand.vi v11,v19,13
vand.vi v12,v20,13
vand.vi v13,v21,13
vand.vi v14,v22,13
vand.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvim_m1:
	m_nop
	li a0, WARMUP
1:
vand.vi v8,v16,13,v0.t
vand.vi v9,v17,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v11,v19,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v13,v21,13,v0.t
vand.vi v14,v22,13,v0.t
vand.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vand.vi v8,v16,13,v0.t
vand.vi v9,v17,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v11,v19,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v13,v21,13,v0.t
vand.vi v14,v22,13,v0.t
vand.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvv_m1:
	m_nop
	li a0, WARMUP
1:
vor.vv v8,v16,v24
vor.vv v9,v17,v25
vor.vv v10,v18,v26
vor.vv v11,v19,v27
vor.vv v12,v20,v28
vor.vv v13,v21,v29
vor.vv v14,v22,v30
vor.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vv v8,v16,v24
vor.vv v9,v17,v25
vor.vv v10,v18,v26
vor.vv v11,v19,v27
vor.vv v12,v20,v28
vor.vv v13,v21,v29
vor.vv v14,v22,v30
vor.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvvm_m1:
	m_nop
	li a0, WARMUP
1:
vor.vv v8,v16,v24,v0.t
vor.vv v9,v17,v25,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v11,v19,v27,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v13,v21,v29,v0.t
vor.vv v14,v22,v30,v0.t
vor.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vv v8,v16,v24,v0.t
vor.vv v9,v17,v25,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v11,v19,v27,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v13,v21,v29,v0.t
vor.vv v14,v22,v30,v0.t
vor.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvx_m1:
	m_nop
	li a0, WARMUP
1:
vor.vx v8,v16,t0
vor.vx v9,v17,t1
vor.vx v10,v18,t2
vor.vx v11,v19,t3
vor.vx v12,v20,t4
vor.vx v13,v21,t5
vor.vx v14,v22,t6
vor.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vx v8,v16,t0
vor.vx v9,v17,t1
vor.vx v10,v18,t2
vor.vx v11,v19,t3
vor.vx v12,v20,t4
vor.vx v13,v21,t5
vor.vx v14,v22,t6
vor.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvxm_m1:
	m_nop
	li a0, WARMUP
1:
vor.vx v8,v16,t0,v0.t
vor.vx v9,v17,t1,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v11,v19,t3,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v13,v21,t5,v0.t
vor.vx v14,v22,t6,v0.t
vor.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vx v8,v16,t0,v0.t
vor.vx v9,v17,t1,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v11,v19,t3,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v13,v21,t5,v0.t
vor.vx v14,v22,t6,v0.t
vor.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvi_m1:
	m_nop
	li a0, WARMUP
1:
vor.vi v8,v16,13
vor.vi v9,v17,13
vor.vi v10,v18,13
vor.vi v11,v19,13
vor.vi v12,v20,13
vor.vi v13,v21,13
vor.vi v14,v22,13
vor.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vi v8,v16,13
vor.vi v9,v17,13
vor.vi v10,v18,13
vor.vi v11,v19,13
vor.vi v12,v20,13
vor.vi v13,v21,13
vor.vi v14,v22,13
vor.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvim_m1:
	m_nop
	li a0, WARMUP
1:
vor.vi v8,v16,13,v0.t
vor.vi v9,v17,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v11,v19,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v13,v21,13,v0.t
vor.vi v14,v22,13,v0.t
vor.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vor.vi v8,v16,13,v0.t
vor.vi v9,v17,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v11,v19,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v13,v21,13,v0.t
vor.vi v14,v22,13,v0.t
vor.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvv_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vv v8,v16,v24
vxor.vv v9,v17,v25
vxor.vv v10,v18,v26
vxor.vv v11,v19,v27
vxor.vv v12,v20,v28
vxor.vv v13,v21,v29
vxor.vv v14,v22,v30
vxor.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vv v8,v16,v24
vxor.vv v9,v17,v25
vxor.vv v10,v18,v26
vxor.vv v11,v19,v27
vxor.vv v12,v20,v28
vxor.vv v13,v21,v29
vxor.vv v14,v22,v30
vxor.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvvm_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vv v8,v16,v24,v0.t
vxor.vv v9,v17,v25,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v11,v19,v27,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v13,v21,v29,v0.t
vxor.vv v14,v22,v30,v0.t
vxor.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vv v8,v16,v24,v0.t
vxor.vv v9,v17,v25,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v11,v19,v27,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v13,v21,v29,v0.t
vxor.vv v14,v22,v30,v0.t
vxor.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvx_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vx v8,v16,t0
vxor.vx v9,v17,t1
vxor.vx v10,v18,t2
vxor.vx v11,v19,t3
vxor.vx v12,v20,t4
vxor.vx v13,v21,t5
vxor.vx v14,v22,t6
vxor.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vx v8,v16,t0
vxor.vx v9,v17,t1
vxor.vx v10,v18,t2
vxor.vx v11,v19,t3
vxor.vx v12,v20,t4
vxor.vx v13,v21,t5
vxor.vx v14,v22,t6
vxor.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvxm_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vx v8,v16,t0,v0.t
vxor.vx v9,v17,t1,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v11,v19,t3,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v13,v21,t5,v0.t
vxor.vx v14,v22,t6,v0.t
vxor.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vx v8,v16,t0,v0.t
vxor.vx v9,v17,t1,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v11,v19,t3,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v13,v21,t5,v0.t
vxor.vx v14,v22,t6,v0.t
vxor.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvi_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vi v8,v16,13
vxor.vi v9,v17,13
vxor.vi v10,v18,13
vxor.vi v11,v19,13
vxor.vi v12,v20,13
vxor.vi v13,v21,13
vxor.vi v14,v22,13
vxor.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vi v8,v16,13
vxor.vi v9,v17,13
vxor.vi v10,v18,13
vxor.vi v11,v19,13
vxor.vi v12,v20,13
vxor.vi v13,v21,13
vxor.vi v14,v22,13
vxor.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvim_m1:
	m_nop
	li a0, WARMUP
1:
vxor.vi v8,v16,13,v0.t
vxor.vi v9,v17,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v11,v19,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v13,v21,13,v0.t
vxor.vi v14,v22,13,v0.t
vxor.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vxor.vi v8,v16,13,v0.t
vxor.vi v9,v17,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v11,v19,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v13,v21,13,v0.t
vxor.vi v14,v22,13,v0.t
vxor.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vrgathervv_m1:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:
vrgather.vv v8,v16,v24
vrgather.vv v9,v17,v25
vrgather.vv v10,v18,v26
vrgather.vv v11,v19,v27
vrgather.vv v12,v20,v28
vrgather.vv v13,v21,v29
vrgather.vv v14,v22,v30
vrgather.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vv v8,v16,v24
vrgather.vv v9,v17,v25
vrgather.vv v10,v18,v26
vrgather.vv v11,v19,v27
vrgather.vv v12,v20,v28
vrgather.vv v13,v21,v29
vrgather.vv v14,v22,v30
vrgather.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervvm_m1:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v9,v17,v25,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v11,v19,v27,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v13,v21,v29,v0.t
vrgather.vv v14,v22,v30,v0.t
vrgather.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v9,v17,v25,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v11,v19,v27,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v13,v21,v29,v0.t
vrgather.vv v14,v22,v30,v0.t
vrgather.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervx_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vrgather.vx v8,v16,t0
vrgather.vx v9,v17,t1
vrgather.vx v10,v18,t2
vrgather.vx v11,v19,t3
vrgather.vx v12,v20,t4
vrgather.vx v13,v21,t5
vrgather.vx v14,v22,t6
vrgather.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vx v8,v16,t0
vrgather.vx v9,v17,t1
vrgather.vx v10,v18,t2
vrgather.vx v11,v19,t3
vrgather.vx v12,v20,t4
vrgather.vx v13,v21,t5
vrgather.vx v14,v22,t6
vrgather.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervxm_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v9,v17,t1,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v11,v19,t3,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v13,v21,t5,v0.t
vrgather.vx v14,v22,t6,v0.t
vrgather.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v9,v17,t1,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v11,v19,t3,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v13,v21,t5,v0.t
vrgather.vx v14,v22,t6,v0.t
vrgather.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervi_m1:
	m_nop
	li a0, WARMUP
1:
vrgather.vi v8,v16,3
vrgather.vi v9,v17,3
vrgather.vi v10,v18,3
vrgather.vi v11,v19,3
vrgather.vi v12,v20,3
vrgather.vi v13,v21,3
vrgather.vi v14,v22,3
vrgather.vi v15,v23,3



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vi v8,v16,3
vrgather.vi v9,v17,3
vrgather.vi v10,v18,3
vrgather.vi v11,v19,3
vrgather.vi v12,v20,3
vrgather.vi v13,v21,3
vrgather.vi v14,v22,3
vrgather.vi v15,v23,3



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervim_m1:
	m_nop
	li a0, WARMUP
1:
vrgather.vi v8,v16,3,v0.t
vrgather.vi v9,v17,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v11,v19,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v13,v21,3,v0.t
vrgather.vi v14,v22,3,v0.t
vrgather.vi v15,v23,3,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgather.vi v8,v16,3,v0.t
vrgather.vi v9,v17,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v11,v19,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v13,v21,3,v0.t
vrgather.vi v14,v22,3,v0.t
vrgather.vi v15,v23,3,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvx_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vslideup.vx v8,v16,t0
vslideup.vx v9,v17,t1
vslideup.vx v10,v18,t2
vslideup.vx v11,v19,t3
vslideup.vx v12,v20,t4
vslideup.vx v13,v21,t5
vslideup.vx v14,v22,t6
vslideup.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslideup.vx v8,v16,t0
vslideup.vx v9,v17,t1
vslideup.vx v10,v18,t2
vslideup.vx v11,v19,t3
vslideup.vx v12,v20,t4
vslideup.vx v13,v21,t5
vslideup.vx v14,v22,t6
vslideup.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvxm_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v9,v17,t1,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v11,v19,t3,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v13,v21,t5,v0.t
vslideup.vx v14,v22,t6,v0.t
vslideup.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v9,v17,t1,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v11,v19,t3,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v13,v21,t5,v0.t
vslideup.vx v14,v22,t6,v0.t
vslideup.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvi_m1:
	m_nop
	li a0, WARMUP
1:
vslideup.vi v8,v16,3
vslideup.vi v9,v17,3
vslideup.vi v10,v18,3
vslideup.vi v11,v19,3
vslideup.vi v12,v20,3
vslideup.vi v13,v21,3
vslideup.vi v14,v22,3
vslideup.vi v15,v23,3



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslideup.vi v8,v16,3
vslideup.vi v9,v17,3
vslideup.vi v10,v18,3
vslideup.vi v11,v19,3
vslideup.vi v12,v20,3
vslideup.vi v13,v21,3
vslideup.vi v14,v22,3
vslideup.vi v15,v23,3



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvim_m1:
	m_nop
	li a0, WARMUP
1:
vslideup.vi v8,v16,3,v0.t
vslideup.vi v9,v17,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v11,v19,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v13,v21,3,v0.t
vslideup.vi v14,v22,3,v0.t
vslideup.vi v15,v23,3,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslideup.vi v8,v16,3,v0.t
vslideup.vi v9,v17,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v11,v19,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v13,v21,3,v0.t
vslideup.vi v14,v22,3,v0.t
vslideup.vi v15,v23,3,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vv_m1:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v9,v17,v25
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v11,v19,v27
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v13,v21,v29
vrgatherei16.vv v14,v22,v30
vrgatherei16.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v9,v17,v25
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v11,v19,v27
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v13,v21,v29
vrgatherei16.vv v14,v22,v30
vrgatherei16.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vvm_m1:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v9,v17,v25,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v11,v19,v27,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v13,v21,v29,v0.t
vrgatherei16.vv v14,v22,v30,v0.t
vrgatherei16.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v9,v17,v25,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v11,v19,v27,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v13,v21,v29,v0.t
vrgatherei16.vv v14,v22,v30,v0.t
vrgatherei16.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslidedownvx_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vslidedown.vx v8,v16,t0
vslidedown.vx v9,v17,t1
vslidedown.vx v10,v18,t2
vslidedown.vx v11,v19,t3
vslidedown.vx v12,v20,t4
vslidedown.vx v13,v21,t5
vslidedown.vx v14,v22,t6
vslidedown.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslidedown.vx v8,v16,t0
vslidedown.vx v9,v17,t1
vslidedown.vx v10,v18,t2
vslidedown.vx v11,v19,t3
vslidedown.vx v12,v20,t4
vslidedown.vx v13,v21,t5
vslidedown.vx v14,v22,t6
vslidedown.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvxm_m1:
	m_mod_t0_vl
	li a0, WARMUP
1:
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v9,v17,t1,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v11,v19,t3,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v13,v21,t5,v0.t
vslidedown.vx v14,v22,t6,v0.t
vslidedown.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v9,v17,t1,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v11,v19,t3,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v13,v21,t5,v0.t
vslidedown.vx v14,v22,t6,v0.t
vslidedown.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvi_m1:
	m_nop
	li a0, WARMUP
1:
vslidedown.vi v8,v16,3
vslidedown.vi v9,v17,3
vslidedown.vi v10,v18,3
vslidedown.vi v11,v19,3
vslidedown.vi v12,v20,3
vslidedown.vi v13,v21,3
vslidedown.vi v14,v22,3
vslidedown.vi v15,v23,3



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslidedown.vi v8,v16,3
vslidedown.vi v9,v17,3
vslidedown.vi v10,v18,3
vslidedown.vi v11,v19,3
vslidedown.vi v12,v20,3
vslidedown.vi v13,v21,3
vslidedown.vi v14,v22,3
vslidedown.vi v15,v23,3



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvim_m1:
	m_nop
	li a0, WARMUP
1:
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v9,v17,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v11,v19,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v13,v21,3,v0.t
vslidedown.vi v14,v22,3,v0.t
vslidedown.vi v15,v23,3,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v9,v17,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v11,v19,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v13,v21,3,v0.t
vslidedown.vi v14,v22,3,v0.t
vslidedown.vi v15,v23,3,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vredsumvs_m1:
	m_nop
	li a0, WARMUP
1:
vredsum.vs v8,v16,v24
vredsum.vs v9,v17,v25
vredsum.vs v10,v18,v26
vredsum.vs v11,v19,v27
vredsum.vs v12,v20,v28
vredsum.vs v13,v21,v29
vredsum.vs v14,v22,v30
vredsum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredsum.vs v8,v16,v24
vredsum.vs v9,v17,v25
vredsum.vs v10,v18,v26
vredsum.vs v11,v19,v27
vredsum.vs v12,v20,v28
vredsum.vs v13,v21,v29
vredsum.vs v14,v22,v30
vredsum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredsumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v9,v17,v25,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v11,v19,v27,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v13,v21,v29,v0.t
vredsum.vs v14,v22,v30,v0.t
vredsum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v9,v17,v25,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v11,v19,v27,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v13,v21,v29,v0.t
vredsum.vs v14,v22,v30,v0.t
vredsum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvs_m1:
	m_nop
	li a0, WARMUP
1:
vredand.vs v8,v16,v24
vredand.vs v9,v17,v25
vredand.vs v10,v18,v26
vredand.vs v11,v19,v27
vredand.vs v12,v20,v28
vredand.vs v13,v21,v29
vredand.vs v14,v22,v30
vredand.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredand.vs v8,v16,v24
vredand.vs v9,v17,v25
vredand.vs v10,v18,v26
vredand.vs v11,v19,v27
vredand.vs v12,v20,v28
vredand.vs v13,v21,v29
vredand.vs v14,v22,v30
vredand.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredand.vs v8,v16,v24,v0.t
vredand.vs v9,v17,v25,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v11,v19,v27,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v13,v21,v29,v0.t
vredand.vs v14,v22,v30,v0.t
vredand.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredand.vs v8,v16,v24,v0.t
vredand.vs v9,v17,v25,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v11,v19,v27,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v13,v21,v29,v0.t
vredand.vs v14,v22,v30,v0.t
vredand.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvs_m1:
	m_nop
	li a0, WARMUP
1:
vredor.vs v8,v16,v24
vredor.vs v9,v17,v25
vredor.vs v10,v18,v26
vredor.vs v11,v19,v27
vredor.vs v12,v20,v28
vredor.vs v13,v21,v29
vredor.vs v14,v22,v30
vredor.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredor.vs v8,v16,v24
vredor.vs v9,v17,v25
vredor.vs v10,v18,v26
vredor.vs v11,v19,v27
vredor.vs v12,v20,v28
vredor.vs v13,v21,v29
vredor.vs v14,v22,v30
vredor.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredor.vs v8,v16,v24,v0.t
vredor.vs v9,v17,v25,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v11,v19,v27,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v13,v21,v29,v0.t
vredor.vs v14,v22,v30,v0.t
vredor.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredor.vs v8,v16,v24,v0.t
vredor.vs v9,v17,v25,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v11,v19,v27,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v13,v21,v29,v0.t
vredor.vs v14,v22,v30,v0.t
vredor.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvs_m1:
	m_nop
	li a0, WARMUP
1:
vredxor.vs v8,v16,v24
vredxor.vs v9,v17,v25
vredxor.vs v10,v18,v26
vredxor.vs v11,v19,v27
vredxor.vs v12,v20,v28
vredxor.vs v13,v21,v29
vredxor.vs v14,v22,v30
vredxor.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredxor.vs v8,v16,v24
vredxor.vs v9,v17,v25
vredxor.vs v10,v18,v26
vredxor.vs v11,v19,v27
vredxor.vs v12,v20,v28
vredxor.vs v13,v21,v29
vredxor.vs v14,v22,v30
vredxor.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v9,v17,v25,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v11,v19,v27,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v13,v21,v29,v0.t
vredxor.vs v14,v22,v30,v0.t
vredxor.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v9,v17,v25,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v11,v19,v27,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v13,v21,v29,v0.t
vredxor.vs v14,v22,v30,v0.t
vredxor.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvs_m1:
	m_nop
	li a0, WARMUP
1:
vredminu.vs v8,v16,v24
vredminu.vs v9,v17,v25
vredminu.vs v10,v18,v26
vredminu.vs v11,v19,v27
vredminu.vs v12,v20,v28
vredminu.vs v13,v21,v29
vredminu.vs v14,v22,v30
vredminu.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredminu.vs v8,v16,v24
vredminu.vs v9,v17,v25
vredminu.vs v10,v18,v26
vredminu.vs v11,v19,v27
vredminu.vs v12,v20,v28
vredminu.vs v13,v21,v29
vredminu.vs v14,v22,v30
vredminu.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v9,v17,v25,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v11,v19,v27,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v13,v21,v29,v0.t
vredminu.vs v14,v22,v30,v0.t
vredminu.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v9,v17,v25,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v11,v19,v27,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v13,v21,v29,v0.t
vredminu.vs v14,v22,v30,v0.t
vredminu.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvs_m1:
	m_nop
	li a0, WARMUP
1:
vredmin.vs v8,v16,v24
vredmin.vs v9,v17,v25
vredmin.vs v10,v18,v26
vredmin.vs v11,v19,v27
vredmin.vs v12,v20,v28
vredmin.vs v13,v21,v29
vredmin.vs v14,v22,v30
vredmin.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmin.vs v8,v16,v24
vredmin.vs v9,v17,v25
vredmin.vs v10,v18,v26
vredmin.vs v11,v19,v27
vredmin.vs v12,v20,v28
vredmin.vs v13,v21,v29
vredmin.vs v14,v22,v30
vredmin.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v9,v17,v25,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v11,v19,v27,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v13,v21,v29,v0.t
vredmin.vs v14,v22,v30,v0.t
vredmin.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v9,v17,v25,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v11,v19,v27,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v13,v21,v29,v0.t
vredmin.vs v14,v22,v30,v0.t
vredmin.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvs_m1:
	m_nop
	li a0, WARMUP
1:
vredmaxu.vs v8,v16,v24
vredmaxu.vs v9,v17,v25
vredmaxu.vs v10,v18,v26
vredmaxu.vs v11,v19,v27
vredmaxu.vs v12,v20,v28
vredmaxu.vs v13,v21,v29
vredmaxu.vs v14,v22,v30
vredmaxu.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmaxu.vs v8,v16,v24
vredmaxu.vs v9,v17,v25
vredmaxu.vs v10,v18,v26
vredmaxu.vs v11,v19,v27
vredmaxu.vs v12,v20,v28
vredmaxu.vs v13,v21,v29
vredmaxu.vs v14,v22,v30
vredmaxu.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v9,v17,v25,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v11,v19,v27,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v13,v21,v29,v0.t
vredmaxu.vs v14,v22,v30,v0.t
vredmaxu.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v9,v17,v25,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v11,v19,v27,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v13,v21,v29,v0.t
vredmaxu.vs v14,v22,v30,v0.t
vredmaxu.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvs_m1:
	m_nop
	li a0, WARMUP
1:
vredmax.vs v8,v16,v24
vredmax.vs v9,v17,v25
vredmax.vs v10,v18,v26
vredmax.vs v11,v19,v27
vredmax.vs v12,v20,v28
vredmax.vs v13,v21,v29
vredmax.vs v14,v22,v30
vredmax.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmax.vs v8,v16,v24
vredmax.vs v9,v17,v25
vredmax.vs v10,v18,v26
vredmax.vs v11,v19,v27
vredmax.vs v12,v20,v28
vredmax.vs v13,v21,v29
vredmax.vs v14,v22,v30
vredmax.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvsm_m1:
	m_nop
	li a0, WARMUP
1:
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v9,v17,v25,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v11,v19,v27,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v13,v21,v29,v0.t
vredmax.vs v14,v22,v30,v0.t
vredmax.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v9,v17,v25,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v11,v19,v27,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v13,v21,v29,v0.t
vredmax.vs v14,v22,v30,v0.t
vredmax.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vaadduvv_m1:
	m_nop
	li a0, WARMUP
1:
vaaddu.vv v8,v16,v24
vaaddu.vv v9,v17,v25
vaaddu.vv v10,v18,v26
vaaddu.vv v11,v19,v27
vaaddu.vv v12,v20,v28
vaaddu.vv v13,v21,v29
vaaddu.vv v14,v22,v30
vaaddu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaaddu.vv v8,v16,v24
vaaddu.vv v9,v17,v25
vaaddu.vv v10,v18,v26
vaaddu.vv v11,v19,v27
vaaddu.vv v12,v20,v28
vaaddu.vv v13,v21,v29
vaaddu.vv v14,v22,v30
vaaddu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvvm_m1:
	m_nop
	li a0, WARMUP
1:
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v9,v17,v25,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v11,v19,v27,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v13,v21,v29,v0.t
vaaddu.vv v14,v22,v30,v0.t
vaaddu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v9,v17,v25,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v11,v19,v27,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v13,v21,v29,v0.t
vaaddu.vv v14,v22,v30,v0.t
vaaddu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvx_m1:
	m_nop
	li a0, WARMUP
1:
vaaddu.vx v8,v16,t0
vaaddu.vx v9,v17,t1
vaaddu.vx v10,v18,t2
vaaddu.vx v11,v19,t3
vaaddu.vx v12,v20,t4
vaaddu.vx v13,v21,t5
vaaddu.vx v14,v22,t6
vaaddu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaaddu.vx v8,v16,t0
vaaddu.vx v9,v17,t1
vaaddu.vx v10,v18,t2
vaaddu.vx v11,v19,t3
vaaddu.vx v12,v20,t4
vaaddu.vx v13,v21,t5
vaaddu.vx v14,v22,t6
vaaddu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvxm_m1:
	m_nop
	li a0, WARMUP
1:
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v9,v17,t1,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v11,v19,t3,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v13,v21,t5,v0.t
vaaddu.vx v14,v22,t6,v0.t
vaaddu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v9,v17,t1,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v11,v19,t3,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v13,v21,t5,v0.t
vaaddu.vx v14,v22,t6,v0.t
vaaddu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vaadd.vv v8,v16,v24
vaadd.vv v9,v17,v25
vaadd.vv v10,v18,v26
vaadd.vv v11,v19,v27
vaadd.vv v12,v20,v28
vaadd.vv v13,v21,v29
vaadd.vv v14,v22,v30
vaadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaadd.vv v8,v16,v24
vaadd.vv v9,v17,v25
vaadd.vv v10,v18,v26
vaadd.vv v11,v19,v27
vaadd.vv v12,v20,v28
vaadd.vv v13,v21,v29
vaadd.vv v14,v22,v30
vaadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v9,v17,v25,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v11,v19,v27,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v13,v21,v29,v0.t
vaadd.vv v14,v22,v30,v0.t
vaadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v9,v17,v25,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v11,v19,v27,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v13,v21,v29,v0.t
vaadd.vv v14,v22,v30,v0.t
vaadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvx_m1:
	m_nop
	li a0, WARMUP
1:
vaadd.vx v8,v16,t0
vaadd.vx v9,v17,t1
vaadd.vx v10,v18,t2
vaadd.vx v11,v19,t3
vaadd.vx v12,v20,t4
vaadd.vx v13,v21,t5
vaadd.vx v14,v22,t6
vaadd.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaadd.vx v8,v16,t0
vaadd.vx v9,v17,t1
vaadd.vx v10,v18,t2
vaadd.vx v11,v19,t3
vaadd.vx v12,v20,t4
vaadd.vx v13,v21,t5
vaadd.vx v14,v22,t6
vaadd.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvxm_m1:
	m_nop
	li a0, WARMUP
1:
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v9,v17,t1,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v11,v19,t3,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v13,v21,t5,v0.t
vaadd.vx v14,v22,t6,v0.t
vaadd.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v9,v17,t1,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v11,v19,t3,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v13,v21,t5,v0.t
vaadd.vx v14,v22,t6,v0.t
vaadd.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvv_m1:
	m_nop
	li a0, WARMUP
1:
vasubu.vv v8,v16,v24
vasubu.vv v9,v17,v25
vasubu.vv v10,v18,v26
vasubu.vv v11,v19,v27
vasubu.vv v12,v20,v28
vasubu.vv v13,v21,v29
vasubu.vv v14,v22,v30
vasubu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasubu.vv v8,v16,v24
vasubu.vv v9,v17,v25
vasubu.vv v10,v18,v26
vasubu.vv v11,v19,v27
vasubu.vv v12,v20,v28
vasubu.vv v13,v21,v29
vasubu.vv v14,v22,v30
vasubu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v9,v17,v25,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v11,v19,v27,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v13,v21,v29,v0.t
vasubu.vv v14,v22,v30,v0.t
vasubu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v9,v17,v25,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v11,v19,v27,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v13,v21,v29,v0.t
vasubu.vv v14,v22,v30,v0.t
vasubu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvx_m1:
	m_nop
	li a0, WARMUP
1:
vasubu.vx v8,v16,t0
vasubu.vx v9,v17,t1
vasubu.vx v10,v18,t2
vasubu.vx v11,v19,t3
vasubu.vx v12,v20,t4
vasubu.vx v13,v21,t5
vasubu.vx v14,v22,t6
vasubu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasubu.vx v8,v16,t0
vasubu.vx v9,v17,t1
vasubu.vx v10,v18,t2
vasubu.vx v11,v19,t3
vasubu.vx v12,v20,t4
vasubu.vx v13,v21,t5
vasubu.vx v14,v22,t6
vasubu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v9,v17,t1,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v11,v19,t3,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v13,v21,t5,v0.t
vasubu.vx v14,v22,t6,v0.t
vasubu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v9,v17,t1,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v11,v19,t3,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v13,v21,t5,v0.t
vasubu.vx v14,v22,t6,v0.t
vasubu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvv_m1:
	m_nop
	li a0, WARMUP
1:
vasub.vv v8,v16,v24
vasub.vv v9,v17,v25
vasub.vv v10,v18,v26
vasub.vv v11,v19,v27
vasub.vv v12,v20,v28
vasub.vv v13,v21,v29
vasub.vv v14,v22,v30
vasub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasub.vv v8,v16,v24
vasub.vv v9,v17,v25
vasub.vv v10,v18,v26
vasub.vv v11,v19,v27
vasub.vv v12,v20,v28
vasub.vv v13,v21,v29
vasub.vv v14,v22,v30
vasub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vasub.vv v8,v16,v24,v0.t
vasub.vv v9,v17,v25,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v11,v19,v27,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v13,v21,v29,v0.t
vasub.vv v14,v22,v30,v0.t
vasub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasub.vv v8,v16,v24,v0.t
vasub.vv v9,v17,v25,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v11,v19,v27,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v13,v21,v29,v0.t
vasub.vv v14,v22,v30,v0.t
vasub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvx_m1:
	m_nop
	li a0, WARMUP
1:
vasub.vx v8,v16,t0
vasub.vx v9,v17,t1
vasub.vx v10,v18,t2
vasub.vx v11,v19,t3
vasub.vx v12,v20,t4
vasub.vx v13,v21,t5
vasub.vx v14,v22,t6
vasub.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasub.vx v8,v16,t0
vasub.vx v9,v17,t1
vasub.vx v10,v18,t2
vasub.vx v11,v19,t3
vasub.vx v12,v20,t4
vasub.vx v13,v21,t5
vasub.vx v14,v22,t6
vasub.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vasub.vx v8,v16,t0,v0.t
vasub.vx v9,v17,t1,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v11,v19,t3,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v13,v21,t5,v0.t
vasub.vx v14,v22,t6,v0.t
vasub.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vasub.vx v8,v16,t0,v0.t
vasub.vx v9,v17,t1,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v11,v19,t3,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v13,v21,t5,v0.t
vasub.vx v14,v22,t6,v0.t
vasub.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslide1upvx_m1:
	m_nop
	li a0, WARMUP
1:
vslide1up.vx v8,v16,t0
vslide1up.vx v9,v17,t1
vslide1up.vx v10,v18,t2
vslide1up.vx v11,v19,t3
vslide1up.vx v12,v20,t4
vslide1up.vx v13,v21,t5
vslide1up.vx v14,v22,t6
vslide1up.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslide1up.vx v8,v16,t0
vslide1up.vx v9,v17,t1
vslide1up.vx v10,v18,t2
vslide1up.vx v11,v19,t3
vslide1up.vx v12,v20,t4
vslide1up.vx v13,v21,t5
vslide1up.vx v14,v22,t6
vslide1up.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1upvxm_m1:
	m_nop
	li a0, WARMUP
1:
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v9,v17,t1,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v11,v19,t3,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v13,v21,t5,v0.t
vslide1up.vx v14,v22,t6,v0.t
vslide1up.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v9,v17,t1,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v11,v19,t3,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v13,v21,t5,v0.t
vslide1up.vx v14,v22,t6,v0.t
vslide1up.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvx_m1:
	m_nop
	li a0, WARMUP
1:
vslide1down.vx v8,v16,t0
vslide1down.vx v9,v17,t1
vslide1down.vx v10,v18,t2
vslide1down.vx v11,v19,t3
vslide1down.vx v12,v20,t4
vslide1down.vx v13,v21,t5
vslide1down.vx v14,v22,t6
vslide1down.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslide1down.vx v8,v16,t0
vslide1down.vx v9,v17,t1
vslide1down.vx v10,v18,t2
vslide1down.vx v11,v19,t3
vslide1down.vx v12,v20,t4
vslide1down.vx v13,v21,t5
vslide1down.vx v14,v22,t6
vslide1down.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvxm_m1:
	m_nop
	li a0, WARMUP
1:
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v9,v17,t1,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v11,v19,t3,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v13,v21,t5,v0.t
vslide1down.vx v14,v22,t6,v0.t
vslide1down.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v9,v17,t1,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v11,v19,t3,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v13,v21,t5,v0.t
vslide1down.vx v14,v22,t6,v0.t
vslide1down.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vadcvvm_m1:
	m_nop
	li a0, WARMUP
1:
vadc.vvm v8,v16,v24,v0
vadc.vvm v9,v17,v25,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v11,v19,v27,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v13,v21,v29,v0
vadc.vvm v14,v22,v30,v0
vadc.vvm v15,v23,v31,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadc.vvm v8,v16,v24,v0
vadc.vvm v9,v17,v25,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v11,v19,v27,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v13,v21,v29,v0
vadc.vvm v14,v22,v30,v0
vadc.vvm v15,v23,v31,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvxm_m1:
	m_nop
	li a0, WARMUP
1:
vadc.vxm v8,v16,t0,v0
vadc.vxm v9,v17,t1,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v11,v19,t3,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v13,v21,t5,v0
vadc.vxm v14,v22,t6,v0
vadc.vxm v15,v23,t7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadc.vxm v8,v16,t0,v0
vadc.vxm v9,v17,t1,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v11,v19,t3,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v13,v21,t5,v0
vadc.vxm v14,v22,t6,v0
vadc.vxm v15,v23,t7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvim_m1:
	m_nop
	li a0, WARMUP
1:
vadc.vim v8,v16,13,v0
vadc.vim v9,v17,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v11,v19,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v13,v21,13,v0
vadc.vim v14,v22,13,v0
vadc.vim v15,v23,13,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vadc.vim v8,v16,13,v0
vadc.vim v9,v17,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v11,v19,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v13,v21,13,v0
vadc.vim v14,v22,13,v0
vadc.vim v15,v23,13,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v9,v17,v25,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v11,v19,v27,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v13,v21,v29,v0
vmadc.vvm v14,v22,v30,v0
vmadc.vvm v15,v23,v31,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v9,v17,v25,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v11,v19,v27,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v13,v21,v29,v0
vmadc.vvm v14,v22,v30,v0
vmadc.vvm v15,v23,v31,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v9,v17,t1,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v11,v19,t3,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v13,v21,t5,v0
vmadc.vxm v14,v22,t6,v0
vmadc.vxm v15,v23,t7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v9,v17,t1,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v11,v19,t3,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v13,v21,t5,v0
vmadc.vxm v14,v22,t6,v0
vmadc.vxm v15,v23,t7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvim_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vim v8,v16,13,v0
vmadc.vim v9,v17,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v11,v19,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v13,v21,13,v0
vmadc.vim v14,v22,13,v0
vmadc.vim v15,v23,13,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vim v8,v16,13,v0
vmadc.vim v9,v17,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v11,v19,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v13,v21,13,v0
vmadc.vim v14,v22,13,v0
vmadc.vim v15,v23,13,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvv_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vv v8,v16,v24
vmadc.vv v9,v17,v25
vmadc.vv v10,v18,v26
vmadc.vv v11,v19,v27
vmadc.vv v12,v20,v28
vmadc.vv v13,v21,v29
vmadc.vv v14,v22,v30
vmadc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vv v8,v16,v24
vmadc.vv v9,v17,v25
vmadc.vv v10,v18,v26
vmadc.vv v11,v19,v27
vmadc.vv v12,v20,v28
vmadc.vv v13,v21,v29
vmadc.vv v14,v22,v30
vmadc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvx_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vx v8,v16,t0
vmadc.vx v9,v17,t1
vmadc.vx v10,v18,t2
vmadc.vx v11,v19,t3
vmadc.vx v12,v20,t4
vmadc.vx v13,v21,t5
vmadc.vx v14,v22,t6
vmadc.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vx v8,v16,t0
vmadc.vx v9,v17,t1
vmadc.vx v10,v18,t2
vmadc.vx v11,v19,t3
vmadc.vx v12,v20,t4
vmadc.vx v13,v21,t5
vmadc.vx v14,v22,t6
vmadc.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvi_m1:
	m_nop
	li a0, WARMUP
1:
vmadc.vi v8,v16,13
vmadc.vi v9,v17,13
vmadc.vi v10,v18,13
vmadc.vi v11,v19,13
vmadc.vi v12,v20,13
vmadc.vi v13,v21,13
vmadc.vi v14,v22,13
vmadc.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadc.vi v8,v16,13
vmadc.vi v9,v17,13
vmadc.vi v10,v18,13
vmadc.vi v11,v19,13
vmadc.vi v12,v20,13
vmadc.vi v13,v21,13
vmadc.vi v14,v22,13
vmadc.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v9,v17,v25,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v11,v19,v27,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v13,v21,v29,v0
vsbc.vvm v14,v22,v30,v0
vsbc.vvm v15,v23,v31,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v9,v17,v25,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v11,v19,v27,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v13,v21,v29,v0
vsbc.vvm v14,v22,v30,v0
vsbc.vvm v15,v23,v31,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v9,v17,t1,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v11,v19,t3,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v13,v21,t5,v0
vsbc.vxm v14,v22,t6,v0
vsbc.vxm v15,v23,t7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v9,v17,t1,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v11,v19,t3,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v13,v21,t5,v0
vsbc.vxm v14,v22,t6,v0
vsbc.vxm v15,v23,t7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v9,v17,v25,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v11,v19,v27,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v13,v21,v29,v0
vmsbc.vvm v14,v22,v30,v0
vmsbc.vvm v15,v23,v31,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v9,v17,v25,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v11,v19,v27,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v13,v21,v29,v0
vmsbc.vvm v14,v22,v30,v0
vmsbc.vvm v15,v23,v31,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v9,v17,t1,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v11,v19,t3,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v13,v21,t5,v0
vmsbc.vxm v14,v22,t6,v0
vmsbc.vxm v15,v23,t7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v9,v17,t1,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v11,v19,t3,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v13,v21,t5,v0
vmsbc.vxm v14,v22,t6,v0
vmsbc.vxm v15,v23,t7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvv_m1:
	m_nop
	li a0, WARMUP
1:
vmsbc.vv v8,v16,v24
vmsbc.vv v9,v17,v25
vmsbc.vv v10,v18,v26
vmsbc.vv v11,v19,v27
vmsbc.vv v12,v20,v28
vmsbc.vv v13,v21,v29
vmsbc.vv v14,v22,v30
vmsbc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbc.vv v8,v16,v24
vmsbc.vv v9,v17,v25
vmsbc.vv v10,v18,v26
vmsbc.vv v11,v19,v27
vmsbc.vv v12,v20,v28
vmsbc.vv v13,v21,v29
vmsbc.vv v14,v22,v30
vmsbc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvx_m1:
	m_nop
	li a0, WARMUP
1:
vmsbc.vx v8,v16,t0
vmsbc.vx v9,v17,t1
vmsbc.vx v10,v18,t2
vmsbc.vx v11,v19,t3
vmsbc.vx v12,v20,t4
vmsbc.vx v13,v21,t5
vmsbc.vx v14,v22,t6
vmsbc.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbc.vx v8,v16,t0
vmsbc.vx v9,v17,t1
vmsbc.vx v10,v18,t2
vmsbc.vx v11,v19,t3
vmsbc.vx v12,v20,t4
vmsbc.vx v13,v21,t5
vmsbc.vx v14,v22,t6
vmsbc.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmergevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v9,v17,v25,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v11,v19,v27,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v13,v21,v29,v0
vmerge.vvm v14,v22,v30,v0
vmerge.vvm v15,v23,v31,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v9,v17,v25,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v11,v19,v27,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v13,v21,v29,v0
vmerge.vvm v14,v22,v30,v0
vmerge.vvm v15,v23,v31,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevxm_m1:
	m_nop
	li a0, WARMUP
1:
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v9,v17,t1,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v11,v19,t3,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v13,v21,t5,v0
vmerge.vxm v14,v22,t6,v0
vmerge.vxm v15,v23,t7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v9,v17,t1,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v11,v19,t3,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v13,v21,t5,v0
vmerge.vxm v14,v22,t6,v0
vmerge.vxm v15,v23,t7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevim_m1:
	m_nop
	li a0, WARMUP
1:
vmerge.vim v8,v16,13,v0
vmerge.vim v9,v17,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v11,v19,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v13,v21,13,v0
vmerge.vim v14,v22,13,v0
vmerge.vim v15,v23,13,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmerge.vim v8,v16,13,v0
vmerge.vim v9,v17,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v11,v19,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v13,v21,13,v0
vmerge.vim v14,v22,13,v0
vmerge.vim v15,v23,13,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvv_m1:
	m_nop
	li a0, WARMUP
1:
vmv.v.v v8,v16
vmv.v.v v9,v17
vmv.v.v v10,v18
vmv.v.v v11,v19
vmv.v.v v12,v20
vmv.v.v v13,v21
vmv.v.v v14,v22
vmv.v.v v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv.v.v v8,v16
vmv.v.v v9,v17
vmv.v.v v10,v18
vmv.v.v v11,v19
vmv.v.v v12,v20
vmv.v.v v13,v21
vmv.v.v v14,v22
vmv.v.v v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvx_m1:
	m_nop
	li a0, WARMUP
1:
vmv.v.x v8,t0
vmv.v.x v9,t1
vmv.v.x v10,t2
vmv.v.x v11,t3
vmv.v.x v12,t4
vmv.v.x v13,t5
vmv.v.x v14,t6
vmv.v.x v15,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv.v.x v8,t0
vmv.v.x v9,t1
vmv.v.x v10,t2
vmv.v.x v11,t3
vmv.v.x v12,t4
vmv.v.x v13,t5
vmv.v.x v14,t6
vmv.v.x v15,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvi_m1:
	m_nop
	li a0, WARMUP
1:
vmv.v.i v8,13
vmv.v.i v9,13
vmv.v.i v10,13
vmv.v.i v11,13
vmv.v.i v12,13
vmv.v.i v13,13
vmv.v.i v14,13
vmv.v.i v15,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv.v.i v8,13
vmv.v.i v9,13
vmv.v.i v10,13
vmv.v.i v11,13
vmv.v.i v12,13
vmv.v.i v13,13
vmv.v.i v14,13
vmv.v.i v15,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvv_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vv v8,v16,v24
vmseq.vv v9,v17,v25
vmseq.vv v10,v18,v26
vmseq.vv v11,v19,v27
vmseq.vv v12,v20,v28
vmseq.vv v13,v21,v29
vmseq.vv v14,v22,v30
vmseq.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vv v8,v16,v24
vmseq.vv v9,v17,v25
vmseq.vv v10,v18,v26
vmseq.vv v11,v19,v27
vmseq.vv v12,v20,v28
vmseq.vv v13,v21,v29
vmseq.vv v14,v22,v30
vmseq.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v9,v17,v25,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v11,v19,v27,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v13,v21,v29,v0.t
vmseq.vv v14,v22,v30,v0.t
vmseq.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v9,v17,v25,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v11,v19,v27,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v13,v21,v29,v0.t
vmseq.vv v14,v22,v30,v0.t
vmseq.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvx_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vx v8,v16,t0
vmseq.vx v9,v17,t1
vmseq.vx v10,v18,t2
vmseq.vx v11,v19,t3
vmseq.vx v12,v20,t4
vmseq.vx v13,v21,t5
vmseq.vx v14,v22,t6
vmseq.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vx v8,v16,t0
vmseq.vx v9,v17,t1
vmseq.vx v10,v18,t2
vmseq.vx v11,v19,t3
vmseq.vx v12,v20,t4
vmseq.vx v13,v21,t5
vmseq.vx v14,v22,t6
vmseq.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v9,v17,t1,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v11,v19,t3,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v13,v21,t5,v0.t
vmseq.vx v14,v22,t6,v0.t
vmseq.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v9,v17,t1,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v11,v19,t3,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v13,v21,t5,v0.t
vmseq.vx v14,v22,t6,v0.t
vmseq.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvi_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vi v8,v16,13
vmseq.vi v9,v17,13
vmseq.vi v10,v18,13
vmseq.vi v11,v19,13
vmseq.vi v12,v20,13
vmseq.vi v13,v21,13
vmseq.vi v14,v22,13
vmseq.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vi v8,v16,13
vmseq.vi v9,v17,13
vmseq.vi v10,v18,13
vmseq.vi v11,v19,13
vmseq.vi v12,v20,13
vmseq.vi v13,v21,13
vmseq.vi v14,v22,13
vmseq.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvim_m1:
	m_nop
	li a0, WARMUP
1:
vmseq.vi v8,v16,13,v0.t
vmseq.vi v9,v17,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v11,v19,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v13,v21,13,v0.t
vmseq.vi v14,v22,13,v0.t
vmseq.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmseq.vi v8,v16,13,v0.t
vmseq.vi v9,v17,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v11,v19,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v13,v21,13,v0.t
vmseq.vi v14,v22,13,v0.t
vmseq.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevv_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vv v8,v16,v24
vmsne.vv v9,v17,v25
vmsne.vv v10,v18,v26
vmsne.vv v11,v19,v27
vmsne.vv v12,v20,v28
vmsne.vv v13,v21,v29
vmsne.vv v14,v22,v30
vmsne.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vv v8,v16,v24
vmsne.vv v9,v17,v25
vmsne.vv v10,v18,v26
vmsne.vv v11,v19,v27
vmsne.vv v12,v20,v28
vmsne.vv v13,v21,v29
vmsne.vv v14,v22,v30
vmsne.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v9,v17,v25,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v11,v19,v27,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v13,v21,v29,v0.t
vmsne.vv v14,v22,v30,v0.t
vmsne.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v9,v17,v25,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v11,v19,v27,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v13,v21,v29,v0.t
vmsne.vv v14,v22,v30,v0.t
vmsne.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevx_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vx v8,v16,t0
vmsne.vx v9,v17,t1
vmsne.vx v10,v18,t2
vmsne.vx v11,v19,t3
vmsne.vx v12,v20,t4
vmsne.vx v13,v21,t5
vmsne.vx v14,v22,t6
vmsne.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vx v8,v16,t0
vmsne.vx v9,v17,t1
vmsne.vx v10,v18,t2
vmsne.vx v11,v19,t3
vmsne.vx v12,v20,t4
vmsne.vx v13,v21,t5
vmsne.vx v14,v22,t6
vmsne.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v9,v17,t1,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v11,v19,t3,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v13,v21,t5,v0.t
vmsne.vx v14,v22,t6,v0.t
vmsne.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v9,v17,t1,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v11,v19,t3,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v13,v21,t5,v0.t
vmsne.vx v14,v22,t6,v0.t
vmsne.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevi_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vi v8,v16,13
vmsne.vi v9,v17,13
vmsne.vi v10,v18,13
vmsne.vi v11,v19,13
vmsne.vi v12,v20,13
vmsne.vi v13,v21,13
vmsne.vi v14,v22,13
vmsne.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vi v8,v16,13
vmsne.vi v9,v17,13
vmsne.vi v10,v18,13
vmsne.vi v11,v19,13
vmsne.vi v12,v20,13
vmsne.vi v13,v21,13
vmsne.vi v14,v22,13
vmsne.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevim_m1:
	m_nop
	li a0, WARMUP
1:
vmsne.vi v8,v16,13,v0.t
vmsne.vi v9,v17,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v11,v19,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v13,v21,13,v0.t
vmsne.vi v14,v22,13,v0.t
vmsne.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsne.vi v8,v16,13,v0.t
vmsne.vi v9,v17,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v11,v19,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v13,v21,13,v0.t
vmsne.vi v14,v22,13,v0.t
vmsne.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvv_m1:
	m_nop
	li a0, WARMUP
1:
vmsltu.vv v8,v16,v24
vmsltu.vv v9,v17,v25
vmsltu.vv v10,v18,v26
vmsltu.vv v11,v19,v27
vmsltu.vv v12,v20,v28
vmsltu.vv v13,v21,v29
vmsltu.vv v14,v22,v30
vmsltu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsltu.vv v8,v16,v24
vmsltu.vv v9,v17,v25
vmsltu.vv v10,v18,v26
vmsltu.vv v11,v19,v27
vmsltu.vv v12,v20,v28
vmsltu.vv v13,v21,v29
vmsltu.vv v14,v22,v30
vmsltu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v9,v17,v25,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v11,v19,v27,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v13,v21,v29,v0.t
vmsltu.vv v14,v22,v30,v0.t
vmsltu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v9,v17,v25,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v11,v19,v27,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v13,v21,v29,v0.t
vmsltu.vv v14,v22,v30,v0.t
vmsltu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmsltu.vx v8,v16,t0
vmsltu.vx v9,v17,t1
vmsltu.vx v10,v18,t2
vmsltu.vx v11,v19,t3
vmsltu.vx v12,v20,t4
vmsltu.vx v13,v21,t5
vmsltu.vx v14,v22,t6
vmsltu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsltu.vx v8,v16,t0
vmsltu.vx v9,v17,t1
vmsltu.vx v10,v18,t2
vmsltu.vx v11,v19,t3
vmsltu.vx v12,v20,t4
vmsltu.vx v13,v21,t5
vmsltu.vx v14,v22,t6
vmsltu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v9,v17,t1,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v11,v19,t3,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v13,v21,t5,v0.t
vmsltu.vx v14,v22,t6,v0.t
vmsltu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v9,v17,t1,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v11,v19,t3,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v13,v21,t5,v0.t
vmsltu.vx v14,v22,t6,v0.t
vmsltu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvv_m1:
	m_nop
	li a0, WARMUP
1:
vmslt.vv v8,v16,v24
vmslt.vv v9,v17,v25
vmslt.vv v10,v18,v26
vmslt.vv v11,v19,v27
vmslt.vv v12,v20,v28
vmslt.vv v13,v21,v29
vmslt.vv v14,v22,v30
vmslt.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmslt.vv v8,v16,v24
vmslt.vv v9,v17,v25
vmslt.vv v10,v18,v26
vmslt.vv v11,v19,v27
vmslt.vv v12,v20,v28
vmslt.vv v13,v21,v29
vmslt.vv v14,v22,v30
vmslt.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v9,v17,v25,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v11,v19,v27,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v13,v21,v29,v0.t
vmslt.vv v14,v22,v30,v0.t
vmslt.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v9,v17,v25,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v11,v19,v27,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v13,v21,v29,v0.t
vmslt.vv v14,v22,v30,v0.t
vmslt.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvx_m1:
	m_nop
	li a0, WARMUP
1:
vmslt.vx v8,v16,t0
vmslt.vx v9,v17,t1
vmslt.vx v10,v18,t2
vmslt.vx v11,v19,t3
vmslt.vx v12,v20,t4
vmslt.vx v13,v21,t5
vmslt.vx v14,v22,t6
vmslt.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmslt.vx v8,v16,t0
vmslt.vx v9,v17,t1
vmslt.vx v10,v18,t2
vmslt.vx v11,v19,t3
vmslt.vx v12,v20,t4
vmslt.vx v13,v21,t5
vmslt.vx v14,v22,t6
vmslt.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v9,v17,t1,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v11,v19,t3,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v13,v21,t5,v0.t
vmslt.vx v14,v22,t6,v0.t
vmslt.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v9,v17,t1,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v11,v19,t3,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v13,v21,t5,v0.t
vmslt.vx v14,v22,t6,v0.t
vmslt.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvv_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vv v8,v16,v24
vmsleu.vv v9,v17,v25
vmsleu.vv v10,v18,v26
vmsleu.vv v11,v19,v27
vmsleu.vv v12,v20,v28
vmsleu.vv v13,v21,v29
vmsleu.vv v14,v22,v30
vmsleu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vv v8,v16,v24
vmsleu.vv v9,v17,v25
vmsleu.vv v10,v18,v26
vmsleu.vv v11,v19,v27
vmsleu.vv v12,v20,v28
vmsleu.vv v13,v21,v29
vmsleu.vv v14,v22,v30
vmsleu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v9,v17,v25,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v11,v19,v27,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v13,v21,v29,v0.t
vmsleu.vv v14,v22,v30,v0.t
vmsleu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v9,v17,v25,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v11,v19,v27,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v13,v21,v29,v0.t
vmsleu.vv v14,v22,v30,v0.t
vmsleu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vx v8,v16,t0
vmsleu.vx v9,v17,t1
vmsleu.vx v10,v18,t2
vmsleu.vx v11,v19,t3
vmsleu.vx v12,v20,t4
vmsleu.vx v13,v21,t5
vmsleu.vx v14,v22,t6
vmsleu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vx v8,v16,t0
vmsleu.vx v9,v17,t1
vmsleu.vx v10,v18,t2
vmsleu.vx v11,v19,t3
vmsleu.vx v12,v20,t4
vmsleu.vx v13,v21,t5
vmsleu.vx v14,v22,t6
vmsleu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v9,v17,t1,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v11,v19,t3,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v13,v21,t5,v0.t
vmsleu.vx v14,v22,t6,v0.t
vmsleu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v9,v17,t1,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v11,v19,t3,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v13,v21,t5,v0.t
vmsleu.vx v14,v22,t6,v0.t
vmsleu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvi_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vi v8,v16,13
vmsleu.vi v9,v17,13
vmsleu.vi v10,v18,13
vmsleu.vi v11,v19,13
vmsleu.vi v12,v20,13
vmsleu.vi v13,v21,13
vmsleu.vi v14,v22,13
vmsleu.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vi v8,v16,13
vmsleu.vi v9,v17,13
vmsleu.vi v10,v18,13
vmsleu.vi v11,v19,13
vmsleu.vi v12,v20,13
vmsleu.vi v13,v21,13
vmsleu.vi v14,v22,13
vmsleu.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvim_m1:
	m_nop
	li a0, WARMUP
1:
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v9,v17,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v11,v19,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v13,v21,13,v0.t
vmsleu.vi v14,v22,13,v0.t
vmsleu.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v9,v17,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v11,v19,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v13,v21,13,v0.t
vmsleu.vi v14,v22,13,v0.t
vmsleu.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevv_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vv v8,v16,v24
vmsle.vv v9,v17,v25
vmsle.vv v10,v18,v26
vmsle.vv v11,v19,v27
vmsle.vv v12,v20,v28
vmsle.vv v13,v21,v29
vmsle.vv v14,v22,v30
vmsle.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vv v8,v16,v24
vmsle.vv v9,v17,v25
vmsle.vv v10,v18,v26
vmsle.vv v11,v19,v27
vmsle.vv v12,v20,v28
vmsle.vv v13,v21,v29
vmsle.vv v14,v22,v30
vmsle.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v9,v17,v25,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v11,v19,v27,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v13,v21,v29,v0.t
vmsle.vv v14,v22,v30,v0.t
vmsle.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v9,v17,v25,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v11,v19,v27,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v13,v21,v29,v0.t
vmsle.vv v14,v22,v30,v0.t
vmsle.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevx_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vx v8,v16,t0
vmsle.vx v9,v17,t1
vmsle.vx v10,v18,t2
vmsle.vx v11,v19,t3
vmsle.vx v12,v20,t4
vmsle.vx v13,v21,t5
vmsle.vx v14,v22,t6
vmsle.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vx v8,v16,t0
vmsle.vx v9,v17,t1
vmsle.vx v10,v18,t2
vmsle.vx v11,v19,t3
vmsle.vx v12,v20,t4
vmsle.vx v13,v21,t5
vmsle.vx v14,v22,t6
vmsle.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v9,v17,t1,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v11,v19,t3,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v13,v21,t5,v0.t
vmsle.vx v14,v22,t6,v0.t
vmsle.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v9,v17,t1,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v11,v19,t3,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v13,v21,t5,v0.t
vmsle.vx v14,v22,t6,v0.t
vmsle.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevi_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vi v8,v16,13
vmsle.vi v9,v17,13
vmsle.vi v10,v18,13
vmsle.vi v11,v19,13
vmsle.vi v12,v20,13
vmsle.vi v13,v21,13
vmsle.vi v14,v22,13
vmsle.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vi v8,v16,13
vmsle.vi v9,v17,13
vmsle.vi v10,v18,13
vmsle.vi v11,v19,13
vmsle.vi v12,v20,13
vmsle.vi v13,v21,13
vmsle.vi v14,v22,13
vmsle.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevim_m1:
	m_nop
	li a0, WARMUP
1:
vmsle.vi v8,v16,13,v0.t
vmsle.vi v9,v17,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v11,v19,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v13,v21,13,v0.t
vmsle.vi v14,v22,13,v0.t
vmsle.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsle.vi v8,v16,13,v0.t
vmsle.vi v9,v17,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v11,v19,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v13,v21,13,v0.t
vmsle.vi v14,v22,13,v0.t
vmsle.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmsgtu.vx v8,v16,t0
vmsgtu.vx v9,v17,t1
vmsgtu.vx v10,v18,t2
vmsgtu.vx v11,v19,t3
vmsgtu.vx v12,v20,t4
vmsgtu.vx v13,v21,t5
vmsgtu.vx v14,v22,t6
vmsgtu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgtu.vx v8,v16,t0
vmsgtu.vx v9,v17,t1
vmsgtu.vx v10,v18,t2
vmsgtu.vx v11,v19,t3
vmsgtu.vx v12,v20,t4
vmsgtu.vx v13,v21,t5
vmsgtu.vx v14,v22,t6
vmsgtu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v9,v17,t1,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v11,v19,t3,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v13,v21,t5,v0.t
vmsgtu.vx v14,v22,t6,v0.t
vmsgtu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v9,v17,t1,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v11,v19,t3,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v13,v21,t5,v0.t
vmsgtu.vx v14,v22,t6,v0.t
vmsgtu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvi_m1:
	m_nop
	li a0, WARMUP
1:
vmsgtu.vi v8,v16,13
vmsgtu.vi v9,v17,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v11,v19,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v13,v21,13
vmsgtu.vi v14,v22,13
vmsgtu.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgtu.vi v8,v16,13
vmsgtu.vi v9,v17,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v11,v19,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v13,v21,13
vmsgtu.vi v14,v22,13
vmsgtu.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvim_m1:
	m_nop
	li a0, WARMUP
1:
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v9,v17,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v11,v19,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v13,v21,13,v0.t
vmsgtu.vi v14,v22,13,v0.t
vmsgtu.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v9,v17,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v11,v19,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v13,v21,13,v0.t
vmsgtu.vi v14,v22,13,v0.t
vmsgtu.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvx_m1:
	m_nop
	li a0, WARMUP
1:
vmsgt.vx v8,v16,t0
vmsgt.vx v9,v17,t1
vmsgt.vx v10,v18,t2
vmsgt.vx v11,v19,t3
vmsgt.vx v12,v20,t4
vmsgt.vx v13,v21,t5
vmsgt.vx v14,v22,t6
vmsgt.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgt.vx v8,v16,t0
vmsgt.vx v9,v17,t1
vmsgt.vx v10,v18,t2
vmsgt.vx v11,v19,t3
vmsgt.vx v12,v20,t4
vmsgt.vx v13,v21,t5
vmsgt.vx v14,v22,t6
vmsgt.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v9,v17,t1,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v11,v19,t3,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v13,v21,t5,v0.t
vmsgt.vx v14,v22,t6,v0.t
vmsgt.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v9,v17,t1,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v11,v19,t3,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v13,v21,t5,v0.t
vmsgt.vx v14,v22,t6,v0.t
vmsgt.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvi_m1:
	m_nop
	li a0, WARMUP
1:
vmsgt.vi v8,v16,13
vmsgt.vi v9,v17,13
vmsgt.vi v10,v18,13
vmsgt.vi v11,v19,13
vmsgt.vi v12,v20,13
vmsgt.vi v13,v21,13
vmsgt.vi v14,v22,13
vmsgt.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgt.vi v8,v16,13
vmsgt.vi v9,v17,13
vmsgt.vi v10,v18,13
vmsgt.vi v11,v19,13
vmsgt.vi v12,v20,13
vmsgt.vi v13,v21,13
vmsgt.vi v14,v22,13
vmsgt.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvim_m1:
	m_nop
	li a0, WARMUP
1:
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v9,v17,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v11,v19,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v13,v21,13,v0.t
vmsgt.vi v14,v22,13,v0.t
vmsgt.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v9,v17,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v11,v19,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v13,v21,13,v0.t
vmsgt.vi v14,v22,13,v0.t
vmsgt.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcompressvm_m1:
	m_nop
	li a0, WARMUP
1:
vcompress.vm v8,v16,v24
vcompress.vm v9,v17,v25
vcompress.vm v10,v18,v26
vcompress.vm v11,v19,v27
vcompress.vm v12,v20,v28
vcompress.vm v13,v21,v29
vcompress.vm v14,v22,v30
vcompress.vm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vcompress.vm v8,v16,v24
vcompress.vm v9,v17,v25
vcompress.vm v10,v18,v26
vcompress.vm v11,v19,v27
vcompress.vm v12,v20,v28
vcompress.vm v13,v21,v29
vcompress.vm v14,v22,v30
vcompress.vm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmandnmm_m1:
	m_nop
	li a0, WARMUP
1:
vmandn.mm v8,v16,v24
vmandn.mm v9,v17,v25
vmandn.mm v10,v18,v26
vmandn.mm v11,v19,v27
vmandn.mm v12,v20,v28
vmandn.mm v13,v21,v29
vmandn.mm v14,v22,v30
vmandn.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmandn.mm v8,v16,v24
vmandn.mm v9,v17,v25
vmandn.mm v10,v18,v26
vmandn.mm v11,v19,v27
vmandn.mm v12,v20,v28
vmandn.mm v13,v21,v29
vmandn.mm v14,v22,v30
vmandn.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmandmm_m1:
	m_nop
	li a0, WARMUP
1:
vmand.mm v8,v16,v24
vmand.mm v9,v17,v25
vmand.mm v10,v18,v26
vmand.mm v11,v19,v27
vmand.mm v12,v20,v28
vmand.mm v13,v21,v29
vmand.mm v14,v22,v30
vmand.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmand.mm v8,v16,v24
vmand.mm v9,v17,v25
vmand.mm v10,v18,v26
vmand.mm v11,v19,v27
vmand.mm v12,v20,v28
vmand.mm v13,v21,v29
vmand.mm v14,v22,v30
vmand.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmormm_m1:
	m_nop
	li a0, WARMUP
1:
vmor.mm v8,v16,v24
vmor.mm v9,v17,v25
vmor.mm v10,v18,v26
vmor.mm v11,v19,v27
vmor.mm v12,v20,v28
vmor.mm v13,v21,v29
vmor.mm v14,v22,v30
vmor.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmor.mm v8,v16,v24
vmor.mm v9,v17,v25
vmor.mm v10,v18,v26
vmor.mm v11,v19,v27
vmor.mm v12,v20,v28
vmor.mm v13,v21,v29
vmor.mm v14,v22,v30
vmor.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxormm_m1:
	m_nop
	li a0, WARMUP
1:
vmxor.mm v8,v16,v24
vmxor.mm v9,v17,v25
vmxor.mm v10,v18,v26
vmxor.mm v11,v19,v27
vmxor.mm v12,v20,v28
vmxor.mm v13,v21,v29
vmxor.mm v14,v22,v30
vmxor.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmxor.mm v8,v16,v24
vmxor.mm v9,v17,v25
vmxor.mm v10,v18,v26
vmxor.mm v11,v19,v27
vmxor.mm v12,v20,v28
vmxor.mm v13,v21,v29
vmxor.mm v14,v22,v30
vmxor.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmornmm_m1:
	m_nop
	li a0, WARMUP
1:
vmorn.mm v8,v16,v24
vmorn.mm v9,v17,v25
vmorn.mm v10,v18,v26
vmorn.mm v11,v19,v27
vmorn.mm v12,v20,v28
vmorn.mm v13,v21,v29
vmorn.mm v14,v22,v30
vmorn.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmorn.mm v8,v16,v24
vmorn.mm v9,v17,v25
vmorn.mm v10,v18,v26
vmorn.mm v11,v19,v27
vmorn.mm v12,v20,v28
vmorn.mm v13,v21,v29
vmorn.mm v14,v22,v30
vmorn.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnandmm_m1:
	m_nop
	li a0, WARMUP
1:
vmnand.mm v8,v16,v24
vmnand.mm v9,v17,v25
vmnand.mm v10,v18,v26
vmnand.mm v11,v19,v27
vmnand.mm v12,v20,v28
vmnand.mm v13,v21,v29
vmnand.mm v14,v22,v30
vmnand.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmnand.mm v8,v16,v24
vmnand.mm v9,v17,v25
vmnand.mm v10,v18,v26
vmnand.mm v11,v19,v27
vmnand.mm v12,v20,v28
vmnand.mm v13,v21,v29
vmnand.mm v14,v22,v30
vmnand.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnormm_m1:
	m_nop
	li a0, WARMUP
1:
vmnor.mm v8,v16,v24
vmnor.mm v9,v17,v25
vmnor.mm v10,v18,v26
vmnor.mm v11,v19,v27
vmnor.mm v12,v20,v28
vmnor.mm v13,v21,v29
vmnor.mm v14,v22,v30
vmnor.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmnor.mm v8,v16,v24
vmnor.mm v9,v17,v25
vmnor.mm v10,v18,v26
vmnor.mm v11,v19,v27
vmnor.mm v12,v20,v28
vmnor.mm v13,v21,v29
vmnor.mm v14,v22,v30
vmnor.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxnormm_m1:
	m_nop
	li a0, WARMUP
1:
vmxnor.mm v8,v16,v24
vmxnor.mm v9,v17,v25
vmxnor.mm v10,v18,v26
vmxnor.mm v11,v19,v27
vmxnor.mm v12,v20,v28
vmxnor.mm v13,v21,v29
vmxnor.mm v14,v22,v30
vmxnor.mm v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmxnor.mm v8,v16,v24
vmxnor.mm v9,v17,v25
vmxnor.mm v10,v18,v26
vmxnor.mm v11,v19,v27
vmxnor.mm v12,v20,v28
vmxnor.mm v13,v21,v29
vmxnor.mm v14,v22,v30
vmxnor.mm v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vsadduvv_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vv v8,v16,v24
vsaddu.vv v9,v17,v25
vsaddu.vv v10,v18,v26
vsaddu.vv v11,v19,v27
vsaddu.vv v12,v20,v28
vsaddu.vv v13,v21,v29
vsaddu.vv v14,v22,v30
vsaddu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vv v8,v16,v24
vsaddu.vv v9,v17,v25
vsaddu.vv v10,v18,v26
vsaddu.vv v11,v19,v27
vsaddu.vv v12,v20,v28
vsaddu.vv v13,v21,v29
vsaddu.vv v14,v22,v30
vsaddu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v9,v17,v25,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v11,v19,v27,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v13,v21,v29,v0.t
vsaddu.vv v14,v22,v30,v0.t
vsaddu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v9,v17,v25,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v11,v19,v27,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v13,v21,v29,v0.t
vsaddu.vv v14,v22,v30,v0.t
vsaddu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvx_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vx v8,v16,t0
vsaddu.vx v9,v17,t1
vsaddu.vx v10,v18,t2
vsaddu.vx v11,v19,t3
vsaddu.vx v12,v20,t4
vsaddu.vx v13,v21,t5
vsaddu.vx v14,v22,t6
vsaddu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vx v8,v16,t0
vsaddu.vx v9,v17,t1
vsaddu.vx v10,v18,t2
vsaddu.vx v11,v19,t3
vsaddu.vx v12,v20,t4
vsaddu.vx v13,v21,t5
vsaddu.vx v14,v22,t6
vsaddu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v9,v17,t1,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v11,v19,t3,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v13,v21,t5,v0.t
vsaddu.vx v14,v22,t6,v0.t
vsaddu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v9,v17,t1,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v11,v19,t3,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v13,v21,t5,v0.t
vsaddu.vx v14,v22,t6,v0.t
vsaddu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvi_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vi v8,v16,13
vsaddu.vi v9,v17,13
vsaddu.vi v10,v18,13
vsaddu.vi v11,v19,13
vsaddu.vi v12,v20,13
vsaddu.vi v13,v21,13
vsaddu.vi v14,v22,13
vsaddu.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vi v8,v16,13
vsaddu.vi v9,v17,13
vsaddu.vi v10,v18,13
vsaddu.vi v11,v19,13
vsaddu.vi v12,v20,13
vsaddu.vi v13,v21,13
vsaddu.vi v14,v22,13
vsaddu.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvim_m1:
	m_nop
	li a0, WARMUP
1:
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v9,v17,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v11,v19,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v13,v21,13,v0.t
vsaddu.vi v14,v22,13,v0.t
vsaddu.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v9,v17,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v11,v19,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v13,v21,13,v0.t
vsaddu.vi v14,v22,13,v0.t
vsaddu.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vv v8,v16,v24
vsadd.vv v9,v17,v25
vsadd.vv v10,v18,v26
vsadd.vv v11,v19,v27
vsadd.vv v12,v20,v28
vsadd.vv v13,v21,v29
vsadd.vv v14,v22,v30
vsadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vv v8,v16,v24
vsadd.vv v9,v17,v25
vsadd.vv v10,v18,v26
vsadd.vv v11,v19,v27
vsadd.vv v12,v20,v28
vsadd.vv v13,v21,v29
vsadd.vv v14,v22,v30
vsadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v9,v17,v25,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v11,v19,v27,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v13,v21,v29,v0.t
vsadd.vv v14,v22,v30,v0.t
vsadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v9,v17,v25,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v11,v19,v27,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v13,v21,v29,v0.t
vsadd.vv v14,v22,v30,v0.t
vsadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvx_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vx v8,v16,t0
vsadd.vx v9,v17,t1
vsadd.vx v10,v18,t2
vsadd.vx v11,v19,t3
vsadd.vx v12,v20,t4
vsadd.vx v13,v21,t5
vsadd.vx v14,v22,t6
vsadd.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vx v8,v16,t0
vsadd.vx v9,v17,t1
vsadd.vx v10,v18,t2
vsadd.vx v11,v19,t3
vsadd.vx v12,v20,t4
vsadd.vx v13,v21,t5
vsadd.vx v14,v22,t6
vsadd.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v9,v17,t1,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v11,v19,t3,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v13,v21,t5,v0.t
vsadd.vx v14,v22,t6,v0.t
vsadd.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v9,v17,t1,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v11,v19,t3,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v13,v21,t5,v0.t
vsadd.vx v14,v22,t6,v0.t
vsadd.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvi_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vi v8,v16,13
vsadd.vi v9,v17,13
vsadd.vi v10,v18,13
vsadd.vi v11,v19,13
vsadd.vi v12,v20,13
vsadd.vi v13,v21,13
vsadd.vi v14,v22,13
vsadd.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vi v8,v16,13
vsadd.vi v9,v17,13
vsadd.vi v10,v18,13
vsadd.vi v11,v19,13
vsadd.vi v12,v20,13
vsadd.vi v13,v21,13
vsadd.vi v14,v22,13
vsadd.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvim_m1:
	m_nop
	li a0, WARMUP
1:
vsadd.vi v8,v16,13,v0.t
vsadd.vi v9,v17,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v11,v19,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v13,v21,13,v0.t
vsadd.vi v14,v22,13,v0.t
vsadd.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsadd.vi v8,v16,13,v0.t
vsadd.vi v9,v17,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v11,v19,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v13,v21,13,v0.t
vsadd.vi v14,v22,13,v0.t
vsadd.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvv_m1:
	m_nop
	li a0, WARMUP
1:
vssubu.vv v8,v16,v24
vssubu.vv v9,v17,v25
vssubu.vv v10,v18,v26
vssubu.vv v11,v19,v27
vssubu.vv v12,v20,v28
vssubu.vv v13,v21,v29
vssubu.vv v14,v22,v30
vssubu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssubu.vv v8,v16,v24
vssubu.vv v9,v17,v25
vssubu.vv v10,v18,v26
vssubu.vv v11,v19,v27
vssubu.vv v12,v20,v28
vssubu.vv v13,v21,v29
vssubu.vv v14,v22,v30
vssubu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v9,v17,v25,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v11,v19,v27,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v13,v21,v29,v0.t
vssubu.vv v14,v22,v30,v0.t
vssubu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v9,v17,v25,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v11,v19,v27,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v13,v21,v29,v0.t
vssubu.vv v14,v22,v30,v0.t
vssubu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvx_m1:
	m_nop
	li a0, WARMUP
1:
vssubu.vx v8,v16,t0
vssubu.vx v9,v17,t1
vssubu.vx v10,v18,t2
vssubu.vx v11,v19,t3
vssubu.vx v12,v20,t4
vssubu.vx v13,v21,t5
vssubu.vx v14,v22,t6
vssubu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssubu.vx v8,v16,t0
vssubu.vx v9,v17,t1
vssubu.vx v10,v18,t2
vssubu.vx v11,v19,t3
vssubu.vx v12,v20,t4
vssubu.vx v13,v21,t5
vssubu.vx v14,v22,t6
vssubu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v9,v17,t1,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v11,v19,t3,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v13,v21,t5,v0.t
vssubu.vx v14,v22,t6,v0.t
vssubu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v9,v17,t1,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v11,v19,t3,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v13,v21,t5,v0.t
vssubu.vx v14,v22,t6,v0.t
vssubu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvv_m1:
	m_nop
	li a0, WARMUP
1:
vssub.vv v8,v16,v24
vssub.vv v9,v17,v25
vssub.vv v10,v18,v26
vssub.vv v11,v19,v27
vssub.vv v12,v20,v28
vssub.vv v13,v21,v29
vssub.vv v14,v22,v30
vssub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssub.vv v8,v16,v24
vssub.vv v9,v17,v25
vssub.vv v10,v18,v26
vssub.vv v11,v19,v27
vssub.vv v12,v20,v28
vssub.vv v13,v21,v29
vssub.vv v14,v22,v30
vssub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vssub.vv v8,v16,v24,v0.t
vssub.vv v9,v17,v25,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v11,v19,v27,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v13,v21,v29,v0.t
vssub.vv v14,v22,v30,v0.t
vssub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssub.vv v8,v16,v24,v0.t
vssub.vv v9,v17,v25,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v11,v19,v27,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v13,v21,v29,v0.t
vssub.vv v14,v22,v30,v0.t
vssub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvx_m1:
	m_nop
	li a0, WARMUP
1:
vssub.vx v8,v16,t0
vssub.vx v9,v17,t1
vssub.vx v10,v18,t2
vssub.vx v11,v19,t3
vssub.vx v12,v20,t4
vssub.vx v13,v21,t5
vssub.vx v14,v22,t6
vssub.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssub.vx v8,v16,t0
vssub.vx v9,v17,t1
vssub.vx v10,v18,t2
vssub.vx v11,v19,t3
vssub.vx v12,v20,t4
vssub.vx v13,v21,t5
vssub.vx v14,v22,t6
vssub.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vssub.vx v8,v16,t0,v0.t
vssub.vx v9,v17,t1,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v11,v19,t3,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v13,v21,t5,v0.t
vssub.vx v14,v22,t6,v0.t
vssub.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssub.vx v8,v16,t0,v0.t
vssub.vx v9,v17,t1,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v11,v19,t3,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v13,v21,t5,v0.t
vssub.vx v14,v22,t6,v0.t
vssub.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvv_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vv v8,v16,v24
vsll.vv v9,v17,v25
vsll.vv v10,v18,v26
vsll.vv v11,v19,v27
vsll.vv v12,v20,v28
vsll.vv v13,v21,v29
vsll.vv v14,v22,v30
vsll.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vv v8,v16,v24
vsll.vv v9,v17,v25
vsll.vv v10,v18,v26
vsll.vv v11,v19,v27
vsll.vv v12,v20,v28
vsll.vv v13,v21,v29
vsll.vv v14,v22,v30
vsll.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vv v8,v16,v24,v0.t
vsll.vv v9,v17,v25,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v11,v19,v27,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v13,v21,v29,v0.t
vsll.vv v14,v22,v30,v0.t
vsll.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vv v8,v16,v24,v0.t
vsll.vv v9,v17,v25,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v11,v19,v27,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v13,v21,v29,v0.t
vsll.vv v14,v22,v30,v0.t
vsll.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvx_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vx v8,v16,t0
vsll.vx v9,v17,t1
vsll.vx v10,v18,t2
vsll.vx v11,v19,t3
vsll.vx v12,v20,t4
vsll.vx v13,v21,t5
vsll.vx v14,v22,t6
vsll.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vx v8,v16,t0
vsll.vx v9,v17,t1
vsll.vx v10,v18,t2
vsll.vx v11,v19,t3
vsll.vx v12,v20,t4
vsll.vx v13,v21,t5
vsll.vx v14,v22,t6
vsll.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vx v8,v16,t0,v0.t
vsll.vx v9,v17,t1,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v11,v19,t3,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v13,v21,t5,v0.t
vsll.vx v14,v22,t6,v0.t
vsll.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vx v8,v16,t0,v0.t
vsll.vx v9,v17,t1,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v11,v19,t3,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v13,v21,t5,v0.t
vsll.vx v14,v22,t6,v0.t
vsll.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvi_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vi v8,v16,13
vsll.vi v9,v17,13
vsll.vi v10,v18,13
vsll.vi v11,v19,13
vsll.vi v12,v20,13
vsll.vi v13,v21,13
vsll.vi v14,v22,13
vsll.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vi v8,v16,13
vsll.vi v9,v17,13
vsll.vi v10,v18,13
vsll.vi v11,v19,13
vsll.vi v12,v20,13
vsll.vi v13,v21,13
vsll.vi v14,v22,13
vsll.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvim_m1:
	m_nop
	li a0, WARMUP
1:
vsll.vi v8,v16,13,v0.t
vsll.vi v9,v17,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v11,v19,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v13,v21,13,v0.t
vsll.vi v14,v22,13,v0.t
vsll.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsll.vi v8,v16,13,v0.t
vsll.vi v9,v17,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v11,v19,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v13,v21,13,v0.t
vsll.vi v14,v22,13,v0.t
vsll.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvv_m1:
	m_nop
	li a0, WARMUP
1:
vsmul.vv v8,v16,v24
vsmul.vv v9,v17,v25
vsmul.vv v10,v18,v26
vsmul.vv v11,v19,v27
vsmul.vv v12,v20,v28
vsmul.vv v13,v21,v29
vsmul.vv v14,v22,v30
vsmul.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsmul.vv v8,v16,v24
vsmul.vv v9,v17,v25
vsmul.vv v10,v18,v26
vsmul.vv v11,v19,v27
vsmul.vv v12,v20,v28
vsmul.vv v13,v21,v29
vsmul.vv v14,v22,v30
vsmul.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v9,v17,v25,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v11,v19,v27,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v13,v21,v29,v0.t
vsmul.vv v14,v22,v30,v0.t
vsmul.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v9,v17,v25,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v11,v19,v27,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v13,v21,v29,v0.t
vsmul.vv v14,v22,v30,v0.t
vsmul.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvx_m1:
	m_nop
	li a0, WARMUP
1:
vsmul.vx v8,v16,t0
vsmul.vx v9,v17,t1
vsmul.vx v10,v18,t2
vsmul.vx v11,v19,t3
vsmul.vx v12,v20,t4
vsmul.vx v13,v21,t5
vsmul.vx v14,v22,t6
vsmul.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsmul.vx v8,v16,t0
vsmul.vx v9,v17,t1
vsmul.vx v10,v18,t2
vsmul.vx v11,v19,t3
vsmul.vx v12,v20,t4
vsmul.vx v13,v21,t5
vsmul.vx v14,v22,t6
vsmul.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v9,v17,t1,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v11,v19,t3,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v13,v21,t5,v0.t
vsmul.vx v14,v22,t6,v0.t
vsmul.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v9,v17,t1,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v11,v19,t3,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v13,v21,t5,v0.t
vsmul.vx v14,v22,t6,v0.t
vsmul.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv1rv_m1:
	m_nop
	li a0, WARMUP
1:
vmv1r.v v8,v16
vmv1r.v v9,v17
vmv1r.v v10,v18
vmv1r.v v11,v19
vmv1r.v v12,v20
vmv1r.v v13,v21
vmv1r.v v14,v22
vmv1r.v v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv1r.v v8,v16
vmv1r.v v9,v17
vmv1r.v v10,v18
vmv1r.v v11,v19
vmv1r.v v12,v20
vmv1r.v v13,v21
vmv1r.v v14,v22
vmv1r.v v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv2rv_m1:
	m_nop
	li a0, WARMUP
1:
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv4rv_m1:
	m_nop
	li a0, WARMUP
1:
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv8rv_m1:
	m_nop
	li a0, WARMUP
1:
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvv_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vv v8,v16,v24
vsrl.vv v9,v17,v25
vsrl.vv v10,v18,v26
vsrl.vv v11,v19,v27
vsrl.vv v12,v20,v28
vsrl.vv v13,v21,v29
vsrl.vv v14,v22,v30
vsrl.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vv v8,v16,v24
vsrl.vv v9,v17,v25
vsrl.vv v10,v18,v26
vsrl.vv v11,v19,v27
vsrl.vv v12,v20,v28
vsrl.vv v13,v21,v29
vsrl.vv v14,v22,v30
vsrl.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvvm_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v9,v17,v25,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v11,v19,v27,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v13,v21,v29,v0.t
vsrl.vv v14,v22,v30,v0.t
vsrl.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v9,v17,v25,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v11,v19,v27,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v13,v21,v29,v0.t
vsrl.vv v14,v22,v30,v0.t
vsrl.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvx_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vx v8,v16,t0
vsrl.vx v9,v17,t1
vsrl.vx v10,v18,t2
vsrl.vx v11,v19,t3
vsrl.vx v12,v20,t4
vsrl.vx v13,v21,t5
vsrl.vx v14,v22,t6
vsrl.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vx v8,v16,t0
vsrl.vx v9,v17,t1
vsrl.vx v10,v18,t2
vsrl.vx v11,v19,t3
vsrl.vx v12,v20,t4
vsrl.vx v13,v21,t5
vsrl.vx v14,v22,t6
vsrl.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvxm_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v9,v17,t1,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v11,v19,t3,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v13,v21,t5,v0.t
vsrl.vx v14,v22,t6,v0.t
vsrl.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v9,v17,t1,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v11,v19,t3,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v13,v21,t5,v0.t
vsrl.vx v14,v22,t6,v0.t
vsrl.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvi_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vi v8,v16,13
vsrl.vi v9,v17,13
vsrl.vi v10,v18,13
vsrl.vi v11,v19,13
vsrl.vi v12,v20,13
vsrl.vi v13,v21,13
vsrl.vi v14,v22,13
vsrl.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vi v8,v16,13
vsrl.vi v9,v17,13
vsrl.vi v10,v18,13
vsrl.vi v11,v19,13
vsrl.vi v12,v20,13
vsrl.vi v13,v21,13
vsrl.vi v14,v22,13
vsrl.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvim_m1:
	m_nop
	li a0, WARMUP
1:
vsrl.vi v8,v16,13,v0.t
vsrl.vi v9,v17,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v11,v19,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v13,v21,13,v0.t
vsrl.vi v14,v22,13,v0.t
vsrl.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsrl.vi v8,v16,13,v0.t
vsrl.vi v9,v17,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v11,v19,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v13,v21,13,v0.t
vsrl.vi v14,v22,13,v0.t
vsrl.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravv_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vv v8,v16,v24
vsra.vv v9,v17,v25
vsra.vv v10,v18,v26
vsra.vv v11,v19,v27
vsra.vv v12,v20,v28
vsra.vv v13,v21,v29
vsra.vv v14,v22,v30
vsra.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vv v8,v16,v24
vsra.vv v9,v17,v25
vsra.vv v10,v18,v26
vsra.vv v11,v19,v27
vsra.vv v12,v20,v28
vsra.vv v13,v21,v29
vsra.vv v14,v22,v30
vsra.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravvm_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vv v8,v16,v24,v0.t
vsra.vv v9,v17,v25,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v11,v19,v27,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v13,v21,v29,v0.t
vsra.vv v14,v22,v30,v0.t
vsra.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vv v8,v16,v24,v0.t
vsra.vv v9,v17,v25,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v11,v19,v27,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v13,v21,v29,v0.t
vsra.vv v14,v22,v30,v0.t
vsra.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravx_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vx v8,v16,t0
vsra.vx v9,v17,t1
vsra.vx v10,v18,t2
vsra.vx v11,v19,t3
vsra.vx v12,v20,t4
vsra.vx v13,v21,t5
vsra.vx v14,v22,t6
vsra.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vx v8,v16,t0
vsra.vx v9,v17,t1
vsra.vx v10,v18,t2
vsra.vx v11,v19,t3
vsra.vx v12,v20,t4
vsra.vx v13,v21,t5
vsra.vx v14,v22,t6
vsra.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravxm_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vx v8,v16,t0,v0.t
vsra.vx v9,v17,t1,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v11,v19,t3,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v13,v21,t5,v0.t
vsra.vx v14,v22,t6,v0.t
vsra.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vx v8,v16,t0,v0.t
vsra.vx v9,v17,t1,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v11,v19,t3,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v13,v21,t5,v0.t
vsra.vx v14,v22,t6,v0.t
vsra.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravi_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vi v8,v16,13
vsra.vi v9,v17,13
vsra.vi v10,v18,13
vsra.vi v11,v19,13
vsra.vi v12,v20,13
vsra.vi v13,v21,13
vsra.vi v14,v22,13
vsra.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vi v8,v16,13
vsra.vi v9,v17,13
vsra.vi v10,v18,13
vsra.vi v11,v19,13
vsra.vi v12,v20,13
vsra.vi v13,v21,13
vsra.vi v14,v22,13
vsra.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravim_m1:
	m_nop
	li a0, WARMUP
1:
vsra.vi v8,v16,13,v0.t
vsra.vi v9,v17,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v11,v19,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v13,v21,13,v0.t
vsra.vi v14,v22,13,v0.t
vsra.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsra.vi v8,v16,13,v0.t
vsra.vi v9,v17,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v11,v19,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v13,v21,13,v0.t
vsra.vi v14,v22,13,v0.t
vsra.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvv_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vv v8,v16,v24
vssrl.vv v9,v17,v25
vssrl.vv v10,v18,v26
vssrl.vv v11,v19,v27
vssrl.vv v12,v20,v28
vssrl.vv v13,v21,v29
vssrl.vv v14,v22,v30
vssrl.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vv v8,v16,v24
vssrl.vv v9,v17,v25
vssrl.vv v10,v18,v26
vssrl.vv v11,v19,v27
vssrl.vv v12,v20,v28
vssrl.vv v13,v21,v29
vssrl.vv v14,v22,v30
vssrl.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvvm_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v9,v17,v25,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v11,v19,v27,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v13,v21,v29,v0.t
vssrl.vv v14,v22,v30,v0.t
vssrl.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v9,v17,v25,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v11,v19,v27,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v13,v21,v29,v0.t
vssrl.vv v14,v22,v30,v0.t
vssrl.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvx_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vx v8,v16,t0
vssrl.vx v9,v17,t1
vssrl.vx v10,v18,t2
vssrl.vx v11,v19,t3
vssrl.vx v12,v20,t4
vssrl.vx v13,v21,t5
vssrl.vx v14,v22,t6
vssrl.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vx v8,v16,t0
vssrl.vx v9,v17,t1
vssrl.vx v10,v18,t2
vssrl.vx v11,v19,t3
vssrl.vx v12,v20,t4
vssrl.vx v13,v21,t5
vssrl.vx v14,v22,t6
vssrl.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvxm_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v9,v17,t1,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v11,v19,t3,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v13,v21,t5,v0.t
vssrl.vx v14,v22,t6,v0.t
vssrl.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v9,v17,t1,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v11,v19,t3,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v13,v21,t5,v0.t
vssrl.vx v14,v22,t6,v0.t
vssrl.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvi_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vi v8,v16,13
vssrl.vi v9,v17,13
vssrl.vi v10,v18,13
vssrl.vi v11,v19,13
vssrl.vi v12,v20,13
vssrl.vi v13,v21,13
vssrl.vi v14,v22,13
vssrl.vi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vi v8,v16,13
vssrl.vi v9,v17,13
vssrl.vi v10,v18,13
vssrl.vi v11,v19,13
vssrl.vi v12,v20,13
vssrl.vi v13,v21,13
vssrl.vi v14,v22,13
vssrl.vi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvim_m1:
	m_nop
	li a0, WARMUP
1:
vssrl.vi v8,v16,13,v0.t
vssrl.vi v9,v17,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v11,v19,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v13,v21,13,v0.t
vssrl.vi v14,v22,13,v0.t
vssrl.vi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vssrl.vi v8,v16,13,v0.t
vssrl.vi v9,v17,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v11,v19,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v13,v21,13,v0.t
vssrl.vi v14,v22,13,v0.t
vssrl.vi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vdivuvv_m1:
	m_nop
	li a0, WARMUP
1:
vdivu.vv v8,v16,v24
vdivu.vv v9,v17,v25
vdivu.vv v10,v18,v26
vdivu.vv v11,v19,v27
vdivu.vv v12,v20,v28
vdivu.vv v13,v21,v29
vdivu.vv v14,v22,v30
vdivu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdivu.vv v8,v16,v24
vdivu.vv v9,v17,v25
vdivu.vv v10,v18,v26
vdivu.vv v11,v19,v27
vdivu.vv v12,v20,v28
vdivu.vv v13,v21,v29
vdivu.vv v14,v22,v30
vdivu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v9,v17,v25,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v11,v19,v27,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v13,v21,v29,v0.t
vdivu.vv v14,v22,v30,v0.t
vdivu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v9,v17,v25,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v11,v19,v27,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v13,v21,v29,v0.t
vdivu.vv v14,v22,v30,v0.t
vdivu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvx_m1:
	m_nop
	li a0, WARMUP
1:
vdivu.vx v8,v16,t0
vdivu.vx v9,v17,t1
vdivu.vx v10,v18,t2
vdivu.vx v11,v19,t3
vdivu.vx v12,v20,t4
vdivu.vx v13,v21,t5
vdivu.vx v14,v22,t6
vdivu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdivu.vx v8,v16,t0
vdivu.vx v9,v17,t1
vdivu.vx v10,v18,t2
vdivu.vx v11,v19,t3
vdivu.vx v12,v20,t4
vdivu.vx v13,v21,t5
vdivu.vx v14,v22,t6
vdivu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v9,v17,t1,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v11,v19,t3,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v13,v21,t5,v0.t
vdivu.vx v14,v22,t6,v0.t
vdivu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v9,v17,t1,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v11,v19,t3,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v13,v21,t5,v0.t
vdivu.vx v14,v22,t6,v0.t
vdivu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvv_m1:
	m_nop
	li a0, WARMUP
1:
vdiv.vv v8,v16,v24
vdiv.vv v9,v17,v25
vdiv.vv v10,v18,v26
vdiv.vv v11,v19,v27
vdiv.vv v12,v20,v28
vdiv.vv v13,v21,v29
vdiv.vv v14,v22,v30
vdiv.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdiv.vv v8,v16,v24
vdiv.vv v9,v17,v25
vdiv.vv v10,v18,v26
vdiv.vv v11,v19,v27
vdiv.vv v12,v20,v28
vdiv.vv v13,v21,v29
vdiv.vv v14,v22,v30
vdiv.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvvm_m1:
	m_nop
	li a0, WARMUP
1:
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v9,v17,v25,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v11,v19,v27,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v13,v21,v29,v0.t
vdiv.vv v14,v22,v30,v0.t
vdiv.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v9,v17,v25,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v11,v19,v27,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v13,v21,v29,v0.t
vdiv.vv v14,v22,v30,v0.t
vdiv.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvx_m1:
	m_nop
	li a0, WARMUP
1:
vdiv.vx v8,v16,t0
vdiv.vx v9,v17,t1
vdiv.vx v10,v18,t2
vdiv.vx v11,v19,t3
vdiv.vx v12,v20,t4
vdiv.vx v13,v21,t5
vdiv.vx v14,v22,t6
vdiv.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdiv.vx v8,v16,t0
vdiv.vx v9,v17,t1
vdiv.vx v10,v18,t2
vdiv.vx v11,v19,t3
vdiv.vx v12,v20,t4
vdiv.vx v13,v21,t5
vdiv.vx v14,v22,t6
vdiv.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvxm_m1:
	m_nop
	li a0, WARMUP
1:
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v9,v17,t1,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v11,v19,t3,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v13,v21,t5,v0.t
vdiv.vx v14,v22,t6,v0.t
vdiv.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v9,v17,t1,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v11,v19,t3,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v13,v21,t5,v0.t
vdiv.vx v14,v22,t6,v0.t
vdiv.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvv_m1:
	m_nop
	li a0, WARMUP
1:
vremu.vv v8,v16,v24
vremu.vv v9,v17,v25
vremu.vv v10,v18,v26
vremu.vv v11,v19,v27
vremu.vv v12,v20,v28
vremu.vv v13,v21,v29
vremu.vv v14,v22,v30
vremu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vremu.vv v8,v16,v24
vremu.vv v9,v17,v25
vremu.vv v10,v18,v26
vremu.vv v11,v19,v27
vremu.vv v12,v20,v28
vremu.vv v13,v21,v29
vremu.vv v14,v22,v30
vremu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vremu.vv v8,v16,v24,v0.t
vremu.vv v9,v17,v25,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v11,v19,v27,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v13,v21,v29,v0.t
vremu.vv v14,v22,v30,v0.t
vremu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vremu.vv v8,v16,v24,v0.t
vremu.vv v9,v17,v25,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v11,v19,v27,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v13,v21,v29,v0.t
vremu.vv v14,v22,v30,v0.t
vremu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvx_m1:
	m_nop
	li a0, WARMUP
1:
vremu.vx v8,v16,t0
vremu.vx v9,v17,t1
vremu.vx v10,v18,t2
vremu.vx v11,v19,t3
vremu.vx v12,v20,t4
vremu.vx v13,v21,t5
vremu.vx v14,v22,t6
vremu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vremu.vx v8,v16,t0
vremu.vx v9,v17,t1
vremu.vx v10,v18,t2
vremu.vx v11,v19,t3
vremu.vx v12,v20,t4
vremu.vx v13,v21,t5
vremu.vx v14,v22,t6
vremu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vremu.vx v8,v16,t0,v0.t
vremu.vx v9,v17,t1,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v11,v19,t3,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v13,v21,t5,v0.t
vremu.vx v14,v22,t6,v0.t
vremu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vremu.vx v8,v16,t0,v0.t
vremu.vx v9,v17,t1,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v11,v19,t3,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v13,v21,t5,v0.t
vremu.vx v14,v22,t6,v0.t
vremu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvv_m1:
	m_nop
	li a0, WARMUP
1:
vrem.vv v8,v16,v24
vrem.vv v9,v17,v25
vrem.vv v10,v18,v26
vrem.vv v11,v19,v27
vrem.vv v12,v20,v28
vrem.vv v13,v21,v29
vrem.vv v14,v22,v30
vrem.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrem.vv v8,v16,v24
vrem.vv v9,v17,v25
vrem.vv v10,v18,v26
vrem.vv v11,v19,v27
vrem.vv v12,v20,v28
vrem.vv v13,v21,v29
vrem.vv v14,v22,v30
vrem.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvvm_m1:
	m_nop
	li a0, WARMUP
1:
vrem.vv v8,v16,v24,v0.t
vrem.vv v9,v17,v25,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v11,v19,v27,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v13,v21,v29,v0.t
vrem.vv v14,v22,v30,v0.t
vrem.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrem.vv v8,v16,v24,v0.t
vrem.vv v9,v17,v25,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v11,v19,v27,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v13,v21,v29,v0.t
vrem.vv v14,v22,v30,v0.t
vrem.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvx_m1:
	m_nop
	li a0, WARMUP
1:
vrem.vx v8,v16,t0
vrem.vx v9,v17,t1
vrem.vx v10,v18,t2
vrem.vx v11,v19,t3
vrem.vx v12,v20,t4
vrem.vx v13,v21,t5
vrem.vx v14,v22,t6
vrem.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrem.vx v8,v16,t0
vrem.vx v9,v17,t1
vrem.vx v10,v18,t2
vrem.vx v11,v19,t3
vrem.vx v12,v20,t4
vrem.vx v13,v21,t5
vrem.vx v14,v22,t6
vrem.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvxm_m1:
	m_nop
	li a0, WARMUP
1:
vrem.vx v8,v16,t0,v0.t
vrem.vx v9,v17,t1,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v11,v19,t3,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v13,v21,t5,v0.t
vrem.vx v14,v22,t6,v0.t
vrem.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vrem.vx v8,v16,t0,v0.t
vrem.vx v9,v17,t1,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v11,v19,t3,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v13,v21,t5,v0.t
vrem.vx v14,v22,t6,v0.t
vrem.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvv_m1:
	m_nop
	li a0, WARMUP
1:
vmulhu.vv v8,v16,v24
vmulhu.vv v9,v17,v25
vmulhu.vv v10,v18,v26
vmulhu.vv v11,v19,v27
vmulhu.vv v12,v20,v28
vmulhu.vv v13,v21,v29
vmulhu.vv v14,v22,v30
vmulhu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhu.vv v8,v16,v24
vmulhu.vv v9,v17,v25
vmulhu.vv v10,v18,v26
vmulhu.vv v11,v19,v27
vmulhu.vv v12,v20,v28
vmulhu.vv v13,v21,v29
vmulhu.vv v14,v22,v30
vmulhu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v9,v17,v25,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v11,v19,v27,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v13,v21,v29,v0.t
vmulhu.vv v14,v22,v30,v0.t
vmulhu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v9,v17,v25,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v11,v19,v27,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v13,v21,v29,v0.t
vmulhu.vv v14,v22,v30,v0.t
vmulhu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmulhu.vx v8,v16,t0
vmulhu.vx v9,v17,t1
vmulhu.vx v10,v18,t2
vmulhu.vx v11,v19,t3
vmulhu.vx v12,v20,t4
vmulhu.vx v13,v21,t5
vmulhu.vx v14,v22,t6
vmulhu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhu.vx v8,v16,t0
vmulhu.vx v9,v17,t1
vmulhu.vx v10,v18,t2
vmulhu.vx v11,v19,t3
vmulhu.vx v12,v20,t4
vmulhu.vx v13,v21,t5
vmulhu.vx v14,v22,t6
vmulhu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v9,v17,t1,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v11,v19,t3,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v13,v21,t5,v0.t
vmulhu.vx v14,v22,t6,v0.t
vmulhu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v9,v17,t1,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v11,v19,t3,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v13,v21,t5,v0.t
vmulhu.vx v14,v22,t6,v0.t
vmulhu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvv_m1:
	m_nop
	li a0, WARMUP
1:
vmul.vv v8,v16,v24
vmul.vv v9,v17,v25
vmul.vv v10,v18,v26
vmul.vv v11,v19,v27
vmul.vv v12,v20,v28
vmul.vv v13,v21,v29
vmul.vv v14,v22,v30
vmul.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmul.vv v8,v16,v24
vmul.vv v9,v17,v25
vmul.vv v10,v18,v26
vmul.vv v11,v19,v27
vmul.vv v12,v20,v28
vmul.vv v13,v21,v29
vmul.vv v14,v22,v30
vmul.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmul.vv v8,v16,v24,v0.t
vmul.vv v9,v17,v25,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v11,v19,v27,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v13,v21,v29,v0.t
vmul.vv v14,v22,v30,v0.t
vmul.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmul.vv v8,v16,v24,v0.t
vmul.vv v9,v17,v25,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v11,v19,v27,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v13,v21,v29,v0.t
vmul.vv v14,v22,v30,v0.t
vmul.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvx_m1:
	m_nop
	li a0, WARMUP
1:
vmul.vx v8,v16,t0
vmul.vx v9,v17,t1
vmul.vx v10,v18,t2
vmul.vx v11,v19,t3
vmul.vx v12,v20,t4
vmul.vx v13,v21,t5
vmul.vx v14,v22,t6
vmul.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmul.vx v8,v16,t0
vmul.vx v9,v17,t1
vmul.vx v10,v18,t2
vmul.vx v11,v19,t3
vmul.vx v12,v20,t4
vmul.vx v13,v21,t5
vmul.vx v14,v22,t6
vmul.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmul.vx v8,v16,t0,v0.t
vmul.vx v9,v17,t1,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v11,v19,t3,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v13,v21,t5,v0.t
vmul.vx v14,v22,t6,v0.t
vmul.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmul.vx v8,v16,t0,v0.t
vmul.vx v9,v17,t1,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v11,v19,t3,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v13,v21,t5,v0.t
vmul.vx v14,v22,t6,v0.t
vmul.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvv_m1:
	m_nop
	li a0, WARMUP
1:
vmulhsu.vv v8,v16,v24
vmulhsu.vv v9,v17,v25
vmulhsu.vv v10,v18,v26
vmulhsu.vv v11,v19,v27
vmulhsu.vv v12,v20,v28
vmulhsu.vv v13,v21,v29
vmulhsu.vv v14,v22,v30
vmulhsu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhsu.vv v8,v16,v24
vmulhsu.vv v9,v17,v25
vmulhsu.vv v10,v18,v26
vmulhsu.vv v11,v19,v27
vmulhsu.vv v12,v20,v28
vmulhsu.vv v13,v21,v29
vmulhsu.vv v14,v22,v30
vmulhsu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v9,v17,v25,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v11,v19,v27,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v13,v21,v29,v0.t
vmulhsu.vv v14,v22,v30,v0.t
vmulhsu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v9,v17,v25,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v11,v19,v27,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v13,v21,v29,v0.t
vmulhsu.vv v14,v22,v30,v0.t
vmulhsu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvx_m1:
	m_nop
	li a0, WARMUP
1:
vmulhsu.vx v8,v16,t0
vmulhsu.vx v9,v17,t1
vmulhsu.vx v10,v18,t2
vmulhsu.vx v11,v19,t3
vmulhsu.vx v12,v20,t4
vmulhsu.vx v13,v21,t5
vmulhsu.vx v14,v22,t6
vmulhsu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhsu.vx v8,v16,t0
vmulhsu.vx v9,v17,t1
vmulhsu.vx v10,v18,t2
vmulhsu.vx v11,v19,t3
vmulhsu.vx v12,v20,t4
vmulhsu.vx v13,v21,t5
vmulhsu.vx v14,v22,t6
vmulhsu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v9,v17,t1,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v11,v19,t3,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v13,v21,t5,v0.t
vmulhsu.vx v14,v22,t6,v0.t
vmulhsu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v9,v17,t1,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v11,v19,t3,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v13,v21,t5,v0.t
vmulhsu.vx v14,v22,t6,v0.t
vmulhsu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvv_m1:
	m_nop
	li a0, WARMUP
1:
vmulh.vv v8,v16,v24
vmulh.vv v9,v17,v25
vmulh.vv v10,v18,v26
vmulh.vv v11,v19,v27
vmulh.vv v12,v20,v28
vmulh.vv v13,v21,v29
vmulh.vv v14,v22,v30
vmulh.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulh.vv v8,v16,v24
vmulh.vv v9,v17,v25
vmulh.vv v10,v18,v26
vmulh.vv v11,v19,v27
vmulh.vv v12,v20,v28
vmulh.vv v13,v21,v29
vmulh.vv v14,v22,v30
vmulh.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v9,v17,v25,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v11,v19,v27,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v13,v21,v29,v0.t
vmulh.vv v14,v22,v30,v0.t
vmulh.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v9,v17,v25,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v11,v19,v27,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v13,v21,v29,v0.t
vmulh.vv v14,v22,v30,v0.t
vmulh.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvx_m1:
	m_nop
	li a0, WARMUP
1:
vmulh.vx v8,v16,t0
vmulh.vx v9,v17,t1
vmulh.vx v10,v18,t2
vmulh.vx v11,v19,t3
vmulh.vx v12,v20,t4
vmulh.vx v13,v21,t5
vmulh.vx v14,v22,t6
vmulh.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulh.vx v8,v16,t0
vmulh.vx v9,v17,t1
vmulh.vx v10,v18,t2
vmulh.vx v11,v19,t3
vmulh.vx v12,v20,t4
vmulh.vx v13,v21,t5
vmulh.vx v14,v22,t6
vmulh.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v9,v17,t1,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v11,v19,t3,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v13,v21,t5,v0.t
vmulh.vx v14,v22,t6,v0.t
vmulh.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v9,v17,t1,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v11,v19,t3,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v13,v21,t5,v0.t
vmulh.vx v14,v22,t6,v0.t
vmulh.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vmadd.vv v8,v16,v24
vmadd.vv v9,v17,v25
vmadd.vv v10,v18,v26
vmadd.vv v11,v19,v27
vmadd.vv v12,v20,v28
vmadd.vv v13,v21,v29
vmadd.vv v14,v22,v30
vmadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadd.vv v8,v16,v24
vmadd.vv v9,v17,v25
vmadd.vv v10,v18,v26
vmadd.vv v11,v19,v27
vmadd.vv v12,v20,v28
vmadd.vv v13,v21,v29
vmadd.vv v14,v22,v30
vmadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v9,v17,v25,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v11,v19,v27,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v13,v21,v29,v0.t
vmadd.vv v14,v22,v30,v0.t
vmadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v9,v17,v25,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v11,v19,v27,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v13,v21,v29,v0.t
vmadd.vv v14,v22,v30,v0.t
vmadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvx_m1:
	m_nop
	li a0, WARMUP
1:
vmadd.vx v8,t0,v16
vmadd.vx v9,t1,v17
vmadd.vx v10,t2,v18
vmadd.vx v11,t3,v19
vmadd.vx v12,t4,v20
vmadd.vx v13,t5,v21
vmadd.vx v14,t6,v22
vmadd.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadd.vx v8,t0,v16
vmadd.vx v9,t1,v17
vmadd.vx v10,t2,v18
vmadd.vx v11,t3,v19
vmadd.vx v12,t4,v20
vmadd.vx v13,t5,v21
vmadd.vx v14,t6,v22
vmadd.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v9,t1,v17,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v11,t3,v19,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v13,t5,v21,v0.t
vmadd.vx v14,t6,v22,v0.t
vmadd.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v9,t1,v17,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v11,t3,v19,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v13,t5,v21,v0.t
vmadd.vx v14,t6,v22,v0.t
vmadd.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vmacc.vv v8,v16,v24
vmacc.vv v9,v17,v25
vmacc.vv v10,v18,v26
vmacc.vv v11,v19,v27
vmacc.vv v12,v20,v28
vmacc.vv v13,v21,v29
vmacc.vv v14,v22,v30
vmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmacc.vv v8,v16,v24
vmacc.vv v9,v17,v25
vmacc.vv v10,v18,v26
vmacc.vv v11,v19,v27
vmacc.vv v12,v20,v28
vmacc.vv v13,v21,v29
vmacc.vv v14,v22,v30
vmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v9,v17,v25,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v11,v19,v27,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v13,v21,v29,v0.t
vmacc.vv v14,v22,v30,v0.t
vmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v9,v17,v25,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v11,v19,v27,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v13,v21,v29,v0.t
vmacc.vv v14,v22,v30,v0.t
vmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvx_m1:
	m_nop
	li a0, WARMUP
1:
vmacc.vx v8,t0,v16
vmacc.vx v9,t1,v17
vmacc.vx v10,t2,v18
vmacc.vx v11,t3,v19
vmacc.vx v12,t4,v20
vmacc.vx v13,t5,v21
vmacc.vx v14,t6,v22
vmacc.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmacc.vx v8,t0,v16
vmacc.vx v9,t1,v17
vmacc.vx v10,t2,v18
vmacc.vx v11,t3,v19
vmacc.vx v12,t4,v20
vmacc.vx v13,t5,v21
vmacc.vx v14,t6,v22
vmacc.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvxm_m1:
	m_nop
	li a0, WARMUP
1:
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v9,t1,v17,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v11,t3,v19,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v13,t5,v21,v0.t
vmacc.vx v14,t6,v22,v0.t
vmacc.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v9,t1,v17,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v11,t3,v19,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v13,t5,v21,v0.t
vmacc.vx v14,t6,v22,v0.t
vmacc.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vnsrlwv_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wv v8,v16,v24
vnsrl.wv v9,v17,v25
vnsrl.wv v10,v18,v26
vnsrl.wv v11,v19,v27
vnsrl.wv v12,v20,v28
vnsrl.wv v13,v21,v29
vnsrl.wv v14,v22,v30
vnsrl.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wv v8,v16,v24
vnsrl.wv v9,v17,v25
vnsrl.wv v10,v18,v26
vnsrl.wv v11,v19,v27
vnsrl.wv v12,v20,v28
vnsrl.wv v13,v21,v29
vnsrl.wv v14,v22,v30
vnsrl.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwvm_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v9,v17,v25,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v11,v19,v27,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v13,v21,v29,v0.t
vnsrl.wv v14,v22,v30,v0.t
vnsrl.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v9,v17,v25,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v11,v19,v27,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v13,v21,v29,v0.t
vnsrl.wv v14,v22,v30,v0.t
vnsrl.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwx_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wx v8,v16,t0
vnsrl.wx v9,v17,t1
vnsrl.wx v10,v18,t2
vnsrl.wx v11,v19,t3
vnsrl.wx v12,v20,t4
vnsrl.wx v13,v21,t5
vnsrl.wx v14,v22,t6
vnsrl.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wx v8,v16,t0
vnsrl.wx v9,v17,t1
vnsrl.wx v10,v18,t2
vnsrl.wx v11,v19,t3
vnsrl.wx v12,v20,t4
vnsrl.wx v13,v21,t5
vnsrl.wx v14,v22,t6
vnsrl.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwxm_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v9,v17,t1,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v11,v19,t3,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v13,v21,t5,v0.t
vnsrl.wx v14,v22,t6,v0.t
vnsrl.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v9,v17,t1,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v11,v19,t3,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v13,v21,t5,v0.t
vnsrl.wx v14,v22,t6,v0.t
vnsrl.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwi_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wi v8,v16,13
vnsrl.wi v9,v17,13
vnsrl.wi v10,v18,13
vnsrl.wi v11,v19,13
vnsrl.wi v12,v20,13
vnsrl.wi v13,v21,13
vnsrl.wi v14,v22,13
vnsrl.wi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wi v8,v16,13
vnsrl.wi v9,v17,13
vnsrl.wi v10,v18,13
vnsrl.wi v11,v19,13
vnsrl.wi v12,v20,13
vnsrl.wi v13,v21,13
vnsrl.wi v14,v22,13
vnsrl.wi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwim_m1:
	m_nop
	li a0, WARMUP
1:
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v9,v17,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v11,v19,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v13,v21,13,v0.t
vnsrl.wi v14,v22,13,v0.t
vnsrl.wi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v9,v17,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v11,v19,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v13,v21,13,v0.t
vnsrl.wi v14,v22,13,v0.t
vnsrl.wi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawv_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wv v8,v16,v24
vnsra.wv v9,v17,v25
vnsra.wv v10,v18,v26
vnsra.wv v11,v19,v27
vnsra.wv v12,v20,v28
vnsra.wv v13,v21,v29
vnsra.wv v14,v22,v30
vnsra.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wv v8,v16,v24
vnsra.wv v9,v17,v25
vnsra.wv v10,v18,v26
vnsra.wv v11,v19,v27
vnsra.wv v12,v20,v28
vnsra.wv v13,v21,v29
vnsra.wv v14,v22,v30
vnsra.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawvm_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v9,v17,v25,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v11,v19,v27,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v13,v21,v29,v0.t
vnsra.wv v14,v22,v30,v0.t
vnsra.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v9,v17,v25,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v11,v19,v27,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v13,v21,v29,v0.t
vnsra.wv v14,v22,v30,v0.t
vnsra.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawx_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wx v8,v16,t0
vnsra.wx v9,v17,t1
vnsra.wx v10,v18,t2
vnsra.wx v11,v19,t3
vnsra.wx v12,v20,t4
vnsra.wx v13,v21,t5
vnsra.wx v14,v22,t6
vnsra.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wx v8,v16,t0
vnsra.wx v9,v17,t1
vnsra.wx v10,v18,t2
vnsra.wx v11,v19,t3
vnsra.wx v12,v20,t4
vnsra.wx v13,v21,t5
vnsra.wx v14,v22,t6
vnsra.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawxm_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v9,v17,t1,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v11,v19,t3,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v13,v21,t5,v0.t
vnsra.wx v14,v22,t6,v0.t
vnsra.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v9,v17,t1,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v11,v19,t3,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v13,v21,t5,v0.t
vnsra.wx v14,v22,t6,v0.t
vnsra.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawi_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wi v8,v16,13
vnsra.wi v9,v17,13
vnsra.wi v10,v18,13
vnsra.wi v11,v19,13
vnsra.wi v12,v20,13
vnsra.wi v13,v21,13
vnsra.wi v14,v22,13
vnsra.wi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wi v8,v16,13
vnsra.wi v9,v17,13
vnsra.wi v10,v18,13
vnsra.wi v11,v19,13
vnsra.wi v12,v20,13
vnsra.wi v13,v21,13
vnsra.wi v14,v22,13
vnsra.wi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawim_m1:
	m_nop
	li a0, WARMUP
1:
vnsra.wi v8,v16,13,v0.t
vnsra.wi v9,v17,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v11,v19,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v13,v21,13,v0.t
vnsra.wi v14,v22,13,v0.t
vnsra.wi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnsra.wi v8,v16,13,v0.t
vnsra.wi v9,v17,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v11,v19,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v13,v21,13,v0.t
vnsra.wi v14,v22,13,v0.t
vnsra.wi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwv_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wv v8,v16,v24
vnclipu.wv v9,v17,v25
vnclipu.wv v10,v18,v26
vnclipu.wv v11,v19,v27
vnclipu.wv v12,v20,v28
vnclipu.wv v13,v21,v29
vnclipu.wv v14,v22,v30
vnclipu.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wv v8,v16,v24
vnclipu.wv v9,v17,v25
vnclipu.wv v10,v18,v26
vnclipu.wv v11,v19,v27
vnclipu.wv v12,v20,v28
vnclipu.wv v13,v21,v29
vnclipu.wv v14,v22,v30
vnclipu.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwvm_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v9,v17,v25,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v11,v19,v27,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v13,v21,v29,v0.t
vnclipu.wv v14,v22,v30,v0.t
vnclipu.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v9,v17,v25,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v11,v19,v27,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v13,v21,v29,v0.t
vnclipu.wv v14,v22,v30,v0.t
vnclipu.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwx_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wx v8,v16,t0
vnclipu.wx v9,v17,t1
vnclipu.wx v10,v18,t2
vnclipu.wx v11,v19,t3
vnclipu.wx v12,v20,t4
vnclipu.wx v13,v21,t5
vnclipu.wx v14,v22,t6
vnclipu.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wx v8,v16,t0
vnclipu.wx v9,v17,t1
vnclipu.wx v10,v18,t2
vnclipu.wx v11,v19,t3
vnclipu.wx v12,v20,t4
vnclipu.wx v13,v21,t5
vnclipu.wx v14,v22,t6
vnclipu.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwxm_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v9,v17,t1,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v11,v19,t3,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v13,v21,t5,v0.t
vnclipu.wx v14,v22,t6,v0.t
vnclipu.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v9,v17,t1,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v11,v19,t3,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v13,v21,t5,v0.t
vnclipu.wx v14,v22,t6,v0.t
vnclipu.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwi_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wi v8,v16,13
vnclipu.wi v9,v17,13
vnclipu.wi v10,v18,13
vnclipu.wi v11,v19,13
vnclipu.wi v12,v20,13
vnclipu.wi v13,v21,13
vnclipu.wi v14,v22,13
vnclipu.wi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wi v8,v16,13
vnclipu.wi v9,v17,13
vnclipu.wi v10,v18,13
vnclipu.wi v11,v19,13
vnclipu.wi v12,v20,13
vnclipu.wi v13,v21,13
vnclipu.wi v14,v22,13
vnclipu.wi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwim_m1:
	m_nop
	li a0, WARMUP
1:
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v9,v17,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v11,v19,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v13,v21,13,v0.t
vnclipu.wi v14,v22,13,v0.t
vnclipu.wi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v9,v17,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v11,v19,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v13,v21,13,v0.t
vnclipu.wi v14,v22,13,v0.t
vnclipu.wi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwv_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wv v8,v16,v24
vnclip.wv v9,v17,v25
vnclip.wv v10,v18,v26
vnclip.wv v11,v19,v27
vnclip.wv v12,v20,v28
vnclip.wv v13,v21,v29
vnclip.wv v14,v22,v30
vnclip.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wv v8,v16,v24
vnclip.wv v9,v17,v25
vnclip.wv v10,v18,v26
vnclip.wv v11,v19,v27
vnclip.wv v12,v20,v28
vnclip.wv v13,v21,v29
vnclip.wv v14,v22,v30
vnclip.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwvm_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v9,v17,v25,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v11,v19,v27,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v13,v21,v29,v0.t
vnclip.wv v14,v22,v30,v0.t
vnclip.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v9,v17,v25,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v11,v19,v27,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v13,v21,v29,v0.t
vnclip.wv v14,v22,v30,v0.t
vnclip.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwx_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wx v8,v16,t0
vnclip.wx v9,v17,t1
vnclip.wx v10,v18,t2
vnclip.wx v11,v19,t3
vnclip.wx v12,v20,t4
vnclip.wx v13,v21,t5
vnclip.wx v14,v22,t6
vnclip.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wx v8,v16,t0
vnclip.wx v9,v17,t1
vnclip.wx v10,v18,t2
vnclip.wx v11,v19,t3
vnclip.wx v12,v20,t4
vnclip.wx v13,v21,t5
vnclip.wx v14,v22,t6
vnclip.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwxm_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v9,v17,t1,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v11,v19,t3,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v13,v21,t5,v0.t
vnclip.wx v14,v22,t6,v0.t
vnclip.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v9,v17,t1,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v11,v19,t3,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v13,v21,t5,v0.t
vnclip.wx v14,v22,t6,v0.t
vnclip.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwi_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wi v8,v16,13
vnclip.wi v9,v17,13
vnclip.wi v10,v18,13
vnclip.wi v11,v19,13
vnclip.wi v12,v20,13
vnclip.wi v13,v21,13
vnclip.wi v14,v22,13
vnclip.wi v15,v23,13



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wi v8,v16,13
vnclip.wi v9,v17,13
vnclip.wi v10,v18,13
vnclip.wi v11,v19,13
vnclip.wi v12,v20,13
vnclip.wi v13,v21,13
vnclip.wi v14,v22,13
vnclip.wi v15,v23,13



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwim_m1:
	m_nop
	li a0, WARMUP
1:
vnclip.wi v8,v16,13,v0.t
vnclip.wi v9,v17,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v11,v19,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v13,v21,13,v0.t
vnclip.wi v14,v22,13,v0.t
vnclip.wi v15,v23,13,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnclip.wi v8,v16,13,v0.t
vnclip.wi v9,v17,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v11,v19,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v13,v21,13,v0.t
vnclip.wi v14,v22,13,v0.t
vnclip.wi v15,v23,13,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vnmsub.vv v8,v16,v24
vnmsub.vv v9,v17,v25
vnmsub.vv v10,v18,v26
vnmsub.vv v11,v19,v27
vnmsub.vv v12,v20,v28
vnmsub.vv v13,v21,v29
vnmsub.vv v14,v22,v30
vnmsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsub.vv v8,v16,v24
vnmsub.vv v9,v17,v25
vnmsub.vv v10,v18,v26
vnmsub.vv v11,v19,v27
vnmsub.vv v12,v20,v28
vnmsub.vv v13,v21,v29
vnmsub.vv v14,v22,v30
vnmsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v9,v17,v25,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v11,v19,v27,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v13,v21,v29,v0.t
vnmsub.vv v14,v22,v30,v0.t
vnmsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v9,v17,v25,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v11,v19,v27,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v13,v21,v29,v0.t
vnmsub.vv v14,v22,v30,v0.t
vnmsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvx_m1:
	m_nop
	li a0, WARMUP
1:
vnmsub.vx v8,t0,v16
vnmsub.vx v9,t1,v17
vnmsub.vx v10,t2,v18
vnmsub.vx v11,t3,v19
vnmsub.vx v12,t4,v20
vnmsub.vx v13,t5,v21
vnmsub.vx v14,t6,v22
vnmsub.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsub.vx v8,t0,v16
vnmsub.vx v9,t1,v17
vnmsub.vx v10,t2,v18
vnmsub.vx v11,t3,v19
vnmsub.vx v12,t4,v20
vnmsub.vx v13,t5,v21
vnmsub.vx v14,t6,v22
vnmsub.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v9,t1,v17,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v11,t3,v19,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v13,t5,v21,v0.t
vnmsub.vx v14,t6,v22,v0.t
vnmsub.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v9,t1,v17,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v11,t3,v19,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v13,t5,v21,v0.t
vnmsub.vx v14,t6,v22,v0.t
vnmsub.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvv_m1:
	m_nop
	li a0, WARMUP
1:
vnmsac.vv v8,v16,v24
vnmsac.vv v9,v17,v25
vnmsac.vv v10,v18,v26
vnmsac.vv v11,v19,v27
vnmsac.vv v12,v20,v28
vnmsac.vv v13,v21,v29
vnmsac.vv v14,v22,v30
vnmsac.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsac.vv v8,v16,v24
vnmsac.vv v9,v17,v25
vnmsac.vv v10,v18,v26
vnmsac.vv v11,v19,v27
vnmsac.vv v12,v20,v28
vnmsac.vv v13,v21,v29
vnmsac.vv v14,v22,v30
vnmsac.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvvm_m1:
	m_nop
	li a0, WARMUP
1:
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v9,v17,v25,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v11,v19,v27,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v13,v21,v29,v0.t
vnmsac.vv v14,v22,v30,v0.t
vnmsac.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v9,v17,v25,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v11,v19,v27,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v13,v21,v29,v0.t
vnmsac.vv v14,v22,v30,v0.t
vnmsac.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvx_m1:
	m_nop
	li a0, WARMUP
1:
vnmsac.vx v8,t0,v16
vnmsac.vx v9,t1,v17
vnmsac.vx v10,t2,v18
vnmsac.vx v11,t3,v19
vnmsac.vx v12,t4,v20
vnmsac.vx v13,t5,v21
vnmsac.vx v14,t6,v22
vnmsac.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsac.vx v8,t0,v16
vnmsac.vx v9,t1,v17
vnmsac.vx v10,t2,v18
vnmsac.vx v11,t3,v19
vnmsac.vx v12,t4,v20
vnmsac.vx v13,t5,v21
vnmsac.vx v14,t6,v22
vnmsac.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvxm_m1:
	m_nop
	li a0, WARMUP
1:
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v9,t1,v17,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v11,t3,v19,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v13,t5,v21,v0.t
vnmsac.vx v14,t6,v22,v0.t
vnmsac.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v9,t1,v17,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v11,t3,v19,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v13,t5,v21,v0.t
vnmsac.vx v14,t6,v22,v0.t
vnmsac.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwadduvv_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.vv v8,v16,v24
vwaddu.vv v9,v17,v25
vwaddu.vv v10,v18,v26
vwaddu.vv v11,v19,v27
vwaddu.vv v12,v20,v28
vwaddu.vv v13,v21,v29
vwaddu.vv v14,v22,v30
vwaddu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.vv v8,v16,v24
vwaddu.vv v9,v17,v25
vwaddu.vv v10,v18,v26
vwaddu.vv v11,v19,v27
vwaddu.vv v12,v20,v28
vwaddu.vv v13,v21,v29
vwaddu.vv v14,v22,v30
vwaddu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v9,v17,v25,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v11,v19,v27,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v13,v21,v29,v0.t
vwaddu.vv v14,v22,v30,v0.t
vwaddu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v9,v17,v25,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v11,v19,v27,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v13,v21,v29,v0.t
vwaddu.vv v14,v22,v30,v0.t
vwaddu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvx_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.vx v8,v16,t0
vwaddu.vx v9,v17,t1
vwaddu.vx v10,v18,t2
vwaddu.vx v11,v19,t3
vwaddu.vx v12,v20,t4
vwaddu.vx v13,v21,t5
vwaddu.vx v14,v22,t6
vwaddu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.vx v8,v16,t0
vwaddu.vx v9,v17,t1
vwaddu.vx v10,v18,t2
vwaddu.vx v11,v19,t3
vwaddu.vx v12,v20,t4
vwaddu.vx v13,v21,t5
vwaddu.vx v14,v22,t6
vwaddu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v9,v17,t1,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v11,v19,t3,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v13,v21,t5,v0.t
vwaddu.vx v14,v22,t6,v0.t
vwaddu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v9,v17,t1,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v11,v19,t3,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v13,v21,t5,v0.t
vwaddu.vx v14,v22,t6,v0.t
vwaddu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.vv v8,v16,v24
vwadd.vv v9,v17,v25
vwadd.vv v10,v18,v26
vwadd.vv v11,v19,v27
vwadd.vv v12,v20,v28
vwadd.vv v13,v21,v29
vwadd.vv v14,v22,v30
vwadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.vv v8,v16,v24
vwadd.vv v9,v17,v25
vwadd.vv v10,v18,v26
vwadd.vv v11,v19,v27
vwadd.vv v12,v20,v28
vwadd.vv v13,v21,v29
vwadd.vv v14,v22,v30
vwadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v9,v17,v25,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v11,v19,v27,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v13,v21,v29,v0.t
vwadd.vv v14,v22,v30,v0.t
vwadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v9,v17,v25,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v11,v19,v27,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v13,v21,v29,v0.t
vwadd.vv v14,v22,v30,v0.t
vwadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvx_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.vx v8,v16,t0
vwadd.vx v9,v17,t1
vwadd.vx v10,v18,t2
vwadd.vx v11,v19,t3
vwadd.vx v12,v20,t4
vwadd.vx v13,v21,t5
vwadd.vx v14,v22,t6
vwadd.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.vx v8,v16,t0
vwadd.vx v9,v17,t1
vwadd.vx v10,v18,t2
vwadd.vx v11,v19,t3
vwadd.vx v12,v20,t4
vwadd.vx v13,v21,t5
vwadd.vx v14,v22,t6
vwadd.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v9,v17,t1,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v11,v19,t3,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v13,v21,t5,v0.t
vwadd.vx v14,v22,t6,v0.t
vwadd.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v9,v17,t1,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v11,v19,t3,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v13,v21,t5,v0.t
vwadd.vx v14,v22,t6,v0.t
vwadd.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvv_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.vv v8,v16,v24
vwsubu.vv v9,v17,v25
vwsubu.vv v10,v18,v26
vwsubu.vv v11,v19,v27
vwsubu.vv v12,v20,v28
vwsubu.vv v13,v21,v29
vwsubu.vv v14,v22,v30
vwsubu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.vv v8,v16,v24
vwsubu.vv v9,v17,v25
vwsubu.vv v10,v18,v26
vwsubu.vv v11,v19,v27
vwsubu.vv v12,v20,v28
vwsubu.vv v13,v21,v29
vwsubu.vv v14,v22,v30
vwsubu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v9,v17,v25,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v11,v19,v27,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v13,v21,v29,v0.t
vwsubu.vv v14,v22,v30,v0.t
vwsubu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v9,v17,v25,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v11,v19,v27,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v13,v21,v29,v0.t
vwsubu.vv v14,v22,v30,v0.t
vwsubu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvx_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.vx v8,v16,t0
vwsubu.vx v9,v17,t1
vwsubu.vx v10,v18,t2
vwsubu.vx v11,v19,t3
vwsubu.vx v12,v20,t4
vwsubu.vx v13,v21,t5
vwsubu.vx v14,v22,t6
vwsubu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.vx v8,v16,t0
vwsubu.vx v9,v17,t1
vwsubu.vx v10,v18,t2
vwsubu.vx v11,v19,t3
vwsubu.vx v12,v20,t4
vwsubu.vx v13,v21,t5
vwsubu.vx v14,v22,t6
vwsubu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v9,v17,t1,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v11,v19,t3,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v13,v21,t5,v0.t
vwsubu.vx v14,v22,t6,v0.t
vwsubu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v9,v17,t1,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v11,v19,t3,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v13,v21,t5,v0.t
vwsubu.vx v14,v22,t6,v0.t
vwsubu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.vv v8,v16,v24
vwsub.vv v9,v17,v25
vwsub.vv v10,v18,v26
vwsub.vv v11,v19,v27
vwsub.vv v12,v20,v28
vwsub.vv v13,v21,v29
vwsub.vv v14,v22,v30
vwsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.vv v8,v16,v24
vwsub.vv v9,v17,v25
vwsub.vv v10,v18,v26
vwsub.vv v11,v19,v27
vwsub.vv v12,v20,v28
vwsub.vv v13,v21,v29
vwsub.vv v14,v22,v30
vwsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v9,v17,v25,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v11,v19,v27,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v13,v21,v29,v0.t
vwsub.vv v14,v22,v30,v0.t
vwsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v9,v17,v25,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v11,v19,v27,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v13,v21,v29,v0.t
vwsub.vv v14,v22,v30,v0.t
vwsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvx_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.vx v8,v16,t0
vwsub.vx v9,v17,t1
vwsub.vx v10,v18,t2
vwsub.vx v11,v19,t3
vwsub.vx v12,v20,t4
vwsub.vx v13,v21,t5
vwsub.vx v14,v22,t6
vwsub.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.vx v8,v16,t0
vwsub.vx v9,v17,t1
vwsub.vx v10,v18,t2
vwsub.vx v11,v19,t3
vwsub.vx v12,v20,t4
vwsub.vx v13,v21,t5
vwsub.vx v14,v22,t6
vwsub.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v9,v17,t1,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v11,v19,t3,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v13,v21,t5,v0.t
vwsub.vx v14,v22,t6,v0.t
vwsub.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v9,v17,t1,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v11,v19,t3,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v13,v21,t5,v0.t
vwsub.vx v14,v22,t6,v0.t
vwsub.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwv_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.wv v8,v16,v24
vwaddu.wv v9,v17,v25
vwaddu.wv v10,v18,v26
vwaddu.wv v11,v19,v27
vwaddu.wv v12,v20,v28
vwaddu.wv v13,v21,v29
vwaddu.wv v14,v22,v30
vwaddu.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.wv v8,v16,v24
vwaddu.wv v9,v17,v25
vwaddu.wv v10,v18,v26
vwaddu.wv v11,v19,v27
vwaddu.wv v12,v20,v28
vwaddu.wv v13,v21,v29
vwaddu.wv v14,v22,v30
vwaddu.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwvm_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v9,v17,v25,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v11,v19,v27,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v13,v21,v29,v0.t
vwaddu.wv v14,v22,v30,v0.t
vwaddu.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v9,v17,v25,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v11,v19,v27,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v13,v21,v29,v0.t
vwaddu.wv v14,v22,v30,v0.t
vwaddu.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwx_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.wx v8,v16,t0
vwaddu.wx v9,v17,t1
vwaddu.wx v10,v18,t2
vwaddu.wx v11,v19,t3
vwaddu.wx v12,v20,t4
vwaddu.wx v13,v21,t5
vwaddu.wx v14,v22,t6
vwaddu.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.wx v8,v16,t0
vwaddu.wx v9,v17,t1
vwaddu.wx v10,v18,t2
vwaddu.wx v11,v19,t3
vwaddu.wx v12,v20,t4
vwaddu.wx v13,v21,t5
vwaddu.wx v14,v22,t6
vwaddu.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwxm_m1:
	m_nop
	li a0, WARMUP
1:
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v9,v17,t1,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v11,v19,t3,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v13,v21,t5,v0.t
vwaddu.wx v14,v22,t6,v0.t
vwaddu.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v9,v17,t1,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v11,v19,t3,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v13,v21,t5,v0.t
vwaddu.wx v14,v22,t6,v0.t
vwaddu.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwv_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.wv v8,v16,v24
vwadd.wv v9,v17,v25
vwadd.wv v10,v18,v26
vwadd.wv v11,v19,v27
vwadd.wv v12,v20,v28
vwadd.wv v13,v21,v29
vwadd.wv v14,v22,v30
vwadd.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.wv v8,v16,v24
vwadd.wv v9,v17,v25
vwadd.wv v10,v18,v26
vwadd.wv v11,v19,v27
vwadd.wv v12,v20,v28
vwadd.wv v13,v21,v29
vwadd.wv v14,v22,v30
vwadd.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwvm_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v9,v17,v25,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v11,v19,v27,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v13,v21,v29,v0.t
vwadd.wv v14,v22,v30,v0.t
vwadd.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v9,v17,v25,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v11,v19,v27,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v13,v21,v29,v0.t
vwadd.wv v14,v22,v30,v0.t
vwadd.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwx_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.wx v8,v16,t0
vwadd.wx v9,v17,t1
vwadd.wx v10,v18,t2
vwadd.wx v11,v19,t3
vwadd.wx v12,v20,t4
vwadd.wx v13,v21,t5
vwadd.wx v14,v22,t6
vwadd.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.wx v8,v16,t0
vwadd.wx v9,v17,t1
vwadd.wx v10,v18,t2
vwadd.wx v11,v19,t3
vwadd.wx v12,v20,t4
vwadd.wx v13,v21,t5
vwadd.wx v14,v22,t6
vwadd.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwxm_m1:
	m_nop
	li a0, WARMUP
1:
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v9,v17,t1,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v11,v19,t3,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v13,v21,t5,v0.t
vwadd.wx v14,v22,t6,v0.t
vwadd.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v9,v17,t1,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v11,v19,t3,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v13,v21,t5,v0.t
vwadd.wx v14,v22,t6,v0.t
vwadd.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwv_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.wv v8,v16,v24
vwsubu.wv v9,v17,v25
vwsubu.wv v10,v18,v26
vwsubu.wv v11,v19,v27
vwsubu.wv v12,v20,v28
vwsubu.wv v13,v21,v29
vwsubu.wv v14,v22,v30
vwsubu.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.wv v8,v16,v24
vwsubu.wv v9,v17,v25
vwsubu.wv v10,v18,v26
vwsubu.wv v11,v19,v27
vwsubu.wv v12,v20,v28
vwsubu.wv v13,v21,v29
vwsubu.wv v14,v22,v30
vwsubu.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwvm_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v9,v17,v25,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v11,v19,v27,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v13,v21,v29,v0.t
vwsubu.wv v14,v22,v30,v0.t
vwsubu.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v9,v17,v25,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v11,v19,v27,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v13,v21,v29,v0.t
vwsubu.wv v14,v22,v30,v0.t
vwsubu.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwx_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.wx v8,v16,t0
vwsubu.wx v9,v17,t1
vwsubu.wx v10,v18,t2
vwsubu.wx v11,v19,t3
vwsubu.wx v12,v20,t4
vwsubu.wx v13,v21,t5
vwsubu.wx v14,v22,t6
vwsubu.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.wx v8,v16,t0
vwsubu.wx v9,v17,t1
vwsubu.wx v10,v18,t2
vwsubu.wx v11,v19,t3
vwsubu.wx v12,v20,t4
vwsubu.wx v13,v21,t5
vwsubu.wx v14,v22,t6
vwsubu.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwxm_m1:
	m_nop
	li a0, WARMUP
1:
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v9,v17,t1,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v11,v19,t3,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v13,v21,t5,v0.t
vwsubu.wx v14,v22,t6,v0.t
vwsubu.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v9,v17,t1,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v11,v19,t3,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v13,v21,t5,v0.t
vwsubu.wx v14,v22,t6,v0.t
vwsubu.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwv_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.wv v8,v16,v24
vwsub.wv v9,v17,v25
vwsub.wv v10,v18,v26
vwsub.wv v11,v19,v27
vwsub.wv v12,v20,v28
vwsub.wv v13,v21,v29
vwsub.wv v14,v22,v30
vwsub.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.wv v8,v16,v24
vwsub.wv v9,v17,v25
vwsub.wv v10,v18,v26
vwsub.wv v11,v19,v27
vwsub.wv v12,v20,v28
vwsub.wv v13,v21,v29
vwsub.wv v14,v22,v30
vwsub.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwvm_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v9,v17,v25,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v11,v19,v27,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v13,v21,v29,v0.t
vwsub.wv v14,v22,v30,v0.t
vwsub.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v9,v17,v25,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v11,v19,v27,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v13,v21,v29,v0.t
vwsub.wv v14,v22,v30,v0.t
vwsub.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwx_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.wx v8,v16,t0
vwsub.wx v9,v17,t1
vwsub.wx v10,v18,t2
vwsub.wx v11,v19,t3
vwsub.wx v12,v20,t4
vwsub.wx v13,v21,t5
vwsub.wx v14,v22,t6
vwsub.wx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.wx v8,v16,t0
vwsub.wx v9,v17,t1
vwsub.wx v10,v18,t2
vwsub.wx v11,v19,t3
vwsub.wx v12,v20,t4
vwsub.wx v13,v21,t5
vwsub.wx v14,v22,t6
vwsub.wx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwxm_m1:
	m_nop
	li a0, WARMUP
1:
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v9,v17,t1,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v11,v19,t3,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v13,v21,t5,v0.t
vwsub.wx v14,v22,t6,v0.t
vwsub.wx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v9,v17,t1,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v11,v19,t3,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v13,v21,t5,v0.t
vwsub.wx v14,v22,t6,v0.t
vwsub.wx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmulu.vv v8,v16,v24
vwmulu.vv v9,v17,v25
vwmulu.vv v10,v18,v26
vwmulu.vv v11,v19,v27
vwmulu.vv v12,v20,v28
vwmulu.vv v13,v21,v29
vwmulu.vv v14,v22,v30
vwmulu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulu.vv v8,v16,v24
vwmulu.vv v9,v17,v25
vwmulu.vv v10,v18,v26
vwmulu.vv v11,v19,v27
vwmulu.vv v12,v20,v28
vwmulu.vv v13,v21,v29
vwmulu.vv v14,v22,v30
vwmulu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v9,v17,v25,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v11,v19,v27,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v13,v21,v29,v0.t
vwmulu.vv v14,v22,v30,v0.t
vwmulu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v9,v17,v25,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v11,v19,v27,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v13,v21,v29,v0.t
vwmulu.vv v14,v22,v30,v0.t
vwmulu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmulu.vx v8,v16,t0
vwmulu.vx v9,v17,t1
vwmulu.vx v10,v18,t2
vwmulu.vx v11,v19,t3
vwmulu.vx v12,v20,t4
vwmulu.vx v13,v21,t5
vwmulu.vx v14,v22,t6
vwmulu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulu.vx v8,v16,t0
vwmulu.vx v9,v17,t1
vwmulu.vx v10,v18,t2
vwmulu.vx v11,v19,t3
vwmulu.vx v12,v20,t4
vwmulu.vx v13,v21,t5
vwmulu.vx v14,v22,t6
vwmulu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v9,v17,t1,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v11,v19,t3,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v13,v21,t5,v0.t
vwmulu.vx v14,v22,t6,v0.t
vwmulu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v9,v17,t1,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v11,v19,t3,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v13,v21,t5,v0.t
vwmulu.vx v14,v22,t6,v0.t
vwmulu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmulsu.vv v8,v16,v24
vwmulsu.vv v9,v17,v25
vwmulsu.vv v10,v18,v26
vwmulsu.vv v11,v19,v27
vwmulsu.vv v12,v20,v28
vwmulsu.vv v13,v21,v29
vwmulsu.vv v14,v22,v30
vwmulsu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulsu.vv v8,v16,v24
vwmulsu.vv v9,v17,v25
vwmulsu.vv v10,v18,v26
vwmulsu.vv v11,v19,v27
vwmulsu.vv v12,v20,v28
vwmulsu.vv v13,v21,v29
vwmulsu.vv v14,v22,v30
vwmulsu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v9,v17,v25,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v11,v19,v27,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v13,v21,v29,v0.t
vwmulsu.vv v14,v22,v30,v0.t
vwmulsu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v9,v17,v25,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v11,v19,v27,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v13,v21,v29,v0.t
vwmulsu.vv v14,v22,v30,v0.t
vwmulsu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmulsu.vx v8,v16,t0
vwmulsu.vx v9,v17,t1
vwmulsu.vx v10,v18,t2
vwmulsu.vx v11,v19,t3
vwmulsu.vx v12,v20,t4
vwmulsu.vx v13,v21,t5
vwmulsu.vx v14,v22,t6
vwmulsu.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulsu.vx v8,v16,t0
vwmulsu.vx v9,v17,t1
vwmulsu.vx v10,v18,t2
vwmulsu.vx v11,v19,t3
vwmulsu.vx v12,v20,t4
vwmulsu.vx v13,v21,t5
vwmulsu.vx v14,v22,t6
vwmulsu.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v9,v17,t1,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v11,v19,t3,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v13,v21,t5,v0.t
vwmulsu.vx v14,v22,t6,v0.t
vwmulsu.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v9,v17,t1,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v11,v19,t3,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v13,v21,t5,v0.t
vwmulsu.vx v14,v22,t6,v0.t
vwmulsu.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmul.vv v8,v16,v24
vwmul.vv v9,v17,v25
vwmul.vv v10,v18,v26
vwmul.vv v11,v19,v27
vwmul.vv v12,v20,v28
vwmul.vv v13,v21,v29
vwmul.vv v14,v22,v30
vwmul.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmul.vv v8,v16,v24
vwmul.vv v9,v17,v25
vwmul.vv v10,v18,v26
vwmul.vv v11,v19,v27
vwmul.vv v12,v20,v28
vwmul.vv v13,v21,v29
vwmul.vv v14,v22,v30
vwmul.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v9,v17,v25,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v11,v19,v27,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v13,v21,v29,v0.t
vwmul.vv v14,v22,v30,v0.t
vwmul.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v9,v17,v25,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v11,v19,v27,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v13,v21,v29,v0.t
vwmul.vv v14,v22,v30,v0.t
vwmul.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmul.vx v8,v16,t0
vwmul.vx v9,v17,t1
vwmul.vx v10,v18,t2
vwmul.vx v11,v19,t3
vwmul.vx v12,v20,t4
vwmul.vx v13,v21,t5
vwmul.vx v14,v22,t6
vwmul.vx v15,v23,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmul.vx v8,v16,t0
vwmul.vx v9,v17,t1
vwmul.vx v10,v18,t2
vwmul.vx v11,v19,t3
vwmul.vx v12,v20,t4
vwmul.vx v13,v21,t5
vwmul.vx v14,v22,t6
vwmul.vx v15,v23,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v9,v17,t1,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v11,v19,t3,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v13,v21,t5,v0.t
vwmul.vx v14,v22,t6,v0.t
vwmul.vx v15,v23,t7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v9,v17,t1,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v11,v19,t3,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v13,v21,t5,v0.t
vwmul.vx v14,v22,t6,v0.t
vwmul.vx v15,v23,t7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccu.vv v8,v16,v24
vwmaccu.vv v9,v17,v25
vwmaccu.vv v10,v18,v26
vwmaccu.vv v11,v19,v27
vwmaccu.vv v12,v20,v28
vwmaccu.vv v13,v21,v29
vwmaccu.vv v14,v22,v30
vwmaccu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccu.vv v8,v16,v24
vwmaccu.vv v9,v17,v25
vwmaccu.vv v10,v18,v26
vwmaccu.vv v11,v19,v27
vwmaccu.vv v12,v20,v28
vwmaccu.vv v13,v21,v29
vwmaccu.vv v14,v22,v30
vwmaccu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v9,v17,v25,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v11,v19,v27,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v13,v21,v29,v0.t
vwmaccu.vv v14,v22,v30,v0.t
vwmaccu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v9,v17,v25,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v11,v19,v27,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v13,v21,v29,v0.t
vwmaccu.vv v14,v22,v30,v0.t
vwmaccu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccu.vx v8,t0,v16
vwmaccu.vx v9,t1,v17
vwmaccu.vx v10,t2,v18
vwmaccu.vx v11,t3,v19
vwmaccu.vx v12,t4,v20
vwmaccu.vx v13,t5,v21
vwmaccu.vx v14,t6,v22
vwmaccu.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccu.vx v8,t0,v16
vwmaccu.vx v9,t1,v17
vwmaccu.vx v10,t2,v18
vwmaccu.vx v11,t3,v19
vwmaccu.vx v12,t4,v20
vwmaccu.vx v13,t5,v21
vwmaccu.vx v14,t6,v22
vwmaccu.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v9,t1,v17,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v11,t3,v19,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v13,t5,v21,v0.t
vwmaccu.vx v14,t6,v22,v0.t
vwmaccu.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v9,t1,v17,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v11,t3,v19,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v13,t5,v21,v0.t
vwmaccu.vx v14,t6,v22,v0.t
vwmaccu.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmacc.vv v8,v16,v24
vwmacc.vv v9,v17,v25
vwmacc.vv v10,v18,v26
vwmacc.vv v11,v19,v27
vwmacc.vv v12,v20,v28
vwmacc.vv v13,v21,v29
vwmacc.vv v14,v22,v30
vwmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmacc.vv v8,v16,v24
vwmacc.vv v9,v17,v25
vwmacc.vv v10,v18,v26
vwmacc.vv v11,v19,v27
vwmacc.vv v12,v20,v28
vwmacc.vv v13,v21,v29
vwmacc.vv v14,v22,v30
vwmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v9,v17,v25,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v11,v19,v27,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v13,v21,v29,v0.t
vwmacc.vv v14,v22,v30,v0.t
vwmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v9,v17,v25,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v11,v19,v27,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v13,v21,v29,v0.t
vwmacc.vv v14,v22,v30,v0.t
vwmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmacc.vx v8,t0,v16
vwmacc.vx v9,t1,v17
vwmacc.vx v10,t2,v18
vwmacc.vx v11,t3,v19
vwmacc.vx v12,t4,v20
vwmacc.vx v13,t5,v21
vwmacc.vx v14,t6,v22
vwmacc.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmacc.vx v8,t0,v16
vwmacc.vx v9,t1,v17
vwmacc.vx v10,t2,v18
vwmacc.vx v11,t3,v19
vwmacc.vx v12,t4,v20
vwmacc.vx v13,t5,v21
vwmacc.vx v14,t6,v22
vwmacc.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v9,t1,v17,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v11,t3,v19,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v13,t5,v21,v0.t
vwmacc.vx v14,t6,v22,v0.t
vwmacc.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v9,t1,v17,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v11,t3,v19,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v13,t5,v21,v0.t
vwmacc.vx v14,t6,v22,v0.t
vwmacc.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvv_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v9,v17,v25
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v11,v19,v27
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v13,v21,v29
vwmaccsu.vv v14,v22,v30
vwmaccsu.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v9,v17,v25
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v11,v19,v27
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v13,v21,v29
vwmaccsu.vv v14,v22,v30
vwmaccsu.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvvm_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v9,v17,v25,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v11,v19,v27,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v13,v21,v29,v0.t
vwmaccsu.vv v14,v22,v30,v0.t
vwmaccsu.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v9,v17,v25,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v11,v19,v27,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v13,v21,v29,v0.t
vwmaccsu.vv v14,v22,v30,v0.t
vwmaccsu.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v9,t1,v17
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v11,t3,v19
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v13,t5,v21
vwmaccsu.vx v14,t6,v22
vwmaccsu.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v9,t1,v17
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v11,t3,v19
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v13,t5,v21
vwmaccsu.vx v14,t6,v22
vwmaccsu.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v9,t1,v17,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v11,t3,v19,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v13,t5,v21,v0.t
vwmaccsu.vx v14,t6,v22,v0.t
vwmaccsu.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v9,t1,v17,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v11,t3,v19,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v13,t5,v21,v0.t
vwmaccsu.vx v14,t6,v22,v0.t
vwmaccsu.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvx_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccus.vx v8,t0,v16
vwmaccus.vx v9,t1,v17
vwmaccus.vx v10,t2,v18
vwmaccus.vx v11,t3,v19
vwmaccus.vx v12,t4,v20
vwmaccus.vx v13,t5,v21
vwmaccus.vx v14,t6,v22
vwmaccus.vx v15,t7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccus.vx v8,t0,v16
vwmaccus.vx v9,t1,v17
vwmaccus.vx v10,t2,v18
vwmaccus.vx v11,t3,v19
vwmaccus.vx v12,t4,v20
vwmaccus.vx v13,t5,v21
vwmaccus.vx v14,t6,v22
vwmaccus.vx v15,t7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvxm_m1:
	m_nop
	li a0, WARMUP
1:
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v9,t1,v17,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v11,t3,v19,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v13,t5,v21,v0.t
vwmaccus.vx v14,t6,v22,v0.t
vwmaccus.vx v15,t7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v9,t1,v17,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v11,t3,v19,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v13,t5,v21,v0.t
vwmaccus.vx v14,t6,v22,v0.t
vwmaccus.vx v15,t7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vfadd.vv v8,v16,v24
vfadd.vv v9,v17,v25
vfadd.vv v10,v18,v26
vfadd.vv v11,v19,v27
vfadd.vv v12,v20,v28
vfadd.vv v13,v21,v29
vfadd.vv v14,v22,v30
vfadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfadd.vv v8,v16,v24
vfadd.vv v9,v17,v25
vfadd.vv v10,v18,v26
vfadd.vv v11,v19,v27
vfadd.vv v12,v20,v28
vfadd.vv v13,v21,v29
vfadd.vv v14,v22,v30
vfadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v9,v17,v25,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v11,v19,v27,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v13,v21,v29,v0.t
vfadd.vv v14,v22,v30,v0.t
vfadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v9,v17,v25,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v11,v19,v27,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v13,v21,v29,v0.t
vfadd.vv v14,v22,v30,v0.t
vfadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvf_m1:
	m_nop
	li a0, WARMUP
1:
vfadd.vf v8,v16,ft0
vfadd.vf v9,v17,ft1
vfadd.vf v10,v18,ft2
vfadd.vf v11,v19,ft3
vfadd.vf v12,v20,ft4
vfadd.vf v13,v21,ft5
vfadd.vf v14,v22,ft6
vfadd.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfadd.vf v8,v16,ft0
vfadd.vf v9,v17,ft1
vfadd.vf v10,v18,ft2
vfadd.vf v11,v19,ft3
vfadd.vf v12,v20,ft4
vfadd.vf v13,v21,ft5
vfadd.vf v14,v22,ft6
vfadd.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v9,v17,ft1,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v11,v19,ft3,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v13,v21,ft5,v0.t
vfadd.vf v14,v22,ft6,v0.t
vfadd.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v9,v17,ft1,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v11,v19,ft3,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v13,v21,ft5,v0.t
vfadd.vf v14,v22,ft6,v0.t
vfadd.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vfsub.vv v8,v16,v24
vfsub.vv v9,v17,v25
vfsub.vv v10,v18,v26
vfsub.vv v11,v19,v27
vfsub.vv v12,v20,v28
vfsub.vv v13,v21,v29
vfsub.vv v14,v22,v30
vfsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsub.vv v8,v16,v24
vfsub.vv v9,v17,v25
vfsub.vv v10,v18,v26
vfsub.vv v11,v19,v27
vfsub.vv v12,v20,v28
vfsub.vv v13,v21,v29
vfsub.vv v14,v22,v30
vfsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v9,v17,v25,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v11,v19,v27,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v13,v21,v29,v0.t
vfsub.vv v14,v22,v30,v0.t
vfsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v9,v17,v25,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v11,v19,v27,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v13,v21,v29,v0.t
vfsub.vv v14,v22,v30,v0.t
vfsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvf_m1:
	m_nop
	li a0, WARMUP
1:
vfsub.vf v8,v16,ft0
vfsub.vf v9,v17,ft1
vfsub.vf v10,v18,ft2
vfsub.vf v11,v19,ft3
vfsub.vf v12,v20,ft4
vfsub.vf v13,v21,ft5
vfsub.vf v14,v22,ft6
vfsub.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsub.vf v8,v16,ft0
vfsub.vf v9,v17,ft1
vfsub.vf v10,v18,ft2
vfsub.vf v11,v19,ft3
vfsub.vf v12,v20,ft4
vfsub.vf v13,v21,ft5
vfsub.vf v14,v22,ft6
vfsub.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v9,v17,ft1,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v11,v19,ft3,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v13,v21,ft5,v0.t
vfsub.vf v14,v22,ft6,v0.t
vfsub.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v9,v17,ft1,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v11,v19,ft3,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v13,v21,ft5,v0.t
vfsub.vf v14,v22,ft6,v0.t
vfsub.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmin.vv v8,v16,v24
vfmin.vv v9,v17,v25
vfmin.vv v10,v18,v26
vfmin.vv v11,v19,v27
vfmin.vv v12,v20,v28
vfmin.vv v13,v21,v29
vfmin.vv v14,v22,v30
vfmin.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmin.vv v8,v16,v24
vfmin.vv v9,v17,v25
vfmin.vv v10,v18,v26
vfmin.vv v11,v19,v27
vfmin.vv v12,v20,v28
vfmin.vv v13,v21,v29
vfmin.vv v14,v22,v30
vfmin.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v9,v17,v25,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v11,v19,v27,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v13,v21,v29,v0.t
vfmin.vv v14,v22,v30,v0.t
vfmin.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v9,v17,v25,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v11,v19,v27,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v13,v21,v29,v0.t
vfmin.vv v14,v22,v30,v0.t
vfmin.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmin.vf v8,v16,ft0
vfmin.vf v9,v17,ft1
vfmin.vf v10,v18,ft2
vfmin.vf v11,v19,ft3
vfmin.vf v12,v20,ft4
vfmin.vf v13,v21,ft5
vfmin.vf v14,v22,ft6
vfmin.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmin.vf v8,v16,ft0
vfmin.vf v9,v17,ft1
vfmin.vf v10,v18,ft2
vfmin.vf v11,v19,ft3
vfmin.vf v12,v20,ft4
vfmin.vf v13,v21,ft5
vfmin.vf v14,v22,ft6
vfmin.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v9,v17,ft1,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v11,v19,ft3,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v13,v21,ft5,v0.t
vfmin.vf v14,v22,ft6,v0.t
vfmin.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v9,v17,ft1,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v11,v19,ft3,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v13,v21,ft5,v0.t
vfmin.vf v14,v22,ft6,v0.t
vfmin.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmax.vv v8,v16,v24
vfmax.vv v9,v17,v25
vfmax.vv v10,v18,v26
vfmax.vv v11,v19,v27
vfmax.vv v12,v20,v28
vfmax.vv v13,v21,v29
vfmax.vv v14,v22,v30
vfmax.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmax.vv v8,v16,v24
vfmax.vv v9,v17,v25
vfmax.vv v10,v18,v26
vfmax.vv v11,v19,v27
vfmax.vv v12,v20,v28
vfmax.vv v13,v21,v29
vfmax.vv v14,v22,v30
vfmax.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v9,v17,v25,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v11,v19,v27,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v13,v21,v29,v0.t
vfmax.vv v14,v22,v30,v0.t
vfmax.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v9,v17,v25,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v11,v19,v27,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v13,v21,v29,v0.t
vfmax.vv v14,v22,v30,v0.t
vfmax.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmax.vf v8,v16,ft0
vfmax.vf v9,v17,ft1
vfmax.vf v10,v18,ft2
vfmax.vf v11,v19,ft3
vfmax.vf v12,v20,ft4
vfmax.vf v13,v21,ft5
vfmax.vf v14,v22,ft6
vfmax.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmax.vf v8,v16,ft0
vfmax.vf v9,v17,ft1
vfmax.vf v10,v18,ft2
vfmax.vf v11,v19,ft3
vfmax.vf v12,v20,ft4
vfmax.vf v13,v21,ft5
vfmax.vf v14,v22,ft6
vfmax.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v9,v17,ft1,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v11,v19,ft3,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v13,v21,ft5,v0.t
vfmax.vf v14,v22,ft6,v0.t
vfmax.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v9,v17,ft1,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v11,v19,ft3,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v13,v21,ft5,v0.t
vfmax.vf v14,v22,ft6,v0.t
vfmax.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvv_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnj.vv v8,v16,v24
vfsgnj.vv v9,v17,v25
vfsgnj.vv v10,v18,v26
vfsgnj.vv v11,v19,v27
vfsgnj.vv v12,v20,v28
vfsgnj.vv v13,v21,v29
vfsgnj.vv v14,v22,v30
vfsgnj.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnj.vv v8,v16,v24
vfsgnj.vv v9,v17,v25
vfsgnj.vv v10,v18,v26
vfsgnj.vv v11,v19,v27
vfsgnj.vv v12,v20,v28
vfsgnj.vv v13,v21,v29
vfsgnj.vv v14,v22,v30
vfsgnj.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v9,v17,v25,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v11,v19,v27,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v13,v21,v29,v0.t
vfsgnj.vv v14,v22,v30,v0.t
vfsgnj.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v9,v17,v25,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v11,v19,v27,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v13,v21,v29,v0.t
vfsgnj.vv v14,v22,v30,v0.t
vfsgnj.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvf_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v9,v17,ft1
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v11,v19,ft3
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v13,v21,ft5
vfsgnj.vf v14,v22,ft6
vfsgnj.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v9,v17,ft1
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v11,v19,ft3
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v13,v21,ft5
vfsgnj.vf v14,v22,ft6
vfsgnj.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v9,v17,ft1,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v11,v19,ft3,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v13,v21,ft5,v0.t
vfsgnj.vf v14,v22,ft6,v0.t
vfsgnj.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v9,v17,ft1,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v11,v19,ft3,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v13,v21,ft5,v0.t
vfsgnj.vf v14,v22,ft6,v0.t
vfsgnj.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvv_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v9,v17,v25
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v11,v19,v27
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v13,v21,v29
vfsgnjn.vv v14,v22,v30
vfsgnjn.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v9,v17,v25
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v11,v19,v27
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v13,v21,v29
vfsgnjn.vv v14,v22,v30
vfsgnjn.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v9,v17,v25,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v11,v19,v27,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v13,v21,v29,v0.t
vfsgnjn.vv v14,v22,v30,v0.t
vfsgnjn.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v9,v17,v25,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v11,v19,v27,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v13,v21,v29,v0.t
vfsgnjn.vv v14,v22,v30,v0.t
vfsgnjn.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvf_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v9,v17,ft1
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v11,v19,ft3
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v13,v21,ft5
vfsgnjn.vf v14,v22,ft6
vfsgnjn.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v9,v17,ft1
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v11,v19,ft3
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v13,v21,ft5
vfsgnjn.vf v14,v22,ft6
vfsgnjn.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v9,v17,ft1,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v11,v19,ft3,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v13,v21,ft5,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t
vfsgnjn.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v9,v17,ft1,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v11,v19,ft3,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v13,v21,ft5,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t
vfsgnjn.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvv_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v9,v17,v25
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v11,v19,v27
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v13,v21,v29
vfsgnjx.vv v14,v22,v30
vfsgnjx.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v9,v17,v25
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v11,v19,v27
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v13,v21,v29
vfsgnjx.vv v14,v22,v30
vfsgnjx.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v9,v17,v25,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v11,v19,v27,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v13,v21,v29,v0.t
vfsgnjx.vv v14,v22,v30,v0.t
vfsgnjx.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v9,v17,v25,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v11,v19,v27,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v13,v21,v29,v0.t
vfsgnjx.vv v14,v22,v30,v0.t
vfsgnjx.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvf_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v9,v17,ft1
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v11,v19,ft3
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v13,v21,ft5
vfsgnjx.vf v14,v22,ft6
vfsgnjx.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v9,v17,ft1
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v11,v19,ft3
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v13,v21,ft5
vfsgnjx.vf v14,v22,ft6
vfsgnjx.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v9,v17,ft1,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v11,v19,ft3,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v13,v21,ft5,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t
vfsgnjx.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v9,v17,ft1,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v11,v19,ft3,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v13,v21,ft5,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t
vfsgnjx.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvf_m1:
	m_nop
	li a0, WARMUP
1:
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v9,v17,ft1
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v11,v19,ft3
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v13,v21,ft5
vfslide1up.vf v14,v22,ft6
vfslide1up.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v9,v17,ft1
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v11,v19,ft3
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v13,v21,ft5
vfslide1up.vf v14,v22,ft6
vfslide1up.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v9,v17,ft1,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v11,v19,ft3,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v13,v21,ft5,v0.t
vfslide1up.vf v14,v22,ft6,v0.t
vfslide1up.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v9,v17,ft1,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v11,v19,ft3,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v13,v21,ft5,v0.t
vfslide1up.vf v14,v22,ft6,v0.t
vfslide1up.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvf_m1:
	m_nop
	li a0, WARMUP
1:
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v9,v17,ft1
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v11,v19,ft3
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v13,v21,ft5
vfslide1down.vf v14,v22,ft6
vfslide1down.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v9,v17,ft1
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v11,v19,ft3
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v13,v21,ft5
vfslide1down.vf v14,v22,ft6
vfslide1down.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v9,v17,ft1,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v11,v19,ft3,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v13,v21,ft5,v0.t
vfslide1down.vf v14,v22,ft6,v0.t
vfslide1down.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v9,v17,ft1,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v11,v19,ft3,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v13,v21,ft5,v0.t
vfslide1down.vf v14,v22,ft6,v0.t
vfslide1down.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfredusumvs_m1:
	m_nop
	li a0, WARMUP
1:
vfredusum.vs v8,v16,v24
vfredusum.vs v9,v17,v25
vfredusum.vs v10,v18,v26
vfredusum.vs v11,v19,v27
vfredusum.vs v12,v20,v28
vfredusum.vs v13,v21,v29
vfredusum.vs v14,v22,v30
vfredusum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredusum.vs v8,v16,v24
vfredusum.vs v9,v17,v25
vfredusum.vs v10,v18,v26
vfredusum.vs v11,v19,v27
vfredusum.vs v12,v20,v28
vfredusum.vs v13,v21,v29
vfredusum.vs v14,v22,v30
vfredusum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredusumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v9,v17,v25,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v11,v19,v27,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v13,v21,v29,v0.t
vfredusum.vs v14,v22,v30,v0.t
vfredusum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v9,v17,v25,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v11,v19,v27,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v13,v21,v29,v0.t
vfredusum.vs v14,v22,v30,v0.t
vfredusum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvs_m1:
	m_nop
	li a0, WARMUP
1:
vfredosum.vs v8,v16,v24
vfredosum.vs v9,v17,v25
vfredosum.vs v10,v18,v26
vfredosum.vs v11,v19,v27
vfredosum.vs v12,v20,v28
vfredosum.vs v13,v21,v29
vfredosum.vs v14,v22,v30
vfredosum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredosum.vs v8,v16,v24
vfredosum.vs v9,v17,v25
vfredosum.vs v10,v18,v26
vfredosum.vs v11,v19,v27
vfredosum.vs v12,v20,v28
vfredosum.vs v13,v21,v29
vfredosum.vs v14,v22,v30
vfredosum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v9,v17,v25,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v11,v19,v27,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v13,v21,v29,v0.t
vfredosum.vs v14,v22,v30,v0.t
vfredosum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v9,v17,v25,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v11,v19,v27,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v13,v21,v29,v0.t
vfredosum.vs v14,v22,v30,v0.t
vfredosum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvs_m1:
	m_nop
	li a0, WARMUP
1:
vfredmin.vs v8,v16,v24
vfredmin.vs v9,v17,v25
vfredmin.vs v10,v18,v26
vfredmin.vs v11,v19,v27
vfredmin.vs v12,v20,v28
vfredmin.vs v13,v21,v29
vfredmin.vs v14,v22,v30
vfredmin.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredmin.vs v8,v16,v24
vfredmin.vs v9,v17,v25
vfredmin.vs v10,v18,v26
vfredmin.vs v11,v19,v27
vfredmin.vs v12,v20,v28
vfredmin.vs v13,v21,v29
vfredmin.vs v14,v22,v30
vfredmin.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v9,v17,v25,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v11,v19,v27,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v13,v21,v29,v0.t
vfredmin.vs v14,v22,v30,v0.t
vfredmin.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v9,v17,v25,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v11,v19,v27,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v13,v21,v29,v0.t
vfredmin.vs v14,v22,v30,v0.t
vfredmin.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvs_m1:
	m_nop
	li a0, WARMUP
1:
vfredmax.vs v8,v16,v24
vfredmax.vs v9,v17,v25
vfredmax.vs v10,v18,v26
vfredmax.vs v11,v19,v27
vfredmax.vs v12,v20,v28
vfredmax.vs v13,v21,v29
vfredmax.vs v14,v22,v30
vfredmax.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredmax.vs v8,v16,v24
vfredmax.vs v9,v17,v25
vfredmax.vs v10,v18,v26
vfredmax.vs v11,v19,v27
vfredmax.vs v12,v20,v28
vfredmax.vs v13,v21,v29
vfredmax.vs v14,v22,v30
vfredmax.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v9,v17,v25,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v11,v19,v27,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v13,v21,v29,v0.t
vfredmax.vs v14,v22,v30,v0.t
vfredmax.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v9,v17,v25,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v11,v19,v27,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v13,v21,v29,v0.t
vfredmax.vs v14,v22,v30,v0.t
vfredmax.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmergevfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v9,v17,ft1,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v11,v19,ft3,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v13,v21,ft5,v0
vfmerge.vfm v14,v22,ft6,v0
vfmerge.vfm v15,v23,ft7,v0



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v9,v17,ft1,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v11,v19,ft3,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v13,v21,ft5,v0
vfmerge.vfm v14,v22,ft6,v0
vfmerge.vfm v15,v23,ft7,v0



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmv.v.f v8,ft0
vfmv.v.f v9,ft1
vfmv.v.f v10,ft2
vfmv.v.f v11,ft3
vfmv.v.f v12,ft4
vfmv.v.f v13,ft5
vfmv.v.f v14,ft6
vfmv.v.f v15,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmv.v.f v8,ft0
vfmv.v.f v9,ft1
vfmv.v.f v10,ft2
vfmv.v.f v11,ft3
vfmv.v.f v12,ft4
vfmv.v.f v13,ft5
vfmv.v.f v14,ft6
vfmv.v.f v15,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmfeqvv_m1:
	m_nop
	li a0, WARMUP
1:
vmfeq.vv v8,v16,v24
vmfeq.vv v9,v17,v25
vmfeq.vv v10,v18,v26
vmfeq.vv v11,v19,v27
vmfeq.vv v12,v20,v28
vmfeq.vv v13,v21,v29
vmfeq.vv v14,v22,v30
vmfeq.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfeq.vv v8,v16,v24
vmfeq.vv v9,v17,v25
vmfeq.vv v10,v18,v26
vmfeq.vv v11,v19,v27
vmfeq.vv v12,v20,v28
vmfeq.vv v13,v21,v29
vmfeq.vv v14,v22,v30
vmfeq.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v9,v17,v25,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v11,v19,v27,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v13,v21,v29,v0.t
vmfeq.vv v14,v22,v30,v0.t
vmfeq.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v9,v17,v25,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v11,v19,v27,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v13,v21,v29,v0.t
vmfeq.vv v14,v22,v30,v0.t
vmfeq.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvf_m1:
	m_nop
	li a0, WARMUP
1:
vmfeq.vf v8,v16,ft0
vmfeq.vf v9,v17,ft1
vmfeq.vf v10,v18,ft2
vmfeq.vf v11,v19,ft3
vmfeq.vf v12,v20,ft4
vmfeq.vf v13,v21,ft5
vmfeq.vf v14,v22,ft6
vmfeq.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfeq.vf v8,v16,ft0
vmfeq.vf v9,v17,ft1
vmfeq.vf v10,v18,ft2
vmfeq.vf v11,v19,ft3
vmfeq.vf v12,v20,ft4
vmfeq.vf v13,v21,ft5
vmfeq.vf v14,v22,ft6
vmfeq.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvfm_m1:
	m_nop
	li a0, WARMUP
1:
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v9,v17,ft1,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v11,v19,ft3,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v13,v21,ft5,v0.t
vmfeq.vf v14,v22,ft6,v0.t
vmfeq.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v9,v17,ft1,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v11,v19,ft3,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v13,v21,ft5,v0.t
vmfeq.vf v14,v22,ft6,v0.t
vmfeq.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevv_m1:
	m_nop
	li a0, WARMUP
1:
vmfle.vv v8,v16,v24
vmfle.vv v9,v17,v25
vmfle.vv v10,v18,v26
vmfle.vv v11,v19,v27
vmfle.vv v12,v20,v28
vmfle.vv v13,v21,v29
vmfle.vv v14,v22,v30
vmfle.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfle.vv v8,v16,v24
vmfle.vv v9,v17,v25
vmfle.vv v10,v18,v26
vmfle.vv v11,v19,v27
vmfle.vv v12,v20,v28
vmfle.vv v13,v21,v29
vmfle.vv v14,v22,v30
vmfle.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v9,v17,v25,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v11,v19,v27,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v13,v21,v29,v0.t
vmfle.vv v14,v22,v30,v0.t
vmfle.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v9,v17,v25,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v11,v19,v27,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v13,v21,v29,v0.t
vmfle.vv v14,v22,v30,v0.t
vmfle.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevf_m1:
	m_nop
	li a0, WARMUP
1:
vmfle.vf v8,v16,ft0
vmfle.vf v9,v17,ft1
vmfle.vf v10,v18,ft2
vmfle.vf v11,v19,ft3
vmfle.vf v12,v20,ft4
vmfle.vf v13,v21,ft5
vmfle.vf v14,v22,ft6
vmfle.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfle.vf v8,v16,ft0
vmfle.vf v9,v17,ft1
vmfle.vf v10,v18,ft2
vmfle.vf v11,v19,ft3
vmfle.vf v12,v20,ft4
vmfle.vf v13,v21,ft5
vmfle.vf v14,v22,ft6
vmfle.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevfm_m1:
	m_nop
	li a0, WARMUP
1:
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v9,v17,ft1,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v11,v19,ft3,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v13,v21,ft5,v0.t
vmfle.vf v14,v22,ft6,v0.t
vmfle.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v9,v17,ft1,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v11,v19,ft3,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v13,v21,ft5,v0.t
vmfle.vf v14,v22,ft6,v0.t
vmfle.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvv_m1:
	m_nop
	li a0, WARMUP
1:
vmflt.vv v8,v16,v24
vmflt.vv v9,v17,v25
vmflt.vv v10,v18,v26
vmflt.vv v11,v19,v27
vmflt.vv v12,v20,v28
vmflt.vv v13,v21,v29
vmflt.vv v14,v22,v30
vmflt.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmflt.vv v8,v16,v24
vmflt.vv v9,v17,v25
vmflt.vv v10,v18,v26
vmflt.vv v11,v19,v27
vmflt.vv v12,v20,v28
vmflt.vv v13,v21,v29
vmflt.vv v14,v22,v30
vmflt.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v9,v17,v25,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v11,v19,v27,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v13,v21,v29,v0.t
vmflt.vv v14,v22,v30,v0.t
vmflt.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v9,v17,v25,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v11,v19,v27,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v13,v21,v29,v0.t
vmflt.vv v14,v22,v30,v0.t
vmflt.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvf_m1:
	m_nop
	li a0, WARMUP
1:
vmflt.vf v8,v16,ft0
vmflt.vf v9,v17,ft1
vmflt.vf v10,v18,ft2
vmflt.vf v11,v19,ft3
vmflt.vf v12,v20,ft4
vmflt.vf v13,v21,ft5
vmflt.vf v14,v22,ft6
vmflt.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmflt.vf v8,v16,ft0
vmflt.vf v9,v17,ft1
vmflt.vf v10,v18,ft2
vmflt.vf v11,v19,ft3
vmflt.vf v12,v20,ft4
vmflt.vf v13,v21,ft5
vmflt.vf v14,v22,ft6
vmflt.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvfm_m1:
	m_nop
	li a0, WARMUP
1:
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v9,v17,ft1,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v11,v19,ft3,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v13,v21,ft5,v0.t
vmflt.vf v14,v22,ft6,v0.t
vmflt.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v9,v17,ft1,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v11,v19,ft3,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v13,v21,ft5,v0.t
vmflt.vf v14,v22,ft6,v0.t
vmflt.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevv_m1:
	m_nop
	li a0, WARMUP
1:
vmfne.vv v8,v16,v24
vmfne.vv v9,v17,v25
vmfne.vv v10,v18,v26
vmfne.vv v11,v19,v27
vmfne.vv v12,v20,v28
vmfne.vv v13,v21,v29
vmfne.vv v14,v22,v30
vmfne.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfne.vv v8,v16,v24
vmfne.vv v9,v17,v25
vmfne.vv v10,v18,v26
vmfne.vv v11,v19,v27
vmfne.vv v12,v20,v28
vmfne.vv v13,v21,v29
vmfne.vv v14,v22,v30
vmfne.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v9,v17,v25,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v11,v19,v27,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v13,v21,v29,v0.t
vmfne.vv v14,v22,v30,v0.t
vmfne.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v9,v17,v25,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v11,v19,v27,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v13,v21,v29,v0.t
vmfne.vv v14,v22,v30,v0.t
vmfne.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevf_m1:
	m_nop
	li a0, WARMUP
1:
vmfne.vf v8,v16,ft0
vmfne.vf v9,v17,ft1
vmfne.vf v10,v18,ft2
vmfne.vf v11,v19,ft3
vmfne.vf v12,v20,ft4
vmfne.vf v13,v21,ft5
vmfne.vf v14,v22,ft6
vmfne.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfne.vf v8,v16,ft0
vmfne.vf v9,v17,ft1
vmfne.vf v10,v18,ft2
vmfne.vf v11,v19,ft3
vmfne.vf v12,v20,ft4
vmfne.vf v13,v21,ft5
vmfne.vf v14,v22,ft6
vmfne.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevfm_m1:
	m_nop
	li a0, WARMUP
1:
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v9,v17,ft1,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v11,v19,ft3,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v13,v21,ft5,v0.t
vmfne.vf v14,v22,ft6,v0.t
vmfne.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v9,v17,ft1,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v11,v19,ft3,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v13,v21,ft5,v0.t
vmfne.vf v14,v22,ft6,v0.t
vmfne.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvv_m1:
	m_nop
	li a0, WARMUP
1:
vmfgt.vv v8,v16,v24
vmfgt.vv v9,v17,v25
vmfgt.vv v10,v18,v26
vmfgt.vv v11,v19,v27
vmfgt.vv v12,v20,v28
vmfgt.vv v13,v21,v29
vmfgt.vv v14,v22,v30
vmfgt.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfgt.vv v8,v16,v24
vmfgt.vv v9,v17,v25
vmfgt.vv v10,v18,v26
vmfgt.vv v11,v19,v27
vmfgt.vv v12,v20,v28
vmfgt.vv v13,v21,v29
vmfgt.vv v14,v22,v30
vmfgt.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvvm_m1:
	m_nop
	li a0, WARMUP
1:
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v9,v17,v25,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v11,v19,v27,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v13,v21,v29,v0.t
vmfgt.vv v14,v22,v30,v0.t
vmfgt.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v9,v17,v25,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v11,v19,v27,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v13,v21,v29,v0.t
vmfgt.vv v14,v22,v30,v0.t
vmfgt.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvf_m1:
	m_nop
	li a0, WARMUP
1:
vmfgt.vf v8,v16,ft0
vmfgt.vf v9,v17,ft1
vmfgt.vf v10,v18,ft2
vmfgt.vf v11,v19,ft3
vmfgt.vf v12,v20,ft4
vmfgt.vf v13,v21,ft5
vmfgt.vf v14,v22,ft6
vmfgt.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfgt.vf v8,v16,ft0
vmfgt.vf v9,v17,ft1
vmfgt.vf v10,v18,ft2
vmfgt.vf v11,v19,ft3
vmfgt.vf v12,v20,ft4
vmfgt.vf v13,v21,ft5
vmfgt.vf v14,v22,ft6
vmfgt.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvfm_m1:
	m_nop
	li a0, WARMUP
1:
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v9,v17,ft1,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v11,v19,ft3,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v13,v21,ft5,v0.t
vmfgt.vf v14,v22,ft6,v0.t
vmfgt.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v9,v17,ft1,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v11,v19,ft3,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v13,v21,ft5,v0.t
vmfgt.vf v14,v22,ft6,v0.t
vmfgt.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevv_m1:
	m_nop
	li a0, WARMUP
1:
vmfge.vv v8,v16,v24
vmfge.vv v9,v17,v25
vmfge.vv v10,v18,v26
vmfge.vv v11,v19,v27
vmfge.vv v12,v20,v28
vmfge.vv v13,v21,v29
vmfge.vv v14,v22,v30
vmfge.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfge.vv v8,v16,v24
vmfge.vv v9,v17,v25
vmfge.vv v10,v18,v26
vmfge.vv v11,v19,v27
vmfge.vv v12,v20,v28
vmfge.vv v13,v21,v29
vmfge.vv v14,v22,v30
vmfge.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevvm_m1:
	m_nop
	li a0, WARMUP
1:
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v9,v17,v25,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v11,v19,v27,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v13,v21,v29,v0.t
vmfge.vv v14,v22,v30,v0.t
vmfge.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v9,v17,v25,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v11,v19,v27,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v13,v21,v29,v0.t
vmfge.vv v14,v22,v30,v0.t
vmfge.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevf_m1:
	m_nop
	li a0, WARMUP
1:
vmfge.vf v8,v16,ft0
vmfge.vf v9,v17,ft1
vmfge.vf v10,v18,ft2
vmfge.vf v11,v19,ft3
vmfge.vf v12,v20,ft4
vmfge.vf v13,v21,ft5
vmfge.vf v14,v22,ft6
vmfge.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfge.vf v8,v16,ft0
vmfge.vf v9,v17,ft1
vmfge.vf v10,v18,ft2
vmfge.vf v11,v19,ft3
vmfge.vf v12,v20,ft4
vmfge.vf v13,v21,ft5
vmfge.vf v14,v22,ft6
vmfge.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevfm_m1:
	m_nop
	li a0, WARMUP
1:
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v9,v17,ft1,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v11,v19,ft3,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v13,v21,ft5,v0.t
vmfge.vf v14,v22,ft6,v0.t
vmfge.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v9,v17,ft1,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v11,v19,ft3,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v13,v21,ft5,v0.t
vmfge.vf v14,v22,ft6,v0.t
vmfge.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfdivvv_m1:
	m_nop
	li a0, WARMUP
1:
vfdiv.vv v8,v16,v24
vfdiv.vv v9,v17,v25
vfdiv.vv v10,v18,v26
vfdiv.vv v11,v19,v27
vfdiv.vv v12,v20,v28
vfdiv.vv v13,v21,v29
vfdiv.vv v14,v22,v30
vfdiv.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfdiv.vv v8,v16,v24
vfdiv.vv v9,v17,v25
vfdiv.vv v10,v18,v26
vfdiv.vv v11,v19,v27
vfdiv.vv v12,v20,v28
vfdiv.vv v13,v21,v29
vfdiv.vv v14,v22,v30
vfdiv.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v9,v17,v25,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v11,v19,v27,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v13,v21,v29,v0.t
vfdiv.vv v14,v22,v30,v0.t
vfdiv.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v9,v17,v25,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v11,v19,v27,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v13,v21,v29,v0.t
vfdiv.vv v14,v22,v30,v0.t
vfdiv.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvf_m1:
	m_nop
	li a0, WARMUP
1:
vfdiv.vf v8,v16,ft0
vfdiv.vf v9,v17,ft1
vfdiv.vf v10,v18,ft2
vfdiv.vf v11,v19,ft3
vfdiv.vf v12,v20,ft4
vfdiv.vf v13,v21,ft5
vfdiv.vf v14,v22,ft6
vfdiv.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfdiv.vf v8,v16,ft0
vfdiv.vf v9,v17,ft1
vfdiv.vf v10,v18,ft2
vfdiv.vf v11,v19,ft3
vfdiv.vf v12,v20,ft4
vfdiv.vf v13,v21,ft5
vfdiv.vf v14,v22,ft6
vfdiv.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v9,v17,ft1,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v11,v19,ft3,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v13,v21,ft5,v0.t
vfdiv.vf v14,v22,ft6,v0.t
vfdiv.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v9,v17,ft1,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v11,v19,ft3,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v13,v21,ft5,v0.t
vfdiv.vf v14,v22,ft6,v0.t
vfdiv.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvf_m1:
	m_nop
	li a0, WARMUP
1:
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v9,v17,ft1
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v11,v19,ft3
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v13,v21,ft5
vfrdiv.vf v14,v22,ft6
vfrdiv.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v9,v17,ft1
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v11,v19,ft3
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v13,v21,ft5
vfrdiv.vf v14,v22,ft6
vfrdiv.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v9,v17,ft1,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v11,v19,ft3,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v13,v21,ft5,v0.t
vfrdiv.vf v14,v22,ft6,v0.t
vfrdiv.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v9,v17,ft1,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v11,v19,ft3,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v13,v21,ft5,v0.t
vfrdiv.vf v14,v22,ft6,v0.t
vfrdiv.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmul.vv v8,v16,v24
vfmul.vv v9,v17,v25
vfmul.vv v10,v18,v26
vfmul.vv v11,v19,v27
vfmul.vv v12,v20,v28
vfmul.vv v13,v21,v29
vfmul.vv v14,v22,v30
vfmul.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmul.vv v8,v16,v24
vfmul.vv v9,v17,v25
vfmul.vv v10,v18,v26
vfmul.vv v11,v19,v27
vfmul.vv v12,v20,v28
vfmul.vv v13,v21,v29
vfmul.vv v14,v22,v30
vfmul.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v9,v17,v25,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v11,v19,v27,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v13,v21,v29,v0.t
vfmul.vv v14,v22,v30,v0.t
vfmul.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v9,v17,v25,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v11,v19,v27,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v13,v21,v29,v0.t
vfmul.vv v14,v22,v30,v0.t
vfmul.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmul.vf v8,v16,ft0
vfmul.vf v9,v17,ft1
vfmul.vf v10,v18,ft2
vfmul.vf v11,v19,ft3
vfmul.vf v12,v20,ft4
vfmul.vf v13,v21,ft5
vfmul.vf v14,v22,ft6
vfmul.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmul.vf v8,v16,ft0
vfmul.vf v9,v17,ft1
vfmul.vf v10,v18,ft2
vfmul.vf v11,v19,ft3
vfmul.vf v12,v20,ft4
vfmul.vf v13,v21,ft5
vfmul.vf v14,v22,ft6
vfmul.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v9,v17,ft1,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v11,v19,ft3,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v13,v21,ft5,v0.t
vfmul.vf v14,v22,ft6,v0.t
vfmul.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v9,v17,ft1,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v11,v19,ft3,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v13,v21,ft5,v0.t
vfmul.vf v14,v22,ft6,v0.t
vfmul.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvf_m1:
	m_nop
	li a0, WARMUP
1:
vfrsub.vf v8,v16,ft0
vfrsub.vf v9,v17,ft1
vfrsub.vf v10,v18,ft2
vfrsub.vf v11,v19,ft3
vfrsub.vf v12,v20,ft4
vfrsub.vf v13,v21,ft5
vfrsub.vf v14,v22,ft6
vfrsub.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfrsub.vf v8,v16,ft0
vfrsub.vf v9,v17,ft1
vfrsub.vf v10,v18,ft2
vfrsub.vf v11,v19,ft3
vfrsub.vf v12,v20,ft4
vfrsub.vf v13,v21,ft5
vfrsub.vf v14,v22,ft6
vfrsub.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v9,v17,ft1,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v11,v19,ft3,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v13,v21,ft5,v0.t
vfrsub.vf v14,v22,ft6,v0.t
vfrsub.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v9,v17,ft1,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v11,v19,ft3,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v13,v21,ft5,v0.t
vfrsub.vf v14,v22,ft6,v0.t
vfrsub.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmadd.vv v8,v16,v24
vfmadd.vv v9,v17,v25
vfmadd.vv v10,v18,v26
vfmadd.vv v11,v19,v27
vfmadd.vv v12,v20,v28
vfmadd.vv v13,v21,v29
vfmadd.vv v14,v22,v30
vfmadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmadd.vv v8,v16,v24
vfmadd.vv v9,v17,v25
vfmadd.vv v10,v18,v26
vfmadd.vv v11,v19,v27
vfmadd.vv v12,v20,v28
vfmadd.vv v13,v21,v29
vfmadd.vv v14,v22,v30
vfmadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v9,v17,v25,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v11,v19,v27,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v13,v21,v29,v0.t
vfmadd.vv v14,v22,v30,v0.t
vfmadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v9,v17,v25,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v11,v19,v27,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v13,v21,v29,v0.t
vfmadd.vv v14,v22,v30,v0.t
vfmadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmadd.vf v8,ft0,v16
vfmadd.vf v9,ft1,v17
vfmadd.vf v10,ft2,v18
vfmadd.vf v11,ft3,v19
vfmadd.vf v12,ft4,v20
vfmadd.vf v13,ft5,v21
vfmadd.vf v14,ft6,v22
vfmadd.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmadd.vf v8,ft0,v16
vfmadd.vf v9,ft1,v17
vfmadd.vf v10,ft2,v18
vfmadd.vf v11,ft3,v19
vfmadd.vf v12,ft4,v20
vfmadd.vf v13,ft5,v21
vfmadd.vf v14,ft6,v22
vfmadd.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v9,ft1,v17,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v11,ft3,v19,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v13,ft5,v21,v0.t
vfmadd.vf v14,ft6,v22,v0.t
vfmadd.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v9,ft1,v17,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v11,ft3,v19,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v13,ft5,v21,v0.t
vfmadd.vf v14,ft6,v22,v0.t
vfmadd.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmsub.vv v8,v16,v24
vfmsub.vv v9,v17,v25
vfmsub.vv v10,v18,v26
vfmsub.vv v11,v19,v27
vfmsub.vv v12,v20,v28
vfmsub.vv v13,v21,v29
vfmsub.vv v14,v22,v30
vfmsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsub.vv v8,v16,v24
vfmsub.vv v9,v17,v25
vfmsub.vv v10,v18,v26
vfmsub.vv v11,v19,v27
vfmsub.vv v12,v20,v28
vfmsub.vv v13,v21,v29
vfmsub.vv v14,v22,v30
vfmsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v9,v17,v25,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v11,v19,v27,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v13,v21,v29,v0.t
vfmsub.vv v14,v22,v30,v0.t
vfmsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v9,v17,v25,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v11,v19,v27,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v13,v21,v29,v0.t
vfmsub.vv v14,v22,v30,v0.t
vfmsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmsub.vf v8,ft0,v16
vfmsub.vf v9,ft1,v17
vfmsub.vf v10,ft2,v18
vfmsub.vf v11,ft3,v19
vfmsub.vf v12,ft4,v20
vfmsub.vf v13,ft5,v21
vfmsub.vf v14,ft6,v22
vfmsub.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsub.vf v8,ft0,v16
vfmsub.vf v9,ft1,v17
vfmsub.vf v10,ft2,v18
vfmsub.vf v11,ft3,v19
vfmsub.vf v12,ft4,v20
vfmsub.vf v13,ft5,v21
vfmsub.vf v14,ft6,v22
vfmsub.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v9,ft1,v17,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v11,ft3,v19,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v13,ft5,v21,v0.t
vfmsub.vf v14,ft6,v22,v0.t
vfmsub.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v9,ft1,v17,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v11,ft3,v19,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v13,ft5,v21,v0.t
vfmsub.vf v14,ft6,v22,v0.t
vfmsub.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmacc.vv v8,v16,v24
vfmacc.vv v9,v17,v25
vfmacc.vv v10,v18,v26
vfmacc.vv v11,v19,v27
vfmacc.vv v12,v20,v28
vfmacc.vv v13,v21,v29
vfmacc.vv v14,v22,v30
vfmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmacc.vv v8,v16,v24
vfmacc.vv v9,v17,v25
vfmacc.vv v10,v18,v26
vfmacc.vv v11,v19,v27
vfmacc.vv v12,v20,v28
vfmacc.vv v13,v21,v29
vfmacc.vv v14,v22,v30
vfmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v9,v17,v25,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v11,v19,v27,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v13,v21,v29,v0.t
vfmacc.vv v14,v22,v30,v0.t
vfmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v9,v17,v25,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v11,v19,v27,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v13,v21,v29,v0.t
vfmacc.vv v14,v22,v30,v0.t
vfmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmacc.vf v8,ft0,v16
vfmacc.vf v9,ft1,v17
vfmacc.vf v10,ft2,v18
vfmacc.vf v11,ft3,v19
vfmacc.vf v12,ft4,v20
vfmacc.vf v13,ft5,v21
vfmacc.vf v14,ft6,v22
vfmacc.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmacc.vf v8,ft0,v16
vfmacc.vf v9,ft1,v17
vfmacc.vf v10,ft2,v18
vfmacc.vf v11,ft3,v19
vfmacc.vf v12,ft4,v20
vfmacc.vf v13,ft5,v21
vfmacc.vf v14,ft6,v22
vfmacc.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v9,ft1,v17,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v11,ft3,v19,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v13,ft5,v21,v0.t
vfmacc.vf v14,ft6,v22,v0.t
vfmacc.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v9,ft1,v17,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v11,ft3,v19,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v13,ft5,v21,v0.t
vfmacc.vf v14,ft6,v22,v0.t
vfmacc.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvv_m1:
	m_nop
	li a0, WARMUP
1:
vfmsac.vv v8,v16,v24
vfmsac.vv v9,v17,v25
vfmsac.vv v10,v18,v26
vfmsac.vv v11,v19,v27
vfmsac.vv v12,v20,v28
vfmsac.vv v13,v21,v29
vfmsac.vv v14,v22,v30
vfmsac.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsac.vv v8,v16,v24
vfmsac.vv v9,v17,v25
vfmsac.vv v10,v18,v26
vfmsac.vv v11,v19,v27
vfmsac.vv v12,v20,v28
vfmsac.vv v13,v21,v29
vfmsac.vv v14,v22,v30
vfmsac.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v9,v17,v25,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v11,v19,v27,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v13,v21,v29,v0.t
vfmsac.vv v14,v22,v30,v0.t
vfmsac.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v9,v17,v25,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v11,v19,v27,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v13,v21,v29,v0.t
vfmsac.vv v14,v22,v30,v0.t
vfmsac.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvf_m1:
	m_nop
	li a0, WARMUP
1:
vfmsac.vf v8,ft0,v16
vfmsac.vf v9,ft1,v17
vfmsac.vf v10,ft2,v18
vfmsac.vf v11,ft3,v19
vfmsac.vf v12,ft4,v20
vfmsac.vf v13,ft5,v21
vfmsac.vf v14,ft6,v22
vfmsac.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsac.vf v8,ft0,v16
vfmsac.vf v9,ft1,v17
vfmsac.vf v10,ft2,v18
vfmsac.vf v11,ft3,v19
vfmsac.vf v12,ft4,v20
vfmsac.vf v13,ft5,v21
vfmsac.vf v14,ft6,v22
vfmsac.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v9,ft1,v17,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v11,ft3,v19,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v13,ft5,v21,v0.t
vfmsac.vf v14,ft6,v22,v0.t
vfmsac.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v9,ft1,v17,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v11,ft3,v19,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v13,ft5,v21,v0.t
vfmsac.vf v14,ft6,v22,v0.t
vfmsac.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfnmsacvv_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsac.vv v8,v16,v24
vfnmsac.vv v9,v17,v25
vfnmsac.vv v10,v18,v26
vfnmsac.vv v11,v19,v27
vfnmsac.vv v12,v20,v28
vfnmsac.vv v13,v21,v29
vfnmsac.vv v14,v22,v30
vfnmsac.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsac.vv v8,v16,v24
vfnmsac.vv v9,v17,v25
vfnmsac.vv v10,v18,v26
vfnmsac.vv v11,v19,v27
vfnmsac.vv v12,v20,v28
vfnmsac.vv v13,v21,v29
vfnmsac.vv v14,v22,v30
vfnmsac.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v9,v17,v25,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v11,v19,v27,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v13,v21,v29,v0.t
vfnmsac.vv v14,v22,v30,v0.t
vfnmsac.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v9,v17,v25,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v11,v19,v27,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v13,v21,v29,v0.t
vfnmsac.vv v14,v22,v30,v0.t
vfnmsac.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvf_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v9,ft1,v17
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v11,ft3,v19
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v13,ft5,v21
vfnmsac.vf v14,ft6,v22
vfnmsac.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v9,ft1,v17
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v11,ft3,v19
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v13,ft5,v21
vfnmsac.vf v14,ft6,v22
vfnmsac.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v9,ft1,v17,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v11,ft3,v19,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v13,ft5,v21,v0.t
vfnmsac.vf v14,ft6,v22,v0.t
vfnmsac.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v9,ft1,v17,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v11,ft3,v19,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v13,ft5,v21,v0.t
vfnmsac.vf v14,ft6,v22,v0.t
vfnmsac.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vfnmacc.vv v8,v16,v24
vfnmacc.vv v9,v17,v25
vfnmacc.vv v10,v18,v26
vfnmacc.vv v11,v19,v27
vfnmacc.vv v12,v20,v28
vfnmacc.vv v13,v21,v29
vfnmacc.vv v14,v22,v30
vfnmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmacc.vv v8,v16,v24
vfnmacc.vv v9,v17,v25
vfnmacc.vv v10,v18,v26
vfnmacc.vv v11,v19,v27
vfnmacc.vv v12,v20,v28
vfnmacc.vv v13,v21,v29
vfnmacc.vv v14,v22,v30
vfnmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v9,v17,v25,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v11,v19,v27,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v13,v21,v29,v0.t
vfnmacc.vv v14,v22,v30,v0.t
vfnmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v9,v17,v25,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v11,v19,v27,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v13,v21,v29,v0.t
vfnmacc.vv v14,v22,v30,v0.t
vfnmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvf_m1:
	m_nop
	li a0, WARMUP
1:
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v9,ft1,v17
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v11,ft3,v19
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v13,ft5,v21
vfnmacc.vf v14,ft6,v22
vfnmacc.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v9,ft1,v17
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v11,ft3,v19
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v13,ft5,v21
vfnmacc.vf v14,ft6,v22
vfnmacc.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v9,ft1,v17,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v11,ft3,v19,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v13,ft5,v21,v0.t
vfnmacc.vf v14,ft6,v22,v0.t
vfnmacc.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v9,ft1,v17,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v11,ft3,v19,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v13,ft5,v21,v0.t
vfnmacc.vf v14,ft6,v22,v0.t
vfnmacc.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsub.vv v8,v16,v24
vfnmsub.vv v9,v17,v25
vfnmsub.vv v10,v18,v26
vfnmsub.vv v11,v19,v27
vfnmsub.vv v12,v20,v28
vfnmsub.vv v13,v21,v29
vfnmsub.vv v14,v22,v30
vfnmsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsub.vv v8,v16,v24
vfnmsub.vv v9,v17,v25
vfnmsub.vv v10,v18,v26
vfnmsub.vv v11,v19,v27
vfnmsub.vv v12,v20,v28
vfnmsub.vv v13,v21,v29
vfnmsub.vv v14,v22,v30
vfnmsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v9,v17,v25,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v11,v19,v27,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v13,v21,v29,v0.t
vfnmsub.vv v14,v22,v30,v0.t
vfnmsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v9,v17,v25,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v11,v19,v27,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v13,v21,v29,v0.t
vfnmsub.vv v14,v22,v30,v0.t
vfnmsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvf_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v9,ft1,v17
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v11,ft3,v19
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v13,ft5,v21
vfnmsub.vf v14,ft6,v22
vfnmsub.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v9,ft1,v17
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v11,ft3,v19
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v13,ft5,v21
vfnmsub.vf v14,ft6,v22
vfnmsub.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v9,ft1,v17,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v11,ft3,v19,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v13,ft5,v21,v0.t
vfnmsub.vf v14,ft6,v22,v0.t
vfnmsub.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v9,ft1,v17,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v11,ft3,v19,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v13,ft5,v21,v0.t
vfnmsub.vf v14,ft6,v22,v0.t
vfnmsub.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vfnmadd.vv v8,v16,v24
vfnmadd.vv v9,v17,v25
vfnmadd.vv v10,v18,v26
vfnmadd.vv v11,v19,v27
vfnmadd.vv v12,v20,v28
vfnmadd.vv v13,v21,v29
vfnmadd.vv v14,v22,v30
vfnmadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmadd.vv v8,v16,v24
vfnmadd.vv v9,v17,v25
vfnmadd.vv v10,v18,v26
vfnmadd.vv v11,v19,v27
vfnmadd.vv v12,v20,v28
vfnmadd.vv v13,v21,v29
vfnmadd.vv v14,v22,v30
vfnmadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v9,v17,v25,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v11,v19,v27,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v13,v21,v29,v0.t
vfnmadd.vv v14,v22,v30,v0.t
vfnmadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v9,v17,v25,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v11,v19,v27,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v13,v21,v29,v0.t
vfnmadd.vv v14,v22,v30,v0.t
vfnmadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvf_m1:
	m_nop
	li a0, WARMUP
1:
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v9,ft1,v17
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v11,ft3,v19
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v13,ft5,v21
vfnmadd.vf v14,ft6,v22
vfnmadd.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v9,ft1,v17
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v11,ft3,v19
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v13,ft5,v21
vfnmadd.vf v14,ft6,v22
vfnmadd.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v9,ft1,v17,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v11,ft3,v19,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v13,ft5,v21,v0.t
vfnmadd.vf v14,ft6,v22,v0.t
vfnmadd.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v9,ft1,v17,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v11,ft3,v19,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v13,ft5,v21,v0.t
vfnmadd.vf v14,ft6,v22,v0.t
vfnmadd.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwredsumuvs_m1:
	m_nop
	li a0, WARMUP
1:
vwredsumu.vs v8,v16,v24
vwredsumu.vs v9,v17,v25
vwredsumu.vs v10,v18,v26
vwredsumu.vs v11,v19,v27
vwredsumu.vs v12,v20,v28
vwredsumu.vs v13,v21,v29
vwredsumu.vs v14,v22,v30
vwredsumu.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwredsumu.vs v8,v16,v24
vwredsumu.vs v9,v17,v25
vwredsumu.vs v10,v18,v26
vwredsumu.vs v11,v19,v27
vwredsumu.vs v12,v20,v28
vwredsumu.vs v13,v21,v29
vwredsumu.vs v14,v22,v30
vwredsumu.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumuvsm_m1:
	m_nop
	li a0, WARMUP
1:
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v9,v17,v25,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v11,v19,v27,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v13,v21,v29,v0.t
vwredsumu.vs v14,v22,v30,v0.t
vwredsumu.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v9,v17,v25,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v11,v19,v27,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v13,v21,v29,v0.t
vwredsumu.vs v14,v22,v30,v0.t
vwredsumu.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvs_m1:
	m_nop
	li a0, WARMUP
1:
vwredsum.vs v8,v16,v24
vwredsum.vs v9,v17,v25
vwredsum.vs v10,v18,v26
vwredsum.vs v11,v19,v27
vwredsum.vs v12,v20,v28
vwredsum.vs v13,v21,v29
vwredsum.vs v14,v22,v30
vwredsum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwredsum.vs v8,v16,v24
vwredsum.vs v9,v17,v25
vwredsum.vs v10,v18,v26
vwredsum.vs v11,v19,v27
vwredsum.vs v12,v20,v28
vwredsum.vs v13,v21,v29
vwredsum.vs v14,v22,v30
vwredsum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v9,v17,v25,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v11,v19,v27,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v13,v21,v29,v0.t
vwredsum.vs v14,v22,v30,v0.t
vwredsum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v9,v17,v25,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v11,v19,v27,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v13,v21,v29,v0.t
vwredsum.vs v14,v22,v30,v0.t
vwredsum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwaddvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.vv v8,v16,v24
vfwadd.vv v9,v17,v25
vfwadd.vv v10,v18,v26
vfwadd.vv v11,v19,v27
vfwadd.vv v12,v20,v28
vfwadd.vv v13,v21,v29
vfwadd.vv v14,v22,v30
vfwadd.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.vv v8,v16,v24
vfwadd.vv v9,v17,v25
vfwadd.vv v10,v18,v26
vfwadd.vv v11,v19,v27
vfwadd.vv v12,v20,v28
vfwadd.vv v13,v21,v29
vfwadd.vv v14,v22,v30
vfwadd.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v9,v17,v25,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v11,v19,v27,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v13,v21,v29,v0.t
vfwadd.vv v14,v22,v30,v0.t
vfwadd.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v9,v17,v25,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v11,v19,v27,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v13,v21,v29,v0.t
vfwadd.vv v14,v22,v30,v0.t
vfwadd.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.vf v8,v16,ft0
vfwadd.vf v9,v17,ft1
vfwadd.vf v10,v18,ft2
vfwadd.vf v11,v19,ft3
vfwadd.vf v12,v20,ft4
vfwadd.vf v13,v21,ft5
vfwadd.vf v14,v22,ft6
vfwadd.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.vf v8,v16,ft0
vfwadd.vf v9,v17,ft1
vfwadd.vf v10,v18,ft2
vfwadd.vf v11,v19,ft3
vfwadd.vf v12,v20,ft4
vfwadd.vf v13,v21,ft5
vfwadd.vf v14,v22,ft6
vfwadd.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v9,v17,ft1,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v11,v19,ft3,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v13,v21,ft5,v0.t
vfwadd.vf v14,v22,ft6,v0.t
vfwadd.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v9,v17,ft1,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v11,v19,ft3,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v13,v21,ft5,v0.t
vfwadd.vf v14,v22,ft6,v0.t
vfwadd.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.vv v8,v16,v24
vfwsub.vv v9,v17,v25
vfwsub.vv v10,v18,v26
vfwsub.vv v11,v19,v27
vfwsub.vv v12,v20,v28
vfwsub.vv v13,v21,v29
vfwsub.vv v14,v22,v30
vfwsub.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.vv v8,v16,v24
vfwsub.vv v9,v17,v25
vfwsub.vv v10,v18,v26
vfwsub.vv v11,v19,v27
vfwsub.vv v12,v20,v28
vfwsub.vv v13,v21,v29
vfwsub.vv v14,v22,v30
vfwsub.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v9,v17,v25,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v11,v19,v27,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v13,v21,v29,v0.t
vfwsub.vv v14,v22,v30,v0.t
vfwsub.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v9,v17,v25,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v11,v19,v27,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v13,v21,v29,v0.t
vfwsub.vv v14,v22,v30,v0.t
vfwsub.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.vf v8,v16,ft0
vfwsub.vf v9,v17,ft1
vfwsub.vf v10,v18,ft2
vfwsub.vf v11,v19,ft3
vfwsub.vf v12,v20,ft4
vfwsub.vf v13,v21,ft5
vfwsub.vf v14,v22,ft6
vfwsub.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.vf v8,v16,ft0
vfwsub.vf v9,v17,ft1
vfwsub.vf v10,v18,ft2
vfwsub.vf v11,v19,ft3
vfwsub.vf v12,v20,ft4
vfwsub.vf v13,v21,ft5
vfwsub.vf v14,v22,ft6
vfwsub.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v9,v17,ft1,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v11,v19,ft3,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v13,v21,ft5,v0.t
vfwsub.vf v14,v22,ft6,v0.t
vfwsub.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v9,v17,ft1,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v11,v19,ft3,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v13,v21,ft5,v0.t
vfwsub.vf v14,v22,ft6,v0.t
vfwsub.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwv_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.wv v8,v16,v24
vfwadd.wv v9,v17,v25
vfwadd.wv v10,v18,v26
vfwadd.wv v11,v19,v27
vfwadd.wv v12,v20,v28
vfwadd.wv v13,v21,v29
vfwadd.wv v14,v22,v30
vfwadd.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.wv v8,v16,v24
vfwadd.wv v9,v17,v25
vfwadd.wv v10,v18,v26
vfwadd.wv v11,v19,v27
vfwadd.wv v12,v20,v28
vfwadd.wv v13,v21,v29
vfwadd.wv v14,v22,v30
vfwadd.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v9,v17,v25,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v11,v19,v27,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v13,v21,v29,v0.t
vfwadd.wv v14,v22,v30,v0.t
vfwadd.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v9,v17,v25,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v11,v19,v27,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v13,v21,v29,v0.t
vfwadd.wv v14,v22,v30,v0.t
vfwadd.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwf_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.wf v8,v16,ft0
vfwadd.wf v9,v17,ft1
vfwadd.wf v10,v18,ft2
vfwadd.wf v11,v19,ft3
vfwadd.wf v12,v20,ft4
vfwadd.wf v13,v21,ft5
vfwadd.wf v14,v22,ft6
vfwadd.wf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.wf v8,v16,ft0
vfwadd.wf v9,v17,ft1
vfwadd.wf v10,v18,ft2
vfwadd.wf v11,v19,ft3
vfwadd.wf v12,v20,ft4
vfwadd.wf v13,v21,ft5
vfwadd.wf v14,v22,ft6
vfwadd.wf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v9,v17,ft1,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v11,v19,ft3,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v13,v21,ft5,v0.t
vfwadd.wf v14,v22,ft6,v0.t
vfwadd.wf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v9,v17,ft1,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v11,v19,ft3,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v13,v21,ft5,v0.t
vfwadd.wf v14,v22,ft6,v0.t
vfwadd.wf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwv_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.wv v8,v16,v24
vfwsub.wv v9,v17,v25
vfwsub.wv v10,v18,v26
vfwsub.wv v11,v19,v27
vfwsub.wv v12,v20,v28
vfwsub.wv v13,v21,v29
vfwsub.wv v14,v22,v30
vfwsub.wv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.wv v8,v16,v24
vfwsub.wv v9,v17,v25
vfwsub.wv v10,v18,v26
vfwsub.wv v11,v19,v27
vfwsub.wv v12,v20,v28
vfwsub.wv v13,v21,v29
vfwsub.wv v14,v22,v30
vfwsub.wv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v9,v17,v25,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v11,v19,v27,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v13,v21,v29,v0.t
vfwsub.wv v14,v22,v30,v0.t
vfwsub.wv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v9,v17,v25,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v11,v19,v27,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v13,v21,v29,v0.t
vfwsub.wv v14,v22,v30,v0.t
vfwsub.wv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwf_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.wf v8,v16,ft0
vfwsub.wf v9,v17,ft1
vfwsub.wf v10,v18,ft2
vfwsub.wf v11,v19,ft3
vfwsub.wf v12,v20,ft4
vfwsub.wf v13,v21,ft5
vfwsub.wf v14,v22,ft6
vfwsub.wf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.wf v8,v16,ft0
vfwsub.wf v9,v17,ft1
vfwsub.wf v10,v18,ft2
vfwsub.wf v11,v19,ft3
vfwsub.wf v12,v20,ft4
vfwsub.wf v13,v21,ft5
vfwsub.wf v14,v22,ft6
vfwsub.wf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v9,v17,ft1,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v11,v19,ft3,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v13,v21,ft5,v0.t
vfwsub.wf v14,v22,ft6,v0.t
vfwsub.wf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v9,v17,ft1,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v11,v19,ft3,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v13,v21,ft5,v0.t
vfwsub.wf v14,v22,ft6,v0.t
vfwsub.wf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwmul.vv v8,v16,v24
vfwmul.vv v9,v17,v25
vfwmul.vv v10,v18,v26
vfwmul.vv v11,v19,v27
vfwmul.vv v12,v20,v28
vfwmul.vv v13,v21,v29
vfwmul.vv v14,v22,v30
vfwmul.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmul.vv v8,v16,v24
vfwmul.vv v9,v17,v25
vfwmul.vv v10,v18,v26
vfwmul.vv v11,v19,v27
vfwmul.vv v12,v20,v28
vfwmul.vv v13,v21,v29
vfwmul.vv v14,v22,v30
vfwmul.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v9,v17,v25,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v11,v19,v27,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v13,v21,v29,v0.t
vfwmul.vv v14,v22,v30,v0.t
vfwmul.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v9,v17,v25,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v11,v19,v27,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v13,v21,v29,v0.t
vfwmul.vv v14,v22,v30,v0.t
vfwmul.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwmul.vf v8,v16,ft0
vfwmul.vf v9,v17,ft1
vfwmul.vf v10,v18,ft2
vfwmul.vf v11,v19,ft3
vfwmul.vf v12,v20,ft4
vfwmul.vf v13,v21,ft5
vfwmul.vf v14,v22,ft6
vfwmul.vf v15,v23,ft7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmul.vf v8,v16,ft0
vfwmul.vf v9,v17,ft1
vfwmul.vf v10,v18,ft2
vfwmul.vf v11,v19,ft3
vfwmul.vf v12,v20,ft4
vfwmul.vf v13,v21,ft5
vfwmul.vf v14,v22,ft6
vfwmul.vf v15,v23,ft7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v9,v17,ft1,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v11,v19,ft3,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v13,v21,ft5,v0.t
vfwmul.vf v14,v22,ft6,v0.t
vfwmul.vf v15,v23,ft7,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v9,v17,ft1,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v11,v19,ft3,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v13,v21,ft5,v0.t
vfwmul.vf v14,v22,ft6,v0.t
vfwmul.vf v15,v23,ft7,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwmacc.vv v8,v16,v24
vfwmacc.vv v9,v17,v25
vfwmacc.vv v10,v18,v26
vfwmacc.vv v11,v19,v27
vfwmacc.vv v12,v20,v28
vfwmacc.vv v13,v21,v29
vfwmacc.vv v14,v22,v30
vfwmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmacc.vv v8,v16,v24
vfwmacc.vv v9,v17,v25
vfwmacc.vv v10,v18,v26
vfwmacc.vv v11,v19,v27
vfwmacc.vv v12,v20,v28
vfwmacc.vv v13,v21,v29
vfwmacc.vv v14,v22,v30
vfwmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v9,v17,v25,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v11,v19,v27,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v13,v21,v29,v0.t
vfwmacc.vv v14,v22,v30,v0.t
vfwmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v9,v17,v25,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v11,v19,v27,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v13,v21,v29,v0.t
vfwmacc.vv v14,v22,v30,v0.t
vfwmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v9,ft1,v17
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v11,ft3,v19
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v13,ft5,v21
vfwmacc.vf v14,ft6,v22
vfwmacc.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v9,ft1,v17
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v11,ft3,v19
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v13,ft5,v21
vfwmacc.vf v14,ft6,v22
vfwmacc.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v9,ft1,v17,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v11,ft3,v19,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v13,ft5,v21,v0.t
vfwmacc.vf v14,ft6,v22,v0.t
vfwmacc.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v9,ft1,v17,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v11,ft3,v19,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v13,ft5,v21,v0.t
vfwmacc.vf v14,ft6,v22,v0.t
vfwmacc.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v9,v17,v25
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v11,v19,v27
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v13,v21,v29
vfwnmacc.vv v14,v22,v30
vfwnmacc.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v9,v17,v25
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v11,v19,v27
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v13,v21,v29
vfwnmacc.vv v14,v22,v30
vfwnmacc.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v9,v17,v25,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v11,v19,v27,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v13,v21,v29,v0.t
vfwnmacc.vv v14,v22,v30,v0.t
vfwnmacc.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v9,v17,v25,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v11,v19,v27,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v13,v21,v29,v0.t
vfwnmacc.vv v14,v22,v30,v0.t
vfwnmacc.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v9,ft1,v17
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v11,ft3,v19
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v13,ft5,v21
vfwnmacc.vf v14,ft6,v22
vfwnmacc.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v9,ft1,v17
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v11,ft3,v19
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v13,ft5,v21
vfwnmacc.vf v14,ft6,v22
vfwnmacc.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v9,ft1,v17,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v11,ft3,v19,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v13,ft5,v21,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t
vfwnmacc.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v9,ft1,v17,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v11,ft3,v19,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v13,ft5,v21,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t
vfwnmacc.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwmsac.vv v8,v16,v24
vfwmsac.vv v9,v17,v25
vfwmsac.vv v10,v18,v26
vfwmsac.vv v11,v19,v27
vfwmsac.vv v12,v20,v28
vfwmsac.vv v13,v21,v29
vfwmsac.vv v14,v22,v30
vfwmsac.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmsac.vv v8,v16,v24
vfwmsac.vv v9,v17,v25
vfwmsac.vv v10,v18,v26
vfwmsac.vv v11,v19,v27
vfwmsac.vv v12,v20,v28
vfwmsac.vv v13,v21,v29
vfwmsac.vv v14,v22,v30
vfwmsac.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v9,v17,v25,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v11,v19,v27,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v13,v21,v29,v0.t
vfwmsac.vv v14,v22,v30,v0.t
vfwmsac.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v9,v17,v25,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v11,v19,v27,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v13,v21,v29,v0.t
vfwmsac.vv v14,v22,v30,v0.t
vfwmsac.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v9,ft1,v17
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v11,ft3,v19
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v13,ft5,v21
vfwmsac.vf v14,ft6,v22
vfwmsac.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v9,ft1,v17
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v11,ft3,v19
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v13,ft5,v21
vfwmsac.vf v14,ft6,v22
vfwmsac.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v9,ft1,v17,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v11,ft3,v19,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v13,ft5,v21,v0.t
vfwmsac.vf v14,ft6,v22,v0.t
vfwmsac.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v9,ft1,v17,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v11,ft3,v19,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v13,ft5,v21,v0.t
vfwmsac.vf v14,ft6,v22,v0.t
vfwmsac.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvv_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v9,v17,v25
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v11,v19,v27
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v13,v21,v29
vfwnmsac.vv v14,v22,v30
vfwnmsac.vv v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v9,v17,v25
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v11,v19,v27
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v13,v21,v29
vfwnmsac.vv v14,v22,v30
vfwnmsac.vv v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvvm_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v9,v17,v25,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v11,v19,v27,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v13,v21,v29,v0.t
vfwnmsac.vv v14,v22,v30,v0.t
vfwnmsac.vv v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v9,v17,v25,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v11,v19,v27,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v13,v21,v29,v0.t
vfwnmsac.vv v14,v22,v30,v0.t
vfwnmsac.vv v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvf_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v9,ft1,v17
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v11,ft3,v19
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v13,ft5,v21
vfwnmsac.vf v14,ft6,v22
vfwnmsac.vf v15,ft7,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v9,ft1,v17
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v11,ft3,v19
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v13,ft5,v21
vfwnmsac.vf v14,ft6,v22
vfwnmsac.vf v15,ft7,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvfm_m1:
	m_nop
	li a0, WARMUP
1:
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v9,ft1,v17,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v11,ft3,v19,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v13,ft5,v21,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t
vfwnmsac.vf v15,ft7,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v9,ft1,v17,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v11,ft3,v19,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v13,ft5,v21,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t
vfwnmsac.vf v15,ft7,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwredosumvs_m1:
	m_nop
	li a0, WARMUP
1:
vfwredosum.vs v8,v16,v24
vfwredosum.vs v9,v17,v25
vfwredosum.vs v10,v18,v26
vfwredosum.vs v11,v19,v27
vfwredosum.vs v12,v20,v28
vfwredosum.vs v13,v21,v29
vfwredosum.vs v14,v22,v30
vfwredosum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwredosum.vs v8,v16,v24
vfwredosum.vs v9,v17,v25
vfwredosum.vs v10,v18,v26
vfwredosum.vs v11,v19,v27
vfwredosum.vs v12,v20,v28
vfwredosum.vs v13,v21,v29
vfwredosum.vs v14,v22,v30
vfwredosum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredosumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v9,v17,v25,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v11,v19,v27,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v13,v21,v29,v0.t
vfwredosum.vs v14,v22,v30,v0.t
vfwredosum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v9,v17,v25,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v11,v19,v27,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v13,v21,v29,v0.t
vfwredosum.vs v14,v22,v30,v0.t
vfwredosum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvs_m1:
	m_nop
	li a0, WARMUP
1:
vfwredusum.vs v8,v16,v24
vfwredusum.vs v9,v17,v25
vfwredusum.vs v10,v18,v26
vfwredusum.vs v11,v19,v27
vfwredusum.vs v12,v20,v28
vfwredusum.vs v13,v21,v29
vfwredusum.vs v14,v22,v30
vfwredusum.vs v15,v23,v31



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwredusum.vs v8,v16,v24
vfwredusum.vs v9,v17,v25
vfwredusum.vs v10,v18,v26
vfwredusum.vs v11,v19,v27
vfwredusum.vs v12,v20,v28
vfwredusum.vs v13,v21,v29
vfwredusum.vs v14,v22,v30
vfwredusum.vs v15,v23,v31



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvsm_m1:
	m_nop
	li a0, WARMUP
1:
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v9,v17,v25,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v11,v19,v27,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v13,v21,v29,v0.t
vfwredusum.vs v14,v22,v30,v0.t
vfwredusum.vs v15,v23,v31,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v9,v17,v25,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v11,v19,v27,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v13,v21,v29,v0.t
vfwredusum.vs v14,v22,v30,v0.t
vfwredusum.vs v15,v23,v31,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmvsx_m1:
	m_nop
	li a0, WARMUP
1:
vmv.s.x v8,t0
vmv.s.x v9,t1
vmv.s.x v10,t2
vmv.s.x v11,t3
vmv.s.x v12,t4
vmv.s.x v13,t5
vmv.s.x v14,t6
vmv.s.x v15,t7



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv.s.x v8,t0
vmv.s.x v9,t1
vmv.s.x v10,t2
vmv.s.x v11,t3
vmv.s.x v12,t4
vmv.s.x v13,t5
vmv.s.x v14,t6
vmv.s.x v15,t7



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvxs_m1:
	m_nop
	li a0, WARMUP
1:
vmv.x.s t0,v8
vmv.x.s t1,v9
vmv.x.s t2,v10
vmv.x.s t3,v11
vmv.x.s t4,v12
vmv.x.s t5,v13
vmv.x.s t6,v14
vmv.x.s t7,v15



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmv.x.s t0,v8
vmv.x.s t1,v9
vmv.x.s t2,v10
vmv.x.s t3,v11
vmv.x.s t4,v12
vmv.x.s t5,v13
vmv.x.s t6,v14
vmv.x.s t7,v15



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcpopm_m1:
	m_nop
	li a0, WARMUP
1:
vcpop.m t0,v8
vcpop.m t1,v9
vcpop.m t2,v10
vcpop.m t3,v11
vcpop.m t4,v12
vcpop.m t5,v13
vcpop.m t6,v14
vcpop.m t7,v15



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vcpop.m t0,v8
vcpop.m t1,v9
vcpop.m t2,v10
vcpop.m t3,v11
vcpop.m t4,v12
vcpop.m t5,v13
vcpop.m t6,v14
vcpop.m t7,v15



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vcpopmm_m1:
	m_nop
	li a0, WARMUP
1:
vcpop.m t0,v8,v0.t
vcpop.m t1,v9,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t3,v11,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t5,v13,v0.t
vcpop.m t6,v14,v0.t
vcpop.m t7,v15,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vcpop.m t0,v8,v0.t
vcpop.m t1,v9,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t3,v11,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t5,v13,v0.t
vcpop.m t6,v14,v0.t
vcpop.m t7,v15,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstm_m1:
	m_1bit
	li a0, WARMUP
1:
vfirst.m t0,v8
vfirst.m t1,v9
vfirst.m t2,v10
vfirst.m t3,v11
vfirst.m t4,v12
vfirst.m t5,v13
vfirst.m t6,v14
vfirst.m t7,v15



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfirst.m t0,v8
vfirst.m t1,v9
vfirst.m t2,v10
vfirst.m t3,v11
vfirst.m t4,v12
vfirst.m t5,v13
vfirst.m t6,v14
vfirst.m t7,v15



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstmm_m1:
	m_1bit
	li a0, WARMUP
1:
vfirst.m t0,v8,v0.t
vfirst.m t1,v9,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t3,v11,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t5,v13,v0.t
vfirst.m t6,v14,v0.t
vfirst.m t7,v15,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vfirst.m t0,v8,v0.t
vfirst.m t1,v9,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t3,v11,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t5,v13,v0.t
vfirst.m t6,v14,v0.t
vfirst.m t7,v15,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf2 v8,v16
vzext.vf2 v9,v17
vzext.vf2 v10,v18
vzext.vf2 v11,v19
vzext.vf2 v12,v20
vzext.vf2 v13,v21
vzext.vf2 v14,v22
vzext.vf2 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf2 v8,v16
vzext.vf2 v9,v17
vzext.vf2 v10,v18
vzext.vf2 v11,v19
vzext.vf2 v12,v20
vzext.vf2 v13,v21
vzext.vf2 v14,v22
vzext.vf2 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2m_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf2 v8,v16,v0.t
vzext.vf2 v9,v17,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v11,v19,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v13,v21,v0.t
vzext.vf2 v14,v22,v0.t
vzext.vf2 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf2 v8,v16,v0.t
vzext.vf2 v9,v17,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v11,v19,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v13,v21,v0.t
vzext.vf2 v14,v22,v0.t
vzext.vf2 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf2 v8,v16
vsext.vf2 v9,v17
vsext.vf2 v10,v18
vsext.vf2 v11,v19
vsext.vf2 v12,v20
vsext.vf2 v13,v21
vsext.vf2 v14,v22
vsext.vf2 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf2 v8,v16
vsext.vf2 v9,v17
vsext.vf2 v10,v18
vsext.vf2 v11,v19
vsext.vf2 v12,v20
vsext.vf2 v13,v21
vsext.vf2 v14,v22
vsext.vf2 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2m_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf2 v8,v16,v0.t
vsext.vf2 v9,v17,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v11,v19,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v13,v21,v0.t
vsext.vf2 v14,v22,v0.t
vsext.vf2 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf2 v8,v16,v0.t
vsext.vf2 v9,v17,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v11,v19,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v13,v21,v0.t
vsext.vf2 v14,v22,v0.t
vsext.vf2 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf4 v8,v16
vzext.vf4 v9,v17
vzext.vf4 v10,v18
vzext.vf4 v11,v19
vzext.vf4 v12,v20
vzext.vf4 v13,v21
vzext.vf4 v14,v22
vzext.vf4 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf4 v8,v16
vzext.vf4 v9,v17
vzext.vf4 v10,v18
vzext.vf4 v11,v19
vzext.vf4 v12,v20
vzext.vf4 v13,v21
vzext.vf4 v14,v22
vzext.vf4 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4m_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf4 v8,v16,v0.t
vzext.vf4 v9,v17,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v11,v19,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v13,v21,v0.t
vzext.vf4 v14,v22,v0.t
vzext.vf4 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf4 v8,v16,v0.t
vzext.vf4 v9,v17,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v11,v19,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v13,v21,v0.t
vzext.vf4 v14,v22,v0.t
vzext.vf4 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf4 v8,v16
vsext.vf4 v9,v17
vsext.vf4 v10,v18
vsext.vf4 v11,v19
vsext.vf4 v12,v20
vsext.vf4 v13,v21
vsext.vf4 v14,v22
vsext.vf4 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf4 v8,v16
vsext.vf4 v9,v17
vsext.vf4 v10,v18
vsext.vf4 v11,v19
vsext.vf4 v12,v20
vsext.vf4 v13,v21
vsext.vf4 v14,v22
vsext.vf4 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4m_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf4 v8,v16,v0.t
vsext.vf4 v9,v17,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v11,v19,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v13,v21,v0.t
vsext.vf4 v14,v22,v0.t
vsext.vf4 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf4 v8,v16,v0.t
vsext.vf4 v9,v17,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v11,v19,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v13,v21,v0.t
vsext.vf4 v14,v22,v0.t
vsext.vf4 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf8 v8,v16
vzext.vf8 v9,v17
vzext.vf8 v10,v18
vzext.vf8 v11,v19
vzext.vf8 v12,v20
vzext.vf8 v13,v21
vzext.vf8 v14,v22
vzext.vf8 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf8 v8,v16
vzext.vf8 v9,v17
vzext.vf8 v10,v18
vzext.vf8 v11,v19
vzext.vf8 v12,v20
vzext.vf8 v13,v21
vzext.vf8 v14,v22
vzext.vf8 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8m_m1:
	m_1bit
	li a0, WARMUP
1:
vzext.vf8 v8,v16,v0.t
vzext.vf8 v9,v17,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v11,v19,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v13,v21,v0.t
vzext.vf8 v14,v22,v0.t
vzext.vf8 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vzext.vf8 v8,v16,v0.t
vzext.vf8 v9,v17,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v11,v19,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v13,v21,v0.t
vzext.vf8 v14,v22,v0.t
vzext.vf8 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf8 v8,v16
vsext.vf8 v9,v17
vsext.vf8 v10,v18
vsext.vf8 v11,v19
vsext.vf8 v12,v20
vsext.vf8 v13,v21
vsext.vf8 v14,v22
vsext.vf8 v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf8 v8,v16
vsext.vf8 v9,v17
vsext.vf8 v10,v18
vsext.vf8 v11,v19
vsext.vf8 v12,v20
vsext.vf8 v13,v21
vsext.vf8 v14,v22
vsext.vf8 v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8m_m1:
	m_1bit
	li a0, WARMUP
1:
vsext.vf8 v8,v16,v0.t
vsext.vf8 v9,v17,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v11,v19,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v13,v21,v0.t
vsext.vf8 v14,v22,v0.t
vsext.vf8 v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vsext.vf8 v8,v16,v0.t
vsext.vf8 v9,v17,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v11,v19,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v13,v21,v0.t
vsext.vf8 v14,v22,v0.t
vsext.vf8 v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



// bench_vfmvfs_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfmv.f.s ft0,v8
// vfmv.f.s ft1,v9
// vfmv.f.s ft2,v10
// vfmv.f.s ft3,v11
// vfmv.f.s ft4,v12
// vfmv.f.s ft5,v13
// vfmv.f.s ft6,v14
// vfmv.f.s ft7,v15
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfmv.f.s ft0,v8
// vfmv.f.s ft1,v9
// vfmv.f.s ft2,v10
// vfmv.f.s ft3,v11
// vfmv.f.s ft4,v12
// vfmv.f.s ft5,v13
// vfmv.f.s ft6,v14
// vfmv.f.s ft7,v15
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfmvsf_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfmv.s.f v8,ft0
// vfmv.s.f v9,ft1
// vfmv.s.f v10,ft2
// vfmv.s.f v11,ft3
// vfmv.s.f v12,ft4
// vfmv.s.f v13,ft5
// vfmv.s.f v14,ft6
// vfmv.s.f v15,ft7
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfmv.s.f v8,ft0
// vfmv.s.f v9,ft1
// vfmv.s.f v10,ft2
// vfmv.s.f v11,ft3
// vfmv.s.f v12,ft4
// vfmv.s.f v13,ft5
// vfmv.s.f v14,ft6
// vfmv.s.f v15,ft7
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
//
// bench_vfcvtxufv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.xu.f.v v8,v16
// vfcvt.xu.f.v v9,v17
// vfcvt.xu.f.v v10,v18
// vfcvt.xu.f.v v11,v19
// vfcvt.xu.f.v v12,v20
// vfcvt.xu.f.v v13,v21
// vfcvt.xu.f.v v14,v22
// vfcvt.xu.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.xu.f.v v8,v16
// vfcvt.xu.f.v v9,v17
// vfcvt.xu.f.v v10,v18
// vfcvt.xu.f.v v11,v19
// vfcvt.xu.f.v v12,v20
// vfcvt.xu.f.v v13,v21
// vfcvt.xu.f.v v14,v22
// vfcvt.xu.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtxufvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.xu.f.v v8,v16,v0.t
// vfcvt.xu.f.v v9,v17,v0.t
// vfcvt.xu.f.v v10,v18,v0.t
// vfcvt.xu.f.v v11,v19,v0.t
// vfcvt.xu.f.v v12,v20,v0.t
// vfcvt.xu.f.v v13,v21,v0.t
// vfcvt.xu.f.v v14,v22,v0.t
// vfcvt.xu.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.xu.f.v v8,v16,v0.t
// vfcvt.xu.f.v v9,v17,v0.t
// vfcvt.xu.f.v v10,v18,v0.t
// vfcvt.xu.f.v v11,v19,v0.t
// vfcvt.xu.f.v v12,v20,v0.t
// vfcvt.xu.f.v v13,v21,v0.t
// vfcvt.xu.f.v v14,v22,v0.t
// vfcvt.xu.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtxfv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.x.f.v v8,v16
// vfcvt.x.f.v v9,v17
// vfcvt.x.f.v v10,v18
// vfcvt.x.f.v v11,v19
// vfcvt.x.f.v v12,v20
// vfcvt.x.f.v v13,v21
// vfcvt.x.f.v v14,v22
// vfcvt.x.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.x.f.v v8,v16
// vfcvt.x.f.v v9,v17
// vfcvt.x.f.v v10,v18
// vfcvt.x.f.v v11,v19
// vfcvt.x.f.v v12,v20
// vfcvt.x.f.v v13,v21
// vfcvt.x.f.v v14,v22
// vfcvt.x.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtxfvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.x.f.v v8,v16,v0.t
// vfcvt.x.f.v v9,v17,v0.t
// vfcvt.x.f.v v10,v18,v0.t
// vfcvt.x.f.v v11,v19,v0.t
// vfcvt.x.f.v v12,v20,v0.t
// vfcvt.x.f.v v13,v21,v0.t
// vfcvt.x.f.v v14,v22,v0.t
// vfcvt.x.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.x.f.v v8,v16,v0.t
// vfcvt.x.f.v v9,v17,v0.t
// vfcvt.x.f.v v10,v18,v0.t
// vfcvt.x.f.v v11,v19,v0.t
// vfcvt.x.f.v v12,v20,v0.t
// vfcvt.x.f.v v13,v21,v0.t
// vfcvt.x.f.v v14,v22,v0.t
// vfcvt.x.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtfxuv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.f.xu.v v8,v16
// vfcvt.f.xu.v v9,v17
// vfcvt.f.xu.v v10,v18
// vfcvt.f.xu.v v11,v19
// vfcvt.f.xu.v v12,v20
// vfcvt.f.xu.v v13,v21
// vfcvt.f.xu.v v14,v22
// vfcvt.f.xu.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.f.xu.v v8,v16
// vfcvt.f.xu.v v9,v17
// vfcvt.f.xu.v v10,v18
// vfcvt.f.xu.v v11,v19
// vfcvt.f.xu.v v12,v20
// vfcvt.f.xu.v v13,v21
// vfcvt.f.xu.v v14,v22
// vfcvt.f.xu.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtfxuvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.f.xu.v v8,v16,v0.t
// vfcvt.f.xu.v v9,v17,v0.t
// vfcvt.f.xu.v v10,v18,v0.t
// vfcvt.f.xu.v v11,v19,v0.t
// vfcvt.f.xu.v v12,v20,v0.t
// vfcvt.f.xu.v v13,v21,v0.t
// vfcvt.f.xu.v v14,v22,v0.t
// vfcvt.f.xu.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.f.xu.v v8,v16,v0.t
// vfcvt.f.xu.v v9,v17,v0.t
// vfcvt.f.xu.v v10,v18,v0.t
// vfcvt.f.xu.v v11,v19,v0.t
// vfcvt.f.xu.v v12,v20,v0.t
// vfcvt.f.xu.v v13,v21,v0.t
// vfcvt.f.xu.v v14,v22,v0.t
// vfcvt.f.xu.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtfxv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.f.x.v v8,v16
// vfcvt.f.x.v v9,v17
// vfcvt.f.x.v v10,v18
// vfcvt.f.x.v v11,v19
// vfcvt.f.x.v v12,v20
// vfcvt.f.x.v v13,v21
// vfcvt.f.x.v v14,v22
// vfcvt.f.x.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.f.x.v v8,v16
// vfcvt.f.x.v v9,v17
// vfcvt.f.x.v v10,v18
// vfcvt.f.x.v v11,v19
// vfcvt.f.x.v v12,v20
// vfcvt.f.x.v v13,v21
// vfcvt.f.x.v v14,v22
// vfcvt.f.x.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtfxvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.f.x.v v8,v16,v0.t
// vfcvt.f.x.v v9,v17,v0.t
// vfcvt.f.x.v v10,v18,v0.t
// vfcvt.f.x.v v11,v19,v0.t
// vfcvt.f.x.v v12,v20,v0.t
// vfcvt.f.x.v v13,v21,v0.t
// vfcvt.f.x.v v14,v22,v0.t
// vfcvt.f.x.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.f.x.v v8,v16,v0.t
// vfcvt.f.x.v v9,v17,v0.t
// vfcvt.f.x.v v10,v18,v0.t
// vfcvt.f.x.v v11,v19,v0.t
// vfcvt.f.x.v v12,v20,v0.t
// vfcvt.f.x.v v13,v21,v0.t
// vfcvt.f.x.v v14,v22,v0.t
// vfcvt.f.x.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtrtzxfv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.rtz.x.f.v v8,v16
// vfcvt.rtz.x.f.v v9,v17
// vfcvt.rtz.x.f.v v10,v18
// vfcvt.rtz.x.f.v v11,v19
// vfcvt.rtz.x.f.v v12,v20
// vfcvt.rtz.x.f.v v13,v21
// vfcvt.rtz.x.f.v v14,v22
// vfcvt.rtz.x.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.rtz.x.f.v v8,v16
// vfcvt.rtz.x.f.v v9,v17
// vfcvt.rtz.x.f.v v10,v18
// vfcvt.rtz.x.f.v v11,v19
// vfcvt.rtz.x.f.v v12,v20
// vfcvt.rtz.x.f.v v13,v21
// vfcvt.rtz.x.f.v v14,v22
// vfcvt.rtz.x.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtrtzxfvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.rtz.x.f.v v8,v16,v0.t
// vfcvt.rtz.x.f.v v9,v17,v0.t
// vfcvt.rtz.x.f.v v10,v18,v0.t
// vfcvt.rtz.x.f.v v11,v19,v0.t
// vfcvt.rtz.x.f.v v12,v20,v0.t
// vfcvt.rtz.x.f.v v13,v21,v0.t
// vfcvt.rtz.x.f.v v14,v22,v0.t
// vfcvt.rtz.x.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.rtz.x.f.v v8,v16,v0.t
// vfcvt.rtz.x.f.v v9,v17,v0.t
// vfcvt.rtz.x.f.v v10,v18,v0.t
// vfcvt.rtz.x.f.v v11,v19,v0.t
// vfcvt.rtz.x.f.v v12,v20,v0.t
// vfcvt.rtz.x.f.v v13,v21,v0.t
// vfcvt.rtz.x.f.v v14,v22,v0.t
// vfcvt.rtz.x.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtrtzxufv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.rtz.xu.f.v v8,v16
// vfcvt.rtz.xu.f.v v9,v17
// vfcvt.rtz.xu.f.v v10,v18
// vfcvt.rtz.xu.f.v v11,v19
// vfcvt.rtz.xu.f.v v12,v20
// vfcvt.rtz.xu.f.v v13,v21
// vfcvt.rtz.xu.f.v v14,v22
// vfcvt.rtz.xu.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.rtz.xu.f.v v8,v16
// vfcvt.rtz.xu.f.v v9,v17
// vfcvt.rtz.xu.f.v v10,v18
// vfcvt.rtz.xu.f.v v11,v19
// vfcvt.rtz.xu.f.v v12,v20
// vfcvt.rtz.xu.f.v v13,v21
// vfcvt.rtz.xu.f.v v14,v22
// vfcvt.rtz.xu.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfcvtrtzxufvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfcvt.rtz.xu.f.v v8,v16,v0.t
// vfcvt.rtz.xu.f.v v9,v17,v0.t
// vfcvt.rtz.xu.f.v v10,v18,v0.t
// vfcvt.rtz.xu.f.v v11,v19,v0.t
// vfcvt.rtz.xu.f.v v12,v20,v0.t
// vfcvt.rtz.xu.f.v v13,v21,v0.t
// vfcvt.rtz.xu.f.v v14,v22,v0.t
// vfcvt.rtz.xu.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfcvt.rtz.xu.f.v v8,v16,v0.t
// vfcvt.rtz.xu.f.v v9,v17,v0.t
// vfcvt.rtz.xu.f.v v10,v18,v0.t
// vfcvt.rtz.xu.f.v v11,v19,v0.t
// vfcvt.rtz.xu.f.v v12,v20,v0.t
// vfcvt.rtz.xu.f.v v13,v21,v0.t
// vfcvt.rtz.xu.f.v v14,v22,v0.t
// vfcvt.rtz.xu.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
//
// bench_vfwcvtxufv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.xu.f.v v8,v16
// vfwcvt.xu.f.v v9,v17
// vfwcvt.xu.f.v v10,v18
// vfwcvt.xu.f.v v11,v19
// vfwcvt.xu.f.v v12,v20
// vfwcvt.xu.f.v v13,v21
// vfwcvt.xu.f.v v14,v22
// vfwcvt.xu.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.xu.f.v v8,v16
// vfwcvt.xu.f.v v9,v17
// vfwcvt.xu.f.v v10,v18
// vfwcvt.xu.f.v v11,v19
// vfwcvt.xu.f.v v12,v20
// vfwcvt.xu.f.v v13,v21
// vfwcvt.xu.f.v v14,v22
// vfwcvt.xu.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtxufvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.xu.f.v v8,v16,v0.t
// vfwcvt.xu.f.v v9,v17,v0.t
// vfwcvt.xu.f.v v10,v18,v0.t
// vfwcvt.xu.f.v v11,v19,v0.t
// vfwcvt.xu.f.v v12,v20,v0.t
// vfwcvt.xu.f.v v13,v21,v0.t
// vfwcvt.xu.f.v v14,v22,v0.t
// vfwcvt.xu.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.xu.f.v v8,v16,v0.t
// vfwcvt.xu.f.v v9,v17,v0.t
// vfwcvt.xu.f.v v10,v18,v0.t
// vfwcvt.xu.f.v v11,v19,v0.t
// vfwcvt.xu.f.v v12,v20,v0.t
// vfwcvt.xu.f.v v13,v21,v0.t
// vfwcvt.xu.f.v v14,v22,v0.t
// vfwcvt.xu.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtxfv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.x.f.v v8,v16
// vfwcvt.x.f.v v9,v17
// vfwcvt.x.f.v v10,v18
// vfwcvt.x.f.v v11,v19
// vfwcvt.x.f.v v12,v20
// vfwcvt.x.f.v v13,v21
// vfwcvt.x.f.v v14,v22
// vfwcvt.x.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.x.f.v v8,v16
// vfwcvt.x.f.v v9,v17
// vfwcvt.x.f.v v10,v18
// vfwcvt.x.f.v v11,v19
// vfwcvt.x.f.v v12,v20
// vfwcvt.x.f.v v13,v21
// vfwcvt.x.f.v v14,v22
// vfwcvt.x.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtxfvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.x.f.v v8,v16,v0.t
// vfwcvt.x.f.v v9,v17,v0.t
// vfwcvt.x.f.v v10,v18,v0.t
// vfwcvt.x.f.v v11,v19,v0.t
// vfwcvt.x.f.v v12,v20,v0.t
// vfwcvt.x.f.v v13,v21,v0.t
// vfwcvt.x.f.v v14,v22,v0.t
// vfwcvt.x.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.x.f.v v8,v16,v0.t
// vfwcvt.x.f.v v9,v17,v0.t
// vfwcvt.x.f.v v10,v18,v0.t
// vfwcvt.x.f.v v11,v19,v0.t
// vfwcvt.x.f.v v12,v20,v0.t
// vfwcvt.x.f.v v13,v21,v0.t
// vfwcvt.x.f.v v14,v22,v0.t
// vfwcvt.x.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtfxuv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.xu.v v8,v16
// vfwcvt.f.xu.v v9,v17
// vfwcvt.f.xu.v v10,v18
// vfwcvt.f.xu.v v11,v19
// vfwcvt.f.xu.v v12,v20
// vfwcvt.f.xu.v v13,v21
// vfwcvt.f.xu.v v14,v22
// vfwcvt.f.xu.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.xu.v v8,v16
// vfwcvt.f.xu.v v9,v17
// vfwcvt.f.xu.v v10,v18
// vfwcvt.f.xu.v v11,v19
// vfwcvt.f.xu.v v12,v20
// vfwcvt.f.xu.v v13,v21
// vfwcvt.f.xu.v v14,v22
// vfwcvt.f.xu.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtfxuvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.xu.v v8,v16,v0.t
// vfwcvt.f.xu.v v9,v17,v0.t
// vfwcvt.f.xu.v v10,v18,v0.t
// vfwcvt.f.xu.v v11,v19,v0.t
// vfwcvt.f.xu.v v12,v20,v0.t
// vfwcvt.f.xu.v v13,v21,v0.t
// vfwcvt.f.xu.v v14,v22,v0.t
// vfwcvt.f.xu.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.xu.v v8,v16,v0.t
// vfwcvt.f.xu.v v9,v17,v0.t
// vfwcvt.f.xu.v v10,v18,v0.t
// vfwcvt.f.xu.v v11,v19,v0.t
// vfwcvt.f.xu.v v12,v20,v0.t
// vfwcvt.f.xu.v v13,v21,v0.t
// vfwcvt.f.xu.v v14,v22,v0.t
// vfwcvt.f.xu.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtfxv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.x.v v8,v16
// vfwcvt.f.x.v v9,v17
// vfwcvt.f.x.v v10,v18
// vfwcvt.f.x.v v11,v19
// vfwcvt.f.x.v v12,v20
// vfwcvt.f.x.v v13,v21
// vfwcvt.f.x.v v14,v22
// vfwcvt.f.x.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.x.v v8,v16
// vfwcvt.f.x.v v9,v17
// vfwcvt.f.x.v v10,v18
// vfwcvt.f.x.v v11,v19
// vfwcvt.f.x.v v12,v20
// vfwcvt.f.x.v v13,v21
// vfwcvt.f.x.v v14,v22
// vfwcvt.f.x.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtfxvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.x.v v8,v16,v0.t
// vfwcvt.f.x.v v9,v17,v0.t
// vfwcvt.f.x.v v10,v18,v0.t
// vfwcvt.f.x.v v11,v19,v0.t
// vfwcvt.f.x.v v12,v20,v0.t
// vfwcvt.f.x.v v13,v21,v0.t
// vfwcvt.f.x.v v14,v22,v0.t
// vfwcvt.f.x.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.x.v v8,v16,v0.t
// vfwcvt.f.x.v v9,v17,v0.t
// vfwcvt.f.x.v v10,v18,v0.t
// vfwcvt.f.x.v v11,v19,v0.t
// vfwcvt.f.x.v v12,v20,v0.t
// vfwcvt.f.x.v v13,v21,v0.t
// vfwcvt.f.x.v v14,v22,v0.t
// vfwcvt.f.x.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtffv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.f.v v8,v16
// vfwcvt.f.f.v v9,v17
// vfwcvt.f.f.v v10,v18
// vfwcvt.f.f.v v11,v19
// vfwcvt.f.f.v v12,v20
// vfwcvt.f.f.v v13,v21
// vfwcvt.f.f.v v14,v22
// vfwcvt.f.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.f.v v8,v16
// vfwcvt.f.f.v v9,v17
// vfwcvt.f.f.v v10,v18
// vfwcvt.f.f.v v11,v19
// vfwcvt.f.f.v v12,v20
// vfwcvt.f.f.v v13,v21
// vfwcvt.f.f.v v14,v22
// vfwcvt.f.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtffvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.f.f.v v8,v16,v0.t
// vfwcvt.f.f.v v9,v17,v0.t
// vfwcvt.f.f.v v10,v18,v0.t
// vfwcvt.f.f.v v11,v19,v0.t
// vfwcvt.f.f.v v12,v20,v0.t
// vfwcvt.f.f.v v13,v21,v0.t
// vfwcvt.f.f.v v14,v22,v0.t
// vfwcvt.f.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.f.f.v v8,v16,v0.t
// vfwcvt.f.f.v v9,v17,v0.t
// vfwcvt.f.f.v v10,v18,v0.t
// vfwcvt.f.f.v v11,v19,v0.t
// vfwcvt.f.f.v v12,v20,v0.t
// vfwcvt.f.f.v v13,v21,v0.t
// vfwcvt.f.f.v v14,v22,v0.t
// vfwcvt.f.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtrtzxufv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.rtz.xu.f.v v8,v16
// vfwcvt.rtz.xu.f.v v9,v17
// vfwcvt.rtz.xu.f.v v10,v18
// vfwcvt.rtz.xu.f.v v11,v19
// vfwcvt.rtz.xu.f.v v12,v20
// vfwcvt.rtz.xu.f.v v13,v21
// vfwcvt.rtz.xu.f.v v14,v22
// vfwcvt.rtz.xu.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.rtz.xu.f.v v8,v16
// vfwcvt.rtz.xu.f.v v9,v17
// vfwcvt.rtz.xu.f.v v10,v18
// vfwcvt.rtz.xu.f.v v11,v19
// vfwcvt.rtz.xu.f.v v12,v20
// vfwcvt.rtz.xu.f.v v13,v21
// vfwcvt.rtz.xu.f.v v14,v22
// vfwcvt.rtz.xu.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtrtzxufvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.rtz.xu.f.v v8,v16,v0.t
// vfwcvt.rtz.xu.f.v v9,v17,v0.t
// vfwcvt.rtz.xu.f.v v10,v18,v0.t
// vfwcvt.rtz.xu.f.v v11,v19,v0.t
// vfwcvt.rtz.xu.f.v v12,v20,v0.t
// vfwcvt.rtz.xu.f.v v13,v21,v0.t
// vfwcvt.rtz.xu.f.v v14,v22,v0.t
// vfwcvt.rtz.xu.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.rtz.xu.f.v v8,v16,v0.t
// vfwcvt.rtz.xu.f.v v9,v17,v0.t
// vfwcvt.rtz.xu.f.v v10,v18,v0.t
// vfwcvt.rtz.xu.f.v v11,v19,v0.t
// vfwcvt.rtz.xu.f.v v12,v20,v0.t
// vfwcvt.rtz.xu.f.v v13,v21,v0.t
// vfwcvt.rtz.xu.f.v v14,v22,v0.t
// vfwcvt.rtz.xu.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtrtzxfv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.rtz.x.f.v v8,v16
// vfwcvt.rtz.x.f.v v9,v17
// vfwcvt.rtz.x.f.v v10,v18
// vfwcvt.rtz.x.f.v v11,v19
// vfwcvt.rtz.x.f.v v12,v20
// vfwcvt.rtz.x.f.v v13,v21
// vfwcvt.rtz.x.f.v v14,v22
// vfwcvt.rtz.x.f.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.rtz.x.f.v v8,v16
// vfwcvt.rtz.x.f.v v9,v17
// vfwcvt.rtz.x.f.v v10,v18
// vfwcvt.rtz.x.f.v v11,v19
// vfwcvt.rtz.x.f.v v12,v20
// vfwcvt.rtz.x.f.v v13,v21
// vfwcvt.rtz.x.f.v v14,v22
// vfwcvt.rtz.x.f.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfwcvtrtzxfvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfwcvt.rtz.x.f.v v8,v16,v0.t
// vfwcvt.rtz.x.f.v v9,v17,v0.t
// vfwcvt.rtz.x.f.v v10,v18,v0.t
// vfwcvt.rtz.x.f.v v11,v19,v0.t
// vfwcvt.rtz.x.f.v v12,v20,v0.t
// vfwcvt.rtz.x.f.v v13,v21,v0.t
// vfwcvt.rtz.x.f.v v14,v22,v0.t
// vfwcvt.rtz.x.f.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfwcvt.rtz.x.f.v v8,v16,v0.t
// vfwcvt.rtz.x.f.v v9,v17,v0.t
// vfwcvt.rtz.x.f.v v10,v18,v0.t
// vfwcvt.rtz.x.f.v v11,v19,v0.t
// vfwcvt.rtz.x.f.v v12,v20,v0.t
// vfwcvt.rtz.x.f.v v13,v21,v0.t
// vfwcvt.rtz.x.f.v v14,v22,v0.t
// vfwcvt.rtz.x.f.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
//
// bench_vfncvtxufw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.xu.f.w v8,v16
// vfncvt.xu.f.w v9,v17
// vfncvt.xu.f.w v10,v18
// vfncvt.xu.f.w v11,v19
// vfncvt.xu.f.w v12,v20
// vfncvt.xu.f.w v13,v21
// vfncvt.xu.f.w v14,v22
// vfncvt.xu.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.xu.f.w v8,v16
// vfncvt.xu.f.w v9,v17
// vfncvt.xu.f.w v10,v18
// vfncvt.xu.f.w v11,v19
// vfncvt.xu.f.w v12,v20
// vfncvt.xu.f.w v13,v21
// vfncvt.xu.f.w v14,v22
// vfncvt.xu.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtxufwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.xu.f.w v8,v16,v0.t
// vfncvt.xu.f.w v9,v17,v0.t
// vfncvt.xu.f.w v10,v18,v0.t
// vfncvt.xu.f.w v11,v19,v0.t
// vfncvt.xu.f.w v12,v20,v0.t
// vfncvt.xu.f.w v13,v21,v0.t
// vfncvt.xu.f.w v14,v22,v0.t
// vfncvt.xu.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.xu.f.w v8,v16,v0.t
// vfncvt.xu.f.w v9,v17,v0.t
// vfncvt.xu.f.w v10,v18,v0.t
// vfncvt.xu.f.w v11,v19,v0.t
// vfncvt.xu.f.w v12,v20,v0.t
// vfncvt.xu.f.w v13,v21,v0.t
// vfncvt.xu.f.w v14,v22,v0.t
// vfncvt.xu.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtxfw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.x.f.w v8,v16
// vfncvt.x.f.w v9,v17
// vfncvt.x.f.w v10,v18
// vfncvt.x.f.w v11,v19
// vfncvt.x.f.w v12,v20
// vfncvt.x.f.w v13,v21
// vfncvt.x.f.w v14,v22
// vfncvt.x.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.x.f.w v8,v16
// vfncvt.x.f.w v9,v17
// vfncvt.x.f.w v10,v18
// vfncvt.x.f.w v11,v19
// vfncvt.x.f.w v12,v20
// vfncvt.x.f.w v13,v21
// vfncvt.x.f.w v14,v22
// vfncvt.x.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtxfwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.x.f.w v8,v16,v0.t
// vfncvt.x.f.w v9,v17,v0.t
// vfncvt.x.f.w v10,v18,v0.t
// vfncvt.x.f.w v11,v19,v0.t
// vfncvt.x.f.w v12,v20,v0.t
// vfncvt.x.f.w v13,v21,v0.t
// vfncvt.x.f.w v14,v22,v0.t
// vfncvt.x.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.x.f.w v8,v16,v0.t
// vfncvt.x.f.w v9,v17,v0.t
// vfncvt.x.f.w v10,v18,v0.t
// vfncvt.x.f.w v11,v19,v0.t
// vfncvt.x.f.w v12,v20,v0.t
// vfncvt.x.f.w v13,v21,v0.t
// vfncvt.x.f.w v14,v22,v0.t
// vfncvt.x.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtfxuw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.xu.w v8,v16
// vfncvt.f.xu.w v9,v17
// vfncvt.f.xu.w v10,v18
// vfncvt.f.xu.w v11,v19
// vfncvt.f.xu.w v12,v20
// vfncvt.f.xu.w v13,v21
// vfncvt.f.xu.w v14,v22
// vfncvt.f.xu.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.xu.w v8,v16
// vfncvt.f.xu.w v9,v17
// vfncvt.f.xu.w v10,v18
// vfncvt.f.xu.w v11,v19
// vfncvt.f.xu.w v12,v20
// vfncvt.f.xu.w v13,v21
// vfncvt.f.xu.w v14,v22
// vfncvt.f.xu.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtfxuwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.xu.w v8,v16,v0.t
// vfncvt.f.xu.w v9,v17,v0.t
// vfncvt.f.xu.w v10,v18,v0.t
// vfncvt.f.xu.w v11,v19,v0.t
// vfncvt.f.xu.w v12,v20,v0.t
// vfncvt.f.xu.w v13,v21,v0.t
// vfncvt.f.xu.w v14,v22,v0.t
// vfncvt.f.xu.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.xu.w v8,v16,v0.t
// vfncvt.f.xu.w v9,v17,v0.t
// vfncvt.f.xu.w v10,v18,v0.t
// vfncvt.f.xu.w v11,v19,v0.t
// vfncvt.f.xu.w v12,v20,v0.t
// vfncvt.f.xu.w v13,v21,v0.t
// vfncvt.f.xu.w v14,v22,v0.t
// vfncvt.f.xu.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtfxw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.x.w v8,v16
// vfncvt.f.x.w v9,v17
// vfncvt.f.x.w v10,v18
// vfncvt.f.x.w v11,v19
// vfncvt.f.x.w v12,v20
// vfncvt.f.x.w v13,v21
// vfncvt.f.x.w v14,v22
// vfncvt.f.x.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.x.w v8,v16
// vfncvt.f.x.w v9,v17
// vfncvt.f.x.w v10,v18
// vfncvt.f.x.w v11,v19
// vfncvt.f.x.w v12,v20
// vfncvt.f.x.w v13,v21
// vfncvt.f.x.w v14,v22
// vfncvt.f.x.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtfxwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.x.w v8,v16,v0.t
// vfncvt.f.x.w v9,v17,v0.t
// vfncvt.f.x.w v10,v18,v0.t
// vfncvt.f.x.w v11,v19,v0.t
// vfncvt.f.x.w v12,v20,v0.t
// vfncvt.f.x.w v13,v21,v0.t
// vfncvt.f.x.w v14,v22,v0.t
// vfncvt.f.x.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.x.w v8,v16,v0.t
// vfncvt.f.x.w v9,v17,v0.t
// vfncvt.f.x.w v10,v18,v0.t
// vfncvt.f.x.w v11,v19,v0.t
// vfncvt.f.x.w v12,v20,v0.t
// vfncvt.f.x.w v13,v21,v0.t
// vfncvt.f.x.w v14,v22,v0.t
// vfncvt.f.x.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtffw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.f.w v8,v16
// vfncvt.f.f.w v9,v17
// vfncvt.f.f.w v10,v18
// vfncvt.f.f.w v11,v19
// vfncvt.f.f.w v12,v20
// vfncvt.f.f.w v13,v21
// vfncvt.f.f.w v14,v22
// vfncvt.f.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.f.w v8,v16
// vfncvt.f.f.w v9,v17
// vfncvt.f.f.w v10,v18
// vfncvt.f.f.w v11,v19
// vfncvt.f.f.w v12,v20
// vfncvt.f.f.w v13,v21
// vfncvt.f.f.w v14,v22
// vfncvt.f.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtffwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.f.f.w v8,v16,v0.t
// vfncvt.f.f.w v9,v17,v0.t
// vfncvt.f.f.w v10,v18,v0.t
// vfncvt.f.f.w v11,v19,v0.t
// vfncvt.f.f.w v12,v20,v0.t
// vfncvt.f.f.w v13,v21,v0.t
// vfncvt.f.f.w v14,v22,v0.t
// vfncvt.f.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.f.f.w v8,v16,v0.t
// vfncvt.f.f.w v9,v17,v0.t
// vfncvt.f.f.w v10,v18,v0.t
// vfncvt.f.f.w v11,v19,v0.t
// vfncvt.f.f.w v12,v20,v0.t
// vfncvt.f.f.w v13,v21,v0.t
// vfncvt.f.f.w v14,v22,v0.t
// vfncvt.f.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtrtzxfw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rtz.x.f.w v8,v16
// vfncvt.rtz.x.f.w v9,v17
// vfncvt.rtz.x.f.w v10,v18
// vfncvt.rtz.x.f.w v11,v19
// vfncvt.rtz.x.f.w v12,v20
// vfncvt.rtz.x.f.w v13,v21
// vfncvt.rtz.x.f.w v14,v22
// vfncvt.rtz.x.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rtz.x.f.w v8,v16
// vfncvt.rtz.x.f.w v9,v17
// vfncvt.rtz.x.f.w v10,v18
// vfncvt.rtz.x.f.w v11,v19
// vfncvt.rtz.x.f.w v12,v20
// vfncvt.rtz.x.f.w v13,v21
// vfncvt.rtz.x.f.w v14,v22
// vfncvt.rtz.x.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtrtzxfwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rtz.x.f.w v8,v16,v0.t
// vfncvt.rtz.x.f.w v9,v17,v0.t
// vfncvt.rtz.x.f.w v10,v18,v0.t
// vfncvt.rtz.x.f.w v11,v19,v0.t
// vfncvt.rtz.x.f.w v12,v20,v0.t
// vfncvt.rtz.x.f.w v13,v21,v0.t
// vfncvt.rtz.x.f.w v14,v22,v0.t
// vfncvt.rtz.x.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rtz.x.f.w v8,v16,v0.t
// vfncvt.rtz.x.f.w v9,v17,v0.t
// vfncvt.rtz.x.f.w v10,v18,v0.t
// vfncvt.rtz.x.f.w v11,v19,v0.t
// vfncvt.rtz.x.f.w v12,v20,v0.t
// vfncvt.rtz.x.f.w v13,v21,v0.t
// vfncvt.rtz.x.f.w v14,v22,v0.t
// vfncvt.rtz.x.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtrtzxufw_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rtz.xu.f.w v8,v16
// vfncvt.rtz.xu.f.w v9,v17
// vfncvt.rtz.xu.f.w v10,v18
// vfncvt.rtz.xu.f.w v11,v19
// vfncvt.rtz.xu.f.w v12,v20
// vfncvt.rtz.xu.f.w v13,v21
// vfncvt.rtz.xu.f.w v14,v22
// vfncvt.rtz.xu.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rtz.xu.f.w v8,v16
// vfncvt.rtz.xu.f.w v9,v17
// vfncvt.rtz.xu.f.w v10,v18
// vfncvt.rtz.xu.f.w v11,v19
// vfncvt.rtz.xu.f.w v12,v20
// vfncvt.rtz.xu.f.w v13,v21
// vfncvt.rtz.xu.f.w v14,v22
// vfncvt.rtz.xu.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvtrtzxufwm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rtz.xu.f.w v8,v16,v0.t
// vfncvt.rtz.xu.f.w v9,v17,v0.t
// vfncvt.rtz.xu.f.w v10,v18,v0.t
// vfncvt.rtz.xu.f.w v11,v19,v0.t
// vfncvt.rtz.xu.f.w v12,v20,v0.t
// vfncvt.rtz.xu.f.w v13,v21,v0.t
// vfncvt.rtz.xu.f.w v14,v22,v0.t
// vfncvt.rtz.xu.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rtz.xu.f.w v8,v16,v0.t
// vfncvt.rtz.xu.f.w v9,v17,v0.t
// vfncvt.rtz.xu.f.w v10,v18,v0.t
// vfncvt.rtz.xu.f.w v11,v19,v0.t
// vfncvt.rtz.xu.f.w v12,v20,v0.t
// vfncvt.rtz.xu.f.w v13,v21,v0.t
// vfncvt.rtz.xu.f.w v14,v22,v0.t
// vfncvt.rtz.xu.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvt.rod.f.f.w_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rod.f.f.w v8,v16
// vfncvt.rod.f.f.w v9,v17
// vfncvt.rod.f.f.w v10,v18
// vfncvt.rod.f.f.w v11,v19
// vfncvt.rod.f.f.w v12,v20
// vfncvt.rod.f.f.w v13,v21
// vfncvt.rod.f.f.w v14,v22
// vfncvt.rod.f.f.w v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rod.f.f.w v8,v16
// vfncvt.rod.f.f.w v9,v17
// vfncvt.rod.f.f.w v10,v18
// vfncvt.rod.f.f.w v11,v19
// vfncvt.rod.f.f.w v12,v20
// vfncvt.rod.f.f.w v13,v21
// vfncvt.rod.f.f.w v14,v22
// vfncvt.rod.f.f.w v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfncvt.rod.f.f.wm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfncvt.rod.f.f.w v8,v16,v0.t
// vfncvt.rod.f.f.w v9,v17,v0.t
// vfncvt.rod.f.f.w v10,v18,v0.t
// vfncvt.rod.f.f.w v11,v19,v0.t
// vfncvt.rod.f.f.w v12,v20,v0.t
// vfncvt.rod.f.f.w v13,v21,v0.t
// vfncvt.rod.f.f.w v14,v22,v0.t
// vfncvt.rod.f.f.w v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfncvt.rod.f.f.w v8,v16,v0.t
// vfncvt.rod.f.f.w v9,v17,v0.t
// vfncvt.rod.f.f.w v10,v18,v0.t
// vfncvt.rod.f.f.w v11,v19,v0.t
// vfncvt.rod.f.f.w v12,v20,v0.t
// vfncvt.rod.f.f.w v13,v21,v0.t
// vfncvt.rod.f.f.w v14,v22,v0.t
// vfncvt.rod.f.f.w v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
//
// bench_vfsqrtv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfsqrt.v v8,v16
// vfsqrt.v v9,v17
// vfsqrt.v v10,v18
// vfsqrt.v v11,v19
// vfsqrt.v v12,v20
// vfsqrt.v v13,v21
// vfsqrt.v v14,v22
// vfsqrt.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfsqrt.v v8,v16
// vfsqrt.v v9,v17
// vfsqrt.v v10,v18
// vfsqrt.v v11,v19
// vfsqrt.v v12,v20
// vfsqrt.v v13,v21
// vfsqrt.v v14,v22
// vfsqrt.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfsqrtvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfsqrt.v v8,v16,v0.t
// vfsqrt.v v9,v17,v0.t
// vfsqrt.v v10,v18,v0.t
// vfsqrt.v v11,v19,v0.t
// vfsqrt.v v12,v20,v0.t
// vfsqrt.v v13,v21,v0.t
// vfsqrt.v v14,v22,v0.t
// vfsqrt.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfsqrt.v v8,v16,v0.t
// vfsqrt.v v9,v17,v0.t
// vfsqrt.v v10,v18,v0.t
// vfsqrt.v v11,v19,v0.t
// vfsqrt.v v12,v20,v0.t
// vfsqrt.v v13,v21,v0.t
// vfsqrt.v v14,v22,v0.t
// vfsqrt.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfrsqrt7v_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfrsqrt7.v v8,v16
// vfrsqrt7.v v9,v17
// vfrsqrt7.v v10,v18
// vfrsqrt7.v v11,v19
// vfrsqrt7.v v12,v20
// vfrsqrt7.v v13,v21
// vfrsqrt7.v v14,v22
// vfrsqrt7.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfrsqrt7.v v8,v16
// vfrsqrt7.v v9,v17
// vfrsqrt7.v v10,v18
// vfrsqrt7.v v11,v19
// vfrsqrt7.v v12,v20
// vfrsqrt7.v v13,v21
// vfrsqrt7.v v14,v22
// vfrsqrt7.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfrsqrt7vm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfrsqrt7.v v8,v16,v0.t
// vfrsqrt7.v v9,v17,v0.t
// vfrsqrt7.v v10,v18,v0.t
// vfrsqrt7.v v11,v19,v0.t
// vfrsqrt7.v v12,v20,v0.t
// vfrsqrt7.v v13,v21,v0.t
// vfrsqrt7.v v14,v22,v0.t
// vfrsqrt7.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfrsqrt7.v v8,v16,v0.t
// vfrsqrt7.v v9,v17,v0.t
// vfrsqrt7.v v10,v18,v0.t
// vfrsqrt7.v v11,v19,v0.t
// vfrsqrt7.v v12,v20,v0.t
// vfrsqrt7.v v13,v21,v0.t
// vfrsqrt7.v v14,v22,v0.t
// vfrsqrt7.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfrec7v_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfrec7.v v8,v16
// vfrec7.v v9,v17
// vfrec7.v v10,v18
// vfrec7.v v11,v19
// vfrec7.v v12,v20
// vfrec7.v v13,v21
// vfrec7.v v14,v22
// vfrec7.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfrec7.v v8,v16
// vfrec7.v v9,v17
// vfrec7.v v10,v18
// vfrec7.v v11,v19
// vfrec7.v v12,v20
// vfrec7.v v13,v21
// vfrec7.v v14,v22
// vfrec7.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfrec7vm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfrec7.v v8,v16,v0.t
// vfrec7.v v9,v17,v0.t
// vfrec7.v v10,v18,v0.t
// vfrec7.v v11,v19,v0.t
// vfrec7.v v12,v20,v0.t
// vfrec7.v v13,v21,v0.t
// vfrec7.v v14,v22,v0.t
// vfrec7.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfrec7.v v8,v16,v0.t
// vfrec7.v v9,v17,v0.t
// vfrec7.v v10,v18,v0.t
// vfrec7.v v11,v19,v0.t
// vfrec7.v v12,v20,v0.t
// vfrec7.v v13,v21,v0.t
// vfrec7.v v14,v22,v0.t
// vfrec7.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfclassv_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfclass.v v8,v16
// vfclass.v v9,v17
// vfclass.v v10,v18
// vfclass.v v11,v19
// vfclass.v v12,v20
// vfclass.v v13,v21
// vfclass.v v14,v22
// vfclass.v v15,v23
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfclass.v v8,v16
// vfclass.v v9,v17
// vfclass.v v10,v18
// vfclass.v v11,v19
// vfclass.v v12,v20
// vfclass.v v13,v21
// vfclass.v v14,v22
// vfclass.v v15,v23
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret
//
//
// bench_vfclassvm_m1:
// 	m_nop
// 	li a0, WARMUP
// 1:
// vfclass.v v8,v16,v0.t
// vfclass.v v9,v17,v0.t
// vfclass.v v10,v18,v0.t
// vfclass.v v11,v19,v0.t
// vfclass.v v12,v20,v0.t
// vfclass.v v13,v21,v0.t
// vfclass.v v14,v22,v0.t
// vfclass.v v15,v23,v0.t
//
//
//
// 	addi a0, a0, -1
// 	bnez a0, 1b
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a3, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a3, mcycle
// #else
// 	csrr a3, cycle
// #endif
// 	li a0, LOOP
// 1:
// .rept UNROLL
// vfclass.v v8,v16,v0.t
// vfclass.v v9,v17,v0.t
// vfclass.v v10,v18,v0.t
// vfclass.v v11,v19,v0.t
// vfclass.v v12,v20,v0.t
// vfclass.v v13,v21,v0.t
// vfclass.v v14,v22,v0.t
// vfclass.v v15,v23,v0.t
//
//
//
// .endr
// 	addi a0, a0, -1
// 	bnez a0, 1b
// 	fence.i
// #if defined(USE_PERF_EVENT_SLOW)
// 	ld a0, nolibc_perf_event_fd
// 	la a1, u64_cycle
// 	li a2, 8
// 	li a7, 63
// 	ecall # clobbers vregs and vtype
// 	ld a0, u64_cycle
// 	vsetvl VL, VL, s2
// 	bnez VL, 2f
// 	vsetvl VL, x0, s2
// 	2:
// #elif defined(READ_MCYCLE)
// 	csrr a0, mcycle
// #else
// 	csrr a0, cycle
// #endif
// 	sub a0, a0, a3
// ret



bench_vmsbfm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsbf.m v8,v16
vmsbf.m v9,v17
vmsbf.m v10,v18
vmsbf.m v11,v19
vmsbf.m v12,v20
vmsbf.m v13,v21
vmsbf.m v14,v22
vmsbf.m v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbf.m v8,v16
vmsbf.m v9,v17
vmsbf.m v10,v18
vmsbf.m v11,v19
vmsbf.m v12,v20
vmsbf.m v13,v21
vmsbf.m v14,v22
vmsbf.m v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbfmm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsbf.m v8,v16,v0.t
vmsbf.m v9,v17,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v11,v19,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v13,v21,v0.t
vmsbf.m v14,v22,v0.t
vmsbf.m v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsbf.m v8,v16,v0.t
vmsbf.m v9,v17,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v11,v19,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v13,v21,v0.t
vmsbf.m v14,v22,v0.t
vmsbf.m v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsof.m v8,v16
vmsof.m v9,v17
vmsof.m v10,v18
vmsof.m v11,v19
vmsof.m v12,v20
vmsof.m v13,v21
vmsof.m v14,v22
vmsof.m v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsof.m v8,v16
vmsof.m v9,v17
vmsof.m v10,v18
vmsof.m v11,v19
vmsof.m v12,v20
vmsof.m v13,v21
vmsof.m v14,v22
vmsof.m v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofmm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsof.m v8,v16,v0.t
vmsof.m v9,v17,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v11,v19,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v13,v21,v0.t
vmsof.m v14,v22,v0.t
vmsof.m v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsof.m v8,v16,v0.t
vmsof.m v9,v17,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v11,v19,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v13,v21,v0.t
vmsof.m v14,v22,v0.t
vmsof.m v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsif.m v8,v16
vmsif.m v9,v17
vmsif.m v10,v18
vmsif.m v11,v19
vmsif.m v12,v20
vmsif.m v13,v21
vmsif.m v14,v22
vmsif.m v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsif.m v8,v16
vmsif.m v9,v17
vmsif.m v10,v18
vmsif.m v11,v19
vmsif.m v12,v20
vmsif.m v13,v21
vmsif.m v14,v22
vmsif.m v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifmm_m1:
	m_1bit
	li a0, WARMUP
1:
vmsif.m v8,v16,v0.t
vmsif.m v9,v17,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v11,v19,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v13,v21,v0.t
vmsif.m v14,v22,v0.t
vmsif.m v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vmsif.m v8,v16,v0.t
vmsif.m v9,v17,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v11,v19,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v13,v21,v0.t
vmsif.m v14,v22,v0.t
vmsif.m v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotam_m1:
	m_nop
	li a0, WARMUP
1:
viota.m v8,v16
viota.m v9,v17
viota.m v10,v18
viota.m v11,v19
viota.m v12,v20
viota.m v13,v21
viota.m v14,v22
viota.m v15,v23



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
viota.m v8,v16
viota.m v9,v17
viota.m v10,v18
viota.m v11,v19
viota.m v12,v20
viota.m v13,v21
viota.m v14,v22
viota.m v15,v23



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotamm_m1:
	m_nop
	li a0, WARMUP
1:
viota.m v8,v16,v0.t
viota.m v9,v17,v0.t
viota.m v10,v18,v0.t
viota.m v11,v19,v0.t
viota.m v12,v20,v0.t
viota.m v13,v21,v0.t
viota.m v14,v22,v0.t
viota.m v15,v23,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
viota.m v8,v16,v0.t
viota.m v9,v17,v0.t
viota.m v10,v18,v0.t
viota.m v11,v19,v0.t
viota.m v12,v20,v0.t
viota.m v13,v21,v0.t
viota.m v14,v22,v0.t
viota.m v15,v23,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidv_m1:
	m_nop
	li a0, WARMUP
1:
vid.v v8
vid.v v9
vid.v v10
vid.v v11
vid.v v12
vid.v v13
vid.v v14
vid.v v15



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vid.v v8
vid.v v9
vid.v v10
vid.v v11
vid.v v12
vid.v v13
vid.v v14
vid.v v15



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidvm_m1:
	m_nop
	li a0, WARMUP
1:
vid.v v8,v0.t
vid.v v9,v0.t
vid.v v10,v0.t
vid.v v11,v0.t
vid.v v12,v0.t
vid.v v13,v0.t
vid.v v14,v0.t
vid.v v15,v0.t



	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL
vid.v v8,v0.t
vid.v v9,v0.t
vid.v v10,v0.t
vid.v v11,v0.t
vid.v v12,v0.t
vid.v v13,v0.t
vid.v v14,v0.t
vid.v v15,v0.t



.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret






bench_add_m2:
	m_nop
	li a0, WARMUP
1:

add t0,t1,t2
add t2,t1,t2
add t4,t1,t2
add t6,t1,t2
add t0,t1,t2
add t2,t1,t2
add t4,t1,t2
add t6,t1,t2


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

add t0,t1,t2
add t2,t1,t2
add t4,t1,t2
add t6,t1,t2
add t0,t1,t2
add t2,t1,t2
add t4,t1,t2
add t6,t1,t2


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_mul_m2:
	m_nop
	li a0, WARMUP
1:

mul t0,t1,t2
mul t2,t1,t2
mul t4,t1,t2
mul t6,t1,t2
mul t0,t1,t2
mul t2,t1,t2
mul t4,t1,t2
mul t6,t1,t2


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

mul t0,t1,t2
mul t2,t1,t2
mul t4,t1,t2
mul t6,t1,t2
mul t0,t1,t2
mul t2,t1,t2
mul t4,t1,t2
mul t6,t1,t2


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vv v8,v16,v24
vadd.vv v10,v18,v26
vadd.vv v12,v20,v28
vadd.vv v14,v22,v30
vadd.vv v8,v16,v24
vadd.vv v10,v18,v26
vadd.vv v12,v20,v28
vadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vv v8,v16,v24
vadd.vv v10,v18,v26
vadd.vv v12,v20,v28
vadd.vv v14,v22,v30
vadd.vv v8,v16,v24
vadd.vv v10,v18,v26
vadd.vv v12,v20,v28
vadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vv v8,v16,v24,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v14,v22,v30,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vv v8,v16,v24,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v14,v22,v30,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v10,v18,v26,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvx_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vx v8,v16,t0
vadd.vx v10,v18,t2
vadd.vx v12,v20,t4
vadd.vx v14,v22,t6
vadd.vx v8,v16,t0
vadd.vx v10,v18,t2
vadd.vx v12,v20,t4
vadd.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vx v8,v16,t0
vadd.vx v10,v18,t2
vadd.vx v12,v20,t4
vadd.vx v14,v22,t6
vadd.vx v8,v16,t0
vadd.vx v10,v18,t2
vadd.vx v12,v20,t4
vadd.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvxm_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vx v8,v16,t0,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v14,v22,t6,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vx v8,v16,t0,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v14,v22,t6,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v10,v18,t2,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvi_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vi v8,v16,13
vadd.vi v10,v18,13
vadd.vi v12,v20,13
vadd.vi v14,v22,13
vadd.vi v8,v16,13
vadd.vi v10,v18,13
vadd.vi v12,v20,13
vadd.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vi v8,v16,13
vadd.vi v10,v18,13
vadd.vi v12,v20,13
vadd.vi v14,v22,13
vadd.vi v8,v16,13
vadd.vi v10,v18,13
vadd.vi v12,v20,13
vadd.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvim_m2:
	m_nop
	li a0, WARMUP
1:

vadd.vi v8,v16,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v14,v22,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadd.vi v8,v16,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v14,v22,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v10,v18,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vsub.vv v8,v16,v24
vsub.vv v10,v18,v26
vsub.vv v12,v20,v28
vsub.vv v14,v22,v30
vsub.vv v8,v16,v24
vsub.vv v10,v18,v26
vsub.vv v12,v20,v28
vsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsub.vv v8,v16,v24
vsub.vv v10,v18,v26
vsub.vv v12,v20,v28
vsub.vv v14,v22,v30
vsub.vv v8,v16,v24
vsub.vv v10,v18,v26
vsub.vv v12,v20,v28
vsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsub.vv v8,v16,v24,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v14,v22,v30,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsub.vv v8,v16,v24,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v14,v22,v30,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v10,v18,v26,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvx_m2:
	m_nop
	li a0, WARMUP
1:

vsub.vx v8,v16,t0
vsub.vx v10,v18,t2
vsub.vx v12,v20,t4
vsub.vx v14,v22,t6
vsub.vx v8,v16,t0
vsub.vx v10,v18,t2
vsub.vx v12,v20,t4
vsub.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsub.vx v8,v16,t0
vsub.vx v10,v18,t2
vsub.vx v12,v20,t4
vsub.vx v14,v22,t6
vsub.vx v8,v16,t0
vsub.vx v10,v18,t2
vsub.vx v12,v20,t4
vsub.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsub.vx v8,v16,t0,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v14,v22,t6,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsub.vx v8,v16,t0,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v14,v22,t6,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v10,v18,t2,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvx_m2:
	m_nop
	li a0, WARMUP
1:

vrsub.vx v8,v16,t0
vrsub.vx v10,v18,t2
vrsub.vx v12,v20,t4
vrsub.vx v14,v22,t6
vrsub.vx v8,v16,t0
vrsub.vx v10,v18,t2
vrsub.vx v12,v20,t4
vrsub.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrsub.vx v8,v16,t0
vrsub.vx v10,v18,t2
vrsub.vx v12,v20,t4
vrsub.vx v14,v22,t6
vrsub.vx v8,v16,t0
vrsub.vx v10,v18,t2
vrsub.vx v12,v20,t4
vrsub.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vrsub.vx v8,v16,t0,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v14,v22,t6,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrsub.vx v8,v16,t0,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v14,v22,t6,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v10,v18,t2,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvi_m2:
	m_nop
	li a0, WARMUP
1:

vrsub.vi v8,v16,13
vrsub.vi v10,v18,13
vrsub.vi v12,v20,13
vrsub.vi v14,v22,13
vrsub.vi v8,v16,13
vrsub.vi v10,v18,13
vrsub.vi v12,v20,13
vrsub.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrsub.vi v8,v16,13
vrsub.vi v10,v18,13
vrsub.vi v12,v20,13
vrsub.vi v14,v22,13
vrsub.vi v8,v16,13
vrsub.vi v10,v18,13
vrsub.vi v12,v20,13
vrsub.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvim_m2:
	m_nop
	li a0, WARMUP
1:

vrsub.vi v8,v16,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v14,v22,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrsub.vi v8,v16,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v14,v22,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v10,v18,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvv_m2:
	m_nop
	li a0, WARMUP
1:

vminu.vv v8,v16,v24
vminu.vv v10,v18,v26
vminu.vv v12,v20,v28
vminu.vv v14,v22,v30
vminu.vv v8,v16,v24
vminu.vv v10,v18,v26
vminu.vv v12,v20,v28
vminu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vminu.vv v8,v16,v24
vminu.vv v10,v18,v26
vminu.vv v12,v20,v28
vminu.vv v14,v22,v30
vminu.vv v8,v16,v24
vminu.vv v10,v18,v26
vminu.vv v12,v20,v28
vminu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vminu.vv v8,v16,v24,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v14,v22,v30,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vminu.vv v8,v16,v24,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v14,v22,v30,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v10,v18,v26,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvx_m2:
	m_nop
	li a0, WARMUP
1:

vminu.vx v8,v16,t0
vminu.vx v10,v18,t2
vminu.vx v12,v20,t4
vminu.vx v14,v22,t6
vminu.vx v8,v16,t0
vminu.vx v10,v18,t2
vminu.vx v12,v20,t4
vminu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vminu.vx v8,v16,t0
vminu.vx v10,v18,t2
vminu.vx v12,v20,t4
vminu.vx v14,v22,t6
vminu.vx v8,v16,t0
vminu.vx v10,v18,t2
vminu.vx v12,v20,t4
vminu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vminu.vx v8,v16,t0,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v14,v22,t6,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vminu.vx v8,v16,t0,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v14,v22,t6,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v10,v18,t2,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvv_m2:
	m_nop
	li a0, WARMUP
1:

vmin.vv v8,v16,v24
vmin.vv v10,v18,v26
vmin.vv v12,v20,v28
vmin.vv v14,v22,v30
vmin.vv v8,v16,v24
vmin.vv v10,v18,v26
vmin.vv v12,v20,v28
vmin.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmin.vv v8,v16,v24
vmin.vv v10,v18,v26
vmin.vv v12,v20,v28
vmin.vv v14,v22,v30
vmin.vv v8,v16,v24
vmin.vv v10,v18,v26
vmin.vv v12,v20,v28
vmin.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmin.vv v8,v16,v24,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v14,v22,v30,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmin.vv v8,v16,v24,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v14,v22,v30,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v10,v18,v26,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvx_m2:
	m_nop
	li a0, WARMUP
1:

vmin.vx v8,v16,t0
vmin.vx v10,v18,t2
vmin.vx v12,v20,t4
vmin.vx v14,v22,t6
vmin.vx v8,v16,t0
vmin.vx v10,v18,t2
vmin.vx v12,v20,t4
vmin.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmin.vx v8,v16,t0
vmin.vx v10,v18,t2
vmin.vx v12,v20,t4
vmin.vx v14,v22,t6
vmin.vx v8,v16,t0
vmin.vx v10,v18,t2
vmin.vx v12,v20,t4
vmin.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmin.vx v8,v16,t0,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v14,v22,t6,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmin.vx v8,v16,t0,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v14,v22,t6,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v10,v18,t2,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvv_m2:
	m_nop
	li a0, WARMUP
1:

vmaxu.vv v8,v16,v24
vmaxu.vv v10,v18,v26
vmaxu.vv v12,v20,v28
vmaxu.vv v14,v22,v30
vmaxu.vv v8,v16,v24
vmaxu.vv v10,v18,v26
vmaxu.vv v12,v20,v28
vmaxu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmaxu.vv v8,v16,v24
vmaxu.vv v10,v18,v26
vmaxu.vv v12,v20,v28
vmaxu.vv v14,v22,v30
vmaxu.vv v8,v16,v24
vmaxu.vv v10,v18,v26
vmaxu.vv v12,v20,v28
vmaxu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v14,v22,v30,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v14,v22,v30,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v10,v18,v26,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmaxu.vx v8,v16,t0
vmaxu.vx v10,v18,t2
vmaxu.vx v12,v20,t4
vmaxu.vx v14,v22,t6
vmaxu.vx v8,v16,t0
vmaxu.vx v10,v18,t2
vmaxu.vx v12,v20,t4
vmaxu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmaxu.vx v8,v16,t0
vmaxu.vx v10,v18,t2
vmaxu.vx v12,v20,t4
vmaxu.vx v14,v22,t6
vmaxu.vx v8,v16,t0
vmaxu.vx v10,v18,t2
vmaxu.vx v12,v20,t4
vmaxu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v14,v22,t6,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v14,v22,t6,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v10,v18,t2,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvv_m2:
	m_nop
	li a0, WARMUP
1:

vmax.vv v8,v16,v24
vmax.vv v10,v18,v26
vmax.vv v12,v20,v28
vmax.vv v14,v22,v30
vmax.vv v8,v16,v24
vmax.vv v10,v18,v26
vmax.vv v12,v20,v28
vmax.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmax.vv v8,v16,v24
vmax.vv v10,v18,v26
vmax.vv v12,v20,v28
vmax.vv v14,v22,v30
vmax.vv v8,v16,v24
vmax.vv v10,v18,v26
vmax.vv v12,v20,v28
vmax.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmax.vv v8,v16,v24,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v14,v22,v30,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmax.vv v8,v16,v24,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v14,v22,v30,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v10,v18,v26,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvx_m2:
	m_nop
	li a0, WARMUP
1:

vmax.vx v8,v16,t0
vmax.vx v10,v18,t2
vmax.vx v12,v20,t4
vmax.vx v14,v22,t6
vmax.vx v8,v16,t0
vmax.vx v10,v18,t2
vmax.vx v12,v20,t4
vmax.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmax.vx v8,v16,t0
vmax.vx v10,v18,t2
vmax.vx v12,v20,t4
vmax.vx v14,v22,t6
vmax.vx v8,v16,t0
vmax.vx v10,v18,t2
vmax.vx v12,v20,t4
vmax.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmax.vx v8,v16,t0,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v14,v22,t6,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmax.vx v8,v16,t0,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v14,v22,t6,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v10,v18,t2,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvv_m2:
	m_nop
	li a0, WARMUP
1:

vand.vv v8,v16,v24
vand.vv v10,v18,v26
vand.vv v12,v20,v28
vand.vv v14,v22,v30
vand.vv v8,v16,v24
vand.vv v10,v18,v26
vand.vv v12,v20,v28
vand.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vv v8,v16,v24
vand.vv v10,v18,v26
vand.vv v12,v20,v28
vand.vv v14,v22,v30
vand.vv v8,v16,v24
vand.vv v10,v18,v26
vand.vv v12,v20,v28
vand.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvvm_m2:
	m_nop
	li a0, WARMUP
1:

vand.vv v8,v16,v24,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v14,v22,v30,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vv v8,v16,v24,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v14,v22,v30,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v10,v18,v26,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvx_m2:
	m_nop
	li a0, WARMUP
1:

vand.vx v8,v16,t0
vand.vx v10,v18,t2
vand.vx v12,v20,t4
vand.vx v14,v22,t6
vand.vx v8,v16,t0
vand.vx v10,v18,t2
vand.vx v12,v20,t4
vand.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vx v8,v16,t0
vand.vx v10,v18,t2
vand.vx v12,v20,t4
vand.vx v14,v22,t6
vand.vx v8,v16,t0
vand.vx v10,v18,t2
vand.vx v12,v20,t4
vand.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvxm_m2:
	m_nop
	li a0, WARMUP
1:

vand.vx v8,v16,t0,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v14,v22,t6,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vx v8,v16,t0,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v14,v22,t6,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v10,v18,t2,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvi_m2:
	m_nop
	li a0, WARMUP
1:

vand.vi v8,v16,13
vand.vi v10,v18,13
vand.vi v12,v20,13
vand.vi v14,v22,13
vand.vi v8,v16,13
vand.vi v10,v18,13
vand.vi v12,v20,13
vand.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vi v8,v16,13
vand.vi v10,v18,13
vand.vi v12,v20,13
vand.vi v14,v22,13
vand.vi v8,v16,13
vand.vi v10,v18,13
vand.vi v12,v20,13
vand.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvim_m2:
	m_nop
	li a0, WARMUP
1:

vand.vi v8,v16,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v14,v22,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vand.vi v8,v16,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v14,v22,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v10,v18,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvv_m2:
	m_nop
	li a0, WARMUP
1:

vor.vv v8,v16,v24
vor.vv v10,v18,v26
vor.vv v12,v20,v28
vor.vv v14,v22,v30
vor.vv v8,v16,v24
vor.vv v10,v18,v26
vor.vv v12,v20,v28
vor.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vv v8,v16,v24
vor.vv v10,v18,v26
vor.vv v12,v20,v28
vor.vv v14,v22,v30
vor.vv v8,v16,v24
vor.vv v10,v18,v26
vor.vv v12,v20,v28
vor.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvvm_m2:
	m_nop
	li a0, WARMUP
1:

vor.vv v8,v16,v24,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v14,v22,v30,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vv v8,v16,v24,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v14,v22,v30,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v10,v18,v26,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvx_m2:
	m_nop
	li a0, WARMUP
1:

vor.vx v8,v16,t0
vor.vx v10,v18,t2
vor.vx v12,v20,t4
vor.vx v14,v22,t6
vor.vx v8,v16,t0
vor.vx v10,v18,t2
vor.vx v12,v20,t4
vor.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vx v8,v16,t0
vor.vx v10,v18,t2
vor.vx v12,v20,t4
vor.vx v14,v22,t6
vor.vx v8,v16,t0
vor.vx v10,v18,t2
vor.vx v12,v20,t4
vor.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvxm_m2:
	m_nop
	li a0, WARMUP
1:

vor.vx v8,v16,t0,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v14,v22,t6,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vx v8,v16,t0,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v14,v22,t6,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v10,v18,t2,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvi_m2:
	m_nop
	li a0, WARMUP
1:

vor.vi v8,v16,13
vor.vi v10,v18,13
vor.vi v12,v20,13
vor.vi v14,v22,13
vor.vi v8,v16,13
vor.vi v10,v18,13
vor.vi v12,v20,13
vor.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vi v8,v16,13
vor.vi v10,v18,13
vor.vi v12,v20,13
vor.vi v14,v22,13
vor.vi v8,v16,13
vor.vi v10,v18,13
vor.vi v12,v20,13
vor.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvim_m2:
	m_nop
	li a0, WARMUP
1:

vor.vi v8,v16,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v14,v22,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vor.vi v8,v16,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v14,v22,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v10,v18,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvv_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vv v8,v16,v24
vxor.vv v10,v18,v26
vxor.vv v12,v20,v28
vxor.vv v14,v22,v30
vxor.vv v8,v16,v24
vxor.vv v10,v18,v26
vxor.vv v12,v20,v28
vxor.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vv v8,v16,v24
vxor.vv v10,v18,v26
vxor.vv v12,v20,v28
vxor.vv v14,v22,v30
vxor.vv v8,v16,v24
vxor.vv v10,v18,v26
vxor.vv v12,v20,v28
vxor.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvvm_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vv v8,v16,v24,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v14,v22,v30,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vv v8,v16,v24,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v14,v22,v30,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v10,v18,v26,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvx_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vx v8,v16,t0
vxor.vx v10,v18,t2
vxor.vx v12,v20,t4
vxor.vx v14,v22,t6
vxor.vx v8,v16,t0
vxor.vx v10,v18,t2
vxor.vx v12,v20,t4
vxor.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vx v8,v16,t0
vxor.vx v10,v18,t2
vxor.vx v12,v20,t4
vxor.vx v14,v22,t6
vxor.vx v8,v16,t0
vxor.vx v10,v18,t2
vxor.vx v12,v20,t4
vxor.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvxm_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vx v8,v16,t0,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v14,v22,t6,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vx v8,v16,t0,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v14,v22,t6,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v10,v18,t2,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvi_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vi v8,v16,13
vxor.vi v10,v18,13
vxor.vi v12,v20,13
vxor.vi v14,v22,13
vxor.vi v8,v16,13
vxor.vi v10,v18,13
vxor.vi v12,v20,13
vxor.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vi v8,v16,13
vxor.vi v10,v18,13
vxor.vi v12,v20,13
vxor.vi v14,v22,13
vxor.vi v8,v16,13
vxor.vi v10,v18,13
vxor.vi v12,v20,13
vxor.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvim_m2:
	m_nop
	li a0, WARMUP
1:

vxor.vi v8,v16,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v14,v22,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vxor.vi v8,v16,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v14,v22,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v10,v18,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vrgathervv_m2:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:

vrgather.vv v8,v16,v24
vrgather.vv v10,v18,v26
vrgather.vv v12,v20,v28
vrgather.vv v14,v22,v30
vrgather.vv v8,v16,v24
vrgather.vv v10,v18,v26
vrgather.vv v12,v20,v28
vrgather.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vv v8,v16,v24
vrgather.vv v10,v18,v26
vrgather.vv v12,v20,v28
vrgather.vv v14,v22,v30
vrgather.vv v8,v16,v24
vrgather.vv v10,v18,v26
vrgather.vv v12,v20,v28
vrgather.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervvm_m2:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:

vrgather.vv v8,v16,v24,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v14,v22,v30,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vv v8,v16,v24,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v14,v22,v30,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v10,v18,v26,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervx_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vrgather.vx v8,v16,t0
vrgather.vx v10,v18,t2
vrgather.vx v12,v20,t4
vrgather.vx v14,v22,t6
vrgather.vx v8,v16,t0
vrgather.vx v10,v18,t2
vrgather.vx v12,v20,t4
vrgather.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vx v8,v16,t0
vrgather.vx v10,v18,t2
vrgather.vx v12,v20,t4
vrgather.vx v14,v22,t6
vrgather.vx v8,v16,t0
vrgather.vx v10,v18,t2
vrgather.vx v12,v20,t4
vrgather.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervxm_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vrgather.vx v8,v16,t0,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v14,v22,t6,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vx v8,v16,t0,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v14,v22,t6,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v10,v18,t2,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervi_m2:
	m_nop
	li a0, WARMUP
1:

vrgather.vi v8,v16,3
vrgather.vi v10,v18,3
vrgather.vi v12,v20,3
vrgather.vi v14,v22,3
vrgather.vi v8,v16,3
vrgather.vi v10,v18,3
vrgather.vi v12,v20,3
vrgather.vi v14,v22,3


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vi v8,v16,3
vrgather.vi v10,v18,3
vrgather.vi v12,v20,3
vrgather.vi v14,v22,3
vrgather.vi v8,v16,3
vrgather.vi v10,v18,3
vrgather.vi v12,v20,3
vrgather.vi v14,v22,3


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervim_m2:
	m_nop
	li a0, WARMUP
1:

vrgather.vi v8,v16,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v14,v22,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v14,v22,3,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgather.vi v8,v16,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v14,v22,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v10,v18,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v14,v22,3,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvx_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vslideup.vx v8,v16,t0
vslideup.vx v10,v18,t2
vslideup.vx v12,v20,t4
vslideup.vx v14,v22,t6
vslideup.vx v8,v16,t0
vslideup.vx v10,v18,t2
vslideup.vx v12,v20,t4
vslideup.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslideup.vx v8,v16,t0
vslideup.vx v10,v18,t2
vslideup.vx v12,v20,t4
vslideup.vx v14,v22,t6
vslideup.vx v8,v16,t0
vslideup.vx v10,v18,t2
vslideup.vx v12,v20,t4
vslideup.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvxm_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vslideup.vx v8,v16,t0,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v14,v22,t6,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslideup.vx v8,v16,t0,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v14,v22,t6,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v10,v18,t2,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvi_m2:
	m_nop
	li a0, WARMUP
1:

vslideup.vi v8,v16,3
vslideup.vi v10,v18,3
vslideup.vi v12,v20,3
vslideup.vi v14,v22,3
vslideup.vi v8,v16,3
vslideup.vi v10,v18,3
vslideup.vi v12,v20,3
vslideup.vi v14,v22,3


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslideup.vi v8,v16,3
vslideup.vi v10,v18,3
vslideup.vi v12,v20,3
vslideup.vi v14,v22,3
vslideup.vi v8,v16,3
vslideup.vi v10,v18,3
vslideup.vi v12,v20,3
vslideup.vi v14,v22,3


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvim_m2:
	m_nop
	li a0, WARMUP
1:

vslideup.vi v8,v16,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v14,v22,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v14,v22,3,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslideup.vi v8,v16,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v14,v22,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v10,v18,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v14,v22,3,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vv_m2:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:

vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v14,v22,v30
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v14,v22,v30
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v10,v18,v26
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vvm_m2:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:

vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v14,v22,v30,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v14,v22,v30,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v10,v18,v26,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslidedownvx_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vslidedown.vx v8,v16,t0
vslidedown.vx v10,v18,t2
vslidedown.vx v12,v20,t4
vslidedown.vx v14,v22,t6
vslidedown.vx v8,v16,t0
vslidedown.vx v10,v18,t2
vslidedown.vx v12,v20,t4
vslidedown.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslidedown.vx v8,v16,t0
vslidedown.vx v10,v18,t2
vslidedown.vx v12,v20,t4
vslidedown.vx v14,v22,t6
vslidedown.vx v8,v16,t0
vslidedown.vx v10,v18,t2
vslidedown.vx v12,v20,t4
vslidedown.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvxm_m2:
	m_mod_t0_vl
	li a0, WARMUP
1:

vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v14,v22,t6,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v14,v22,t6,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v10,v18,t2,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvi_m2:
	m_nop
	li a0, WARMUP
1:

vslidedown.vi v8,v16,3
vslidedown.vi v10,v18,3
vslidedown.vi v12,v20,3
vslidedown.vi v14,v22,3
vslidedown.vi v8,v16,3
vslidedown.vi v10,v18,3
vslidedown.vi v12,v20,3
vslidedown.vi v14,v22,3


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslidedown.vi v8,v16,3
vslidedown.vi v10,v18,3
vslidedown.vi v12,v20,3
vslidedown.vi v14,v22,3
vslidedown.vi v8,v16,3
vslidedown.vi v10,v18,3
vslidedown.vi v12,v20,3
vslidedown.vi v14,v22,3


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvim_m2:
	m_nop
	li a0, WARMUP
1:

vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v14,v22,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v14,v22,3,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v14,v22,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v10,v18,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v14,v22,3,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vredsumvs_m2:
	m_nop
	li a0, WARMUP
1:

vredsum.vs v8,v16,v24
vredsum.vs v10,v18,v26
vredsum.vs v12,v20,v28
vredsum.vs v14,v22,v30
vredsum.vs v8,v16,v24
vredsum.vs v10,v18,v26
vredsum.vs v12,v20,v28
vredsum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredsum.vs v8,v16,v24
vredsum.vs v10,v18,v26
vredsum.vs v12,v20,v28
vredsum.vs v14,v22,v30
vredsum.vs v8,v16,v24
vredsum.vs v10,v18,v26
vredsum.vs v12,v20,v28
vredsum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredsumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredsum.vs v8,v16,v24,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v14,v22,v30,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredsum.vs v8,v16,v24,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v14,v22,v30,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v10,v18,v26,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvs_m2:
	m_nop
	li a0, WARMUP
1:

vredand.vs v8,v16,v24
vredand.vs v10,v18,v26
vredand.vs v12,v20,v28
vredand.vs v14,v22,v30
vredand.vs v8,v16,v24
vredand.vs v10,v18,v26
vredand.vs v12,v20,v28
vredand.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredand.vs v8,v16,v24
vredand.vs v10,v18,v26
vredand.vs v12,v20,v28
vredand.vs v14,v22,v30
vredand.vs v8,v16,v24
vredand.vs v10,v18,v26
vredand.vs v12,v20,v28
vredand.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredand.vs v8,v16,v24,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v14,v22,v30,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredand.vs v8,v16,v24,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v14,v22,v30,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v10,v18,v26,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvs_m2:
	m_nop
	li a0, WARMUP
1:

vredor.vs v8,v16,v24
vredor.vs v10,v18,v26
vredor.vs v12,v20,v28
vredor.vs v14,v22,v30
vredor.vs v8,v16,v24
vredor.vs v10,v18,v26
vredor.vs v12,v20,v28
vredor.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredor.vs v8,v16,v24
vredor.vs v10,v18,v26
vredor.vs v12,v20,v28
vredor.vs v14,v22,v30
vredor.vs v8,v16,v24
vredor.vs v10,v18,v26
vredor.vs v12,v20,v28
vredor.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredor.vs v8,v16,v24,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v14,v22,v30,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredor.vs v8,v16,v24,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v14,v22,v30,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v10,v18,v26,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvs_m2:
	m_nop
	li a0, WARMUP
1:

vredxor.vs v8,v16,v24
vredxor.vs v10,v18,v26
vredxor.vs v12,v20,v28
vredxor.vs v14,v22,v30
vredxor.vs v8,v16,v24
vredxor.vs v10,v18,v26
vredxor.vs v12,v20,v28
vredxor.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredxor.vs v8,v16,v24
vredxor.vs v10,v18,v26
vredxor.vs v12,v20,v28
vredxor.vs v14,v22,v30
vredxor.vs v8,v16,v24
vredxor.vs v10,v18,v26
vredxor.vs v12,v20,v28
vredxor.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredxor.vs v8,v16,v24,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v14,v22,v30,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredxor.vs v8,v16,v24,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v14,v22,v30,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v10,v18,v26,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvs_m2:
	m_nop
	li a0, WARMUP
1:

vredminu.vs v8,v16,v24
vredminu.vs v10,v18,v26
vredminu.vs v12,v20,v28
vredminu.vs v14,v22,v30
vredminu.vs v8,v16,v24
vredminu.vs v10,v18,v26
vredminu.vs v12,v20,v28
vredminu.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredminu.vs v8,v16,v24
vredminu.vs v10,v18,v26
vredminu.vs v12,v20,v28
vredminu.vs v14,v22,v30
vredminu.vs v8,v16,v24
vredminu.vs v10,v18,v26
vredminu.vs v12,v20,v28
vredminu.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredminu.vs v8,v16,v24,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v14,v22,v30,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredminu.vs v8,v16,v24,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v14,v22,v30,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v10,v18,v26,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvs_m2:
	m_nop
	li a0, WARMUP
1:

vredmin.vs v8,v16,v24
vredmin.vs v10,v18,v26
vredmin.vs v12,v20,v28
vredmin.vs v14,v22,v30
vredmin.vs v8,v16,v24
vredmin.vs v10,v18,v26
vredmin.vs v12,v20,v28
vredmin.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmin.vs v8,v16,v24
vredmin.vs v10,v18,v26
vredmin.vs v12,v20,v28
vredmin.vs v14,v22,v30
vredmin.vs v8,v16,v24
vredmin.vs v10,v18,v26
vredmin.vs v12,v20,v28
vredmin.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredmin.vs v8,v16,v24,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v14,v22,v30,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmin.vs v8,v16,v24,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v14,v22,v30,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v10,v18,v26,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvs_m2:
	m_nop
	li a0, WARMUP
1:

vredmaxu.vs v8,v16,v24
vredmaxu.vs v10,v18,v26
vredmaxu.vs v12,v20,v28
vredmaxu.vs v14,v22,v30
vredmaxu.vs v8,v16,v24
vredmaxu.vs v10,v18,v26
vredmaxu.vs v12,v20,v28
vredmaxu.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmaxu.vs v8,v16,v24
vredmaxu.vs v10,v18,v26
vredmaxu.vs v12,v20,v28
vredmaxu.vs v14,v22,v30
vredmaxu.vs v8,v16,v24
vredmaxu.vs v10,v18,v26
vredmaxu.vs v12,v20,v28
vredmaxu.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v14,v22,v30,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v14,v22,v30,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v10,v18,v26,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvs_m2:
	m_nop
	li a0, WARMUP
1:

vredmax.vs v8,v16,v24
vredmax.vs v10,v18,v26
vredmax.vs v12,v20,v28
vredmax.vs v14,v22,v30
vredmax.vs v8,v16,v24
vredmax.vs v10,v18,v26
vredmax.vs v12,v20,v28
vredmax.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmax.vs v8,v16,v24
vredmax.vs v10,v18,v26
vredmax.vs v12,v20,v28
vredmax.vs v14,v22,v30
vredmax.vs v8,v16,v24
vredmax.vs v10,v18,v26
vredmax.vs v12,v20,v28
vredmax.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvsm_m2:
	m_nop
	li a0, WARMUP
1:

vredmax.vs v8,v16,v24,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v14,v22,v30,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vredmax.vs v8,v16,v24,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v14,v22,v30,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v10,v18,v26,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vaadduvv_m2:
	m_nop
	li a0, WARMUP
1:

vaaddu.vv v8,v16,v24
vaaddu.vv v10,v18,v26
vaaddu.vv v12,v20,v28
vaaddu.vv v14,v22,v30
vaaddu.vv v8,v16,v24
vaaddu.vv v10,v18,v26
vaaddu.vv v12,v20,v28
vaaddu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaaddu.vv v8,v16,v24
vaaddu.vv v10,v18,v26
vaaddu.vv v12,v20,v28
vaaddu.vv v14,v22,v30
vaaddu.vv v8,v16,v24
vaaddu.vv v10,v18,v26
vaaddu.vv v12,v20,v28
vaaddu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvvm_m2:
	m_nop
	li a0, WARMUP
1:

vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v14,v22,v30,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v14,v22,v30,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v10,v18,v26,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvx_m2:
	m_nop
	li a0, WARMUP
1:

vaaddu.vx v8,v16,t0
vaaddu.vx v10,v18,t2
vaaddu.vx v12,v20,t4
vaaddu.vx v14,v22,t6
vaaddu.vx v8,v16,t0
vaaddu.vx v10,v18,t2
vaaddu.vx v12,v20,t4
vaaddu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaaddu.vx v8,v16,t0
vaaddu.vx v10,v18,t2
vaaddu.vx v12,v20,t4
vaaddu.vx v14,v22,t6
vaaddu.vx v8,v16,t0
vaaddu.vx v10,v18,t2
vaaddu.vx v12,v20,t4
vaaddu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvxm_m2:
	m_nop
	li a0, WARMUP
1:

vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v14,v22,t6,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v14,v22,t6,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v10,v18,t2,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vaadd.vv v8,v16,v24
vaadd.vv v10,v18,v26
vaadd.vv v12,v20,v28
vaadd.vv v14,v22,v30
vaadd.vv v8,v16,v24
vaadd.vv v10,v18,v26
vaadd.vv v12,v20,v28
vaadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaadd.vv v8,v16,v24
vaadd.vv v10,v18,v26
vaadd.vv v12,v20,v28
vaadd.vv v14,v22,v30
vaadd.vv v8,v16,v24
vaadd.vv v10,v18,v26
vaadd.vv v12,v20,v28
vaadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vaadd.vv v8,v16,v24,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v14,v22,v30,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaadd.vv v8,v16,v24,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v14,v22,v30,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v10,v18,v26,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvx_m2:
	m_nop
	li a0, WARMUP
1:

vaadd.vx v8,v16,t0
vaadd.vx v10,v18,t2
vaadd.vx v12,v20,t4
vaadd.vx v14,v22,t6
vaadd.vx v8,v16,t0
vaadd.vx v10,v18,t2
vaadd.vx v12,v20,t4
vaadd.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaadd.vx v8,v16,t0
vaadd.vx v10,v18,t2
vaadd.vx v12,v20,t4
vaadd.vx v14,v22,t6
vaadd.vx v8,v16,t0
vaadd.vx v10,v18,t2
vaadd.vx v12,v20,t4
vaadd.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvxm_m2:
	m_nop
	li a0, WARMUP
1:

vaadd.vx v8,v16,t0,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v14,v22,t6,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vaadd.vx v8,v16,t0,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v14,v22,t6,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v10,v18,t2,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvv_m2:
	m_nop
	li a0, WARMUP
1:

vasubu.vv v8,v16,v24
vasubu.vv v10,v18,v26
vasubu.vv v12,v20,v28
vasubu.vv v14,v22,v30
vasubu.vv v8,v16,v24
vasubu.vv v10,v18,v26
vasubu.vv v12,v20,v28
vasubu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasubu.vv v8,v16,v24
vasubu.vv v10,v18,v26
vasubu.vv v12,v20,v28
vasubu.vv v14,v22,v30
vasubu.vv v8,v16,v24
vasubu.vv v10,v18,v26
vasubu.vv v12,v20,v28
vasubu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vasubu.vv v8,v16,v24,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v14,v22,v30,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasubu.vv v8,v16,v24,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v14,v22,v30,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v10,v18,v26,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvx_m2:
	m_nop
	li a0, WARMUP
1:

vasubu.vx v8,v16,t0
vasubu.vx v10,v18,t2
vasubu.vx v12,v20,t4
vasubu.vx v14,v22,t6
vasubu.vx v8,v16,t0
vasubu.vx v10,v18,t2
vasubu.vx v12,v20,t4
vasubu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasubu.vx v8,v16,t0
vasubu.vx v10,v18,t2
vasubu.vx v12,v20,t4
vasubu.vx v14,v22,t6
vasubu.vx v8,v16,t0
vasubu.vx v10,v18,t2
vasubu.vx v12,v20,t4
vasubu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vasubu.vx v8,v16,t0,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v14,v22,t6,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasubu.vx v8,v16,t0,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v14,v22,t6,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v10,v18,t2,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvv_m2:
	m_nop
	li a0, WARMUP
1:

vasub.vv v8,v16,v24
vasub.vv v10,v18,v26
vasub.vv v12,v20,v28
vasub.vv v14,v22,v30
vasub.vv v8,v16,v24
vasub.vv v10,v18,v26
vasub.vv v12,v20,v28
vasub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasub.vv v8,v16,v24
vasub.vv v10,v18,v26
vasub.vv v12,v20,v28
vasub.vv v14,v22,v30
vasub.vv v8,v16,v24
vasub.vv v10,v18,v26
vasub.vv v12,v20,v28
vasub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vasub.vv v8,v16,v24,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v14,v22,v30,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasub.vv v8,v16,v24,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v14,v22,v30,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v10,v18,v26,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvx_m2:
	m_nop
	li a0, WARMUP
1:

vasub.vx v8,v16,t0
vasub.vx v10,v18,t2
vasub.vx v12,v20,t4
vasub.vx v14,v22,t6
vasub.vx v8,v16,t0
vasub.vx v10,v18,t2
vasub.vx v12,v20,t4
vasub.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasub.vx v8,v16,t0
vasub.vx v10,v18,t2
vasub.vx v12,v20,t4
vasub.vx v14,v22,t6
vasub.vx v8,v16,t0
vasub.vx v10,v18,t2
vasub.vx v12,v20,t4
vasub.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vasub.vx v8,v16,t0,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v14,v22,t6,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vasub.vx v8,v16,t0,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v14,v22,t6,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v10,v18,t2,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslide1upvx_m2:
	m_nop
	li a0, WARMUP
1:

vslide1up.vx v8,v16,t0
vslide1up.vx v10,v18,t2
vslide1up.vx v12,v20,t4
vslide1up.vx v14,v22,t6
vslide1up.vx v8,v16,t0
vslide1up.vx v10,v18,t2
vslide1up.vx v12,v20,t4
vslide1up.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslide1up.vx v8,v16,t0
vslide1up.vx v10,v18,t2
vslide1up.vx v12,v20,t4
vslide1up.vx v14,v22,t6
vslide1up.vx v8,v16,t0
vslide1up.vx v10,v18,t2
vslide1up.vx v12,v20,t4
vslide1up.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1upvxm_m2:
	m_nop
	li a0, WARMUP
1:

vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v14,v22,t6,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v14,v22,t6,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v10,v18,t2,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvx_m2:
	m_nop
	li a0, WARMUP
1:

vslide1down.vx v8,v16,t0
vslide1down.vx v10,v18,t2
vslide1down.vx v12,v20,t4
vslide1down.vx v14,v22,t6
vslide1down.vx v8,v16,t0
vslide1down.vx v10,v18,t2
vslide1down.vx v12,v20,t4
vslide1down.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslide1down.vx v8,v16,t0
vslide1down.vx v10,v18,t2
vslide1down.vx v12,v20,t4
vslide1down.vx v14,v22,t6
vslide1down.vx v8,v16,t0
vslide1down.vx v10,v18,t2
vslide1down.vx v12,v20,t4
vslide1down.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvxm_m2:
	m_nop
	li a0, WARMUP
1:

vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v14,v22,t6,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v14,v22,t6,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v10,v18,t2,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vadcvvm_m2:
	m_nop
	li a0, WARMUP
1:

vadc.vvm v8,v16,v24,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v14,v22,v30,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v14,v22,v30,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadc.vvm v8,v16,v24,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v14,v22,v30,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v10,v18,v26,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v14,v22,v30,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvxm_m2:
	m_nop
	li a0, WARMUP
1:

vadc.vxm v8,v16,t0,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v14,v22,t6,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v14,v22,t6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadc.vxm v8,v16,t0,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v14,v22,t6,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v10,v18,t2,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v14,v22,t6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvim_m2:
	m_nop
	li a0, WARMUP
1:

vadc.vim v8,v16,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v14,v22,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v14,v22,13,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vadc.vim v8,v16,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v14,v22,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v10,v18,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v14,v22,13,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vvm v8,v16,v24,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v14,v22,v30,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v14,v22,v30,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vvm v8,v16,v24,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v14,v22,v30,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v10,v18,v26,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v14,v22,v30,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vxm v8,v16,t0,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v14,v22,t6,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v14,v22,t6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vxm v8,v16,t0,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v14,v22,t6,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v10,v18,t2,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v14,v22,t6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvim_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vim v8,v16,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v14,v22,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v14,v22,13,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vim v8,v16,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v14,v22,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v10,v18,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v14,v22,13,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvv_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vv v8,v16,v24
vmadc.vv v10,v18,v26
vmadc.vv v12,v20,v28
vmadc.vv v14,v22,v30
vmadc.vv v8,v16,v24
vmadc.vv v10,v18,v26
vmadc.vv v12,v20,v28
vmadc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vv v8,v16,v24
vmadc.vv v10,v18,v26
vmadc.vv v12,v20,v28
vmadc.vv v14,v22,v30
vmadc.vv v8,v16,v24
vmadc.vv v10,v18,v26
vmadc.vv v12,v20,v28
vmadc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvx_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vx v8,v16,t0
vmadc.vx v10,v18,t2
vmadc.vx v12,v20,t4
vmadc.vx v14,v22,t6
vmadc.vx v8,v16,t0
vmadc.vx v10,v18,t2
vmadc.vx v12,v20,t4
vmadc.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vx v8,v16,t0
vmadc.vx v10,v18,t2
vmadc.vx v12,v20,t4
vmadc.vx v14,v22,t6
vmadc.vx v8,v16,t0
vmadc.vx v10,v18,t2
vmadc.vx v12,v20,t4
vmadc.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvi_m2:
	m_nop
	li a0, WARMUP
1:

vmadc.vi v8,v16,13
vmadc.vi v10,v18,13
vmadc.vi v12,v20,13
vmadc.vi v14,v22,13
vmadc.vi v8,v16,13
vmadc.vi v10,v18,13
vmadc.vi v12,v20,13
vmadc.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadc.vi v8,v16,13
vmadc.vi v10,v18,13
vmadc.vi v12,v20,13
vmadc.vi v14,v22,13
vmadc.vi v8,v16,13
vmadc.vi v10,v18,13
vmadc.vi v12,v20,13
vmadc.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsbc.vvm v8,v16,v24,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v14,v22,v30,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v14,v22,v30,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsbc.vvm v8,v16,v24,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v14,v22,v30,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v10,v18,v26,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v14,v22,v30,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsbc.vxm v8,v16,t0,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v14,v22,t6,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v14,v22,t6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsbc.vxm v8,v16,t0,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v14,v22,t6,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v10,v18,t2,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v14,v22,t6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v14,v22,v30,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v14,v22,v30,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v14,v22,v30,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v10,v18,v26,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v14,v22,v30,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v14,v22,t6,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v14,v22,t6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v14,v22,t6,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v10,v18,t2,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v14,v22,t6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvv_m2:
	m_nop
	li a0, WARMUP
1:

vmsbc.vv v8,v16,v24
vmsbc.vv v10,v18,v26
vmsbc.vv v12,v20,v28
vmsbc.vv v14,v22,v30
vmsbc.vv v8,v16,v24
vmsbc.vv v10,v18,v26
vmsbc.vv v12,v20,v28
vmsbc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbc.vv v8,v16,v24
vmsbc.vv v10,v18,v26
vmsbc.vv v12,v20,v28
vmsbc.vv v14,v22,v30
vmsbc.vv v8,v16,v24
vmsbc.vv v10,v18,v26
vmsbc.vv v12,v20,v28
vmsbc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvx_m2:
	m_nop
	li a0, WARMUP
1:

vmsbc.vx v8,v16,t0
vmsbc.vx v10,v18,t2
vmsbc.vx v12,v20,t4
vmsbc.vx v14,v22,t6
vmsbc.vx v8,v16,t0
vmsbc.vx v10,v18,t2
vmsbc.vx v12,v20,t4
vmsbc.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbc.vx v8,v16,t0
vmsbc.vx v10,v18,t2
vmsbc.vx v12,v20,t4
vmsbc.vx v14,v22,t6
vmsbc.vx v8,v16,t0
vmsbc.vx v10,v18,t2
vmsbc.vx v12,v20,t4
vmsbc.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmergevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmerge.vvm v8,v16,v24,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v14,v22,v30,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v14,v22,v30,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmerge.vvm v8,v16,v24,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v14,v22,v30,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v10,v18,v26,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v14,v22,v30,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevxm_m2:
	m_nop
	li a0, WARMUP
1:

vmerge.vxm v8,v16,t0,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v14,v22,t6,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v14,v22,t6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmerge.vxm v8,v16,t0,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v14,v22,t6,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v10,v18,t2,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v14,v22,t6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevim_m2:
	m_nop
	li a0, WARMUP
1:

vmerge.vim v8,v16,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v14,v22,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v14,v22,13,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmerge.vim v8,v16,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v14,v22,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v10,v18,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v14,v22,13,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvv_m2:
	m_nop
	li a0, WARMUP
1:

vmv.v.v v8,v16
vmv.v.v v10,v18
vmv.v.v v12,v20
vmv.v.v v14,v22
vmv.v.v v8,v16
vmv.v.v v10,v18
vmv.v.v v12,v20
vmv.v.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv.v.v v8,v16
vmv.v.v v10,v18
vmv.v.v v12,v20
vmv.v.v v14,v22
vmv.v.v v8,v16
vmv.v.v v10,v18
vmv.v.v v12,v20
vmv.v.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvx_m2:
	m_nop
	li a0, WARMUP
1:

vmv.v.x v8,t0
vmv.v.x v10,t2
vmv.v.x v12,t4
vmv.v.x v14,t6
vmv.v.x v8,t0
vmv.v.x v10,t2
vmv.v.x v12,t4
vmv.v.x v14,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv.v.x v8,t0
vmv.v.x v10,t2
vmv.v.x v12,t4
vmv.v.x v14,t6
vmv.v.x v8,t0
vmv.v.x v10,t2
vmv.v.x v12,t4
vmv.v.x v14,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvi_m2:
	m_nop
	li a0, WARMUP
1:

vmv.v.i v8,13
vmv.v.i v10,13
vmv.v.i v12,13
vmv.v.i v14,13
vmv.v.i v8,13
vmv.v.i v10,13
vmv.v.i v12,13
vmv.v.i v14,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv.v.i v8,13
vmv.v.i v10,13
vmv.v.i v12,13
vmv.v.i v14,13
vmv.v.i v8,13
vmv.v.i v10,13
vmv.v.i v12,13
vmv.v.i v14,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvv_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vv v8,v16,v24
vmseq.vv v10,v18,v26
vmseq.vv v12,v20,v28
vmseq.vv v14,v22,v30
vmseq.vv v8,v16,v24
vmseq.vv v10,v18,v26
vmseq.vv v12,v20,v28
vmseq.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vv v8,v16,v24
vmseq.vv v10,v18,v26
vmseq.vv v12,v20,v28
vmseq.vv v14,v22,v30
vmseq.vv v8,v16,v24
vmseq.vv v10,v18,v26
vmseq.vv v12,v20,v28
vmseq.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vv v8,v16,v24,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v14,v22,v30,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vv v8,v16,v24,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v14,v22,v30,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v10,v18,v26,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvx_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vx v8,v16,t0
vmseq.vx v10,v18,t2
vmseq.vx v12,v20,t4
vmseq.vx v14,v22,t6
vmseq.vx v8,v16,t0
vmseq.vx v10,v18,t2
vmseq.vx v12,v20,t4
vmseq.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vx v8,v16,t0
vmseq.vx v10,v18,t2
vmseq.vx v12,v20,t4
vmseq.vx v14,v22,t6
vmseq.vx v8,v16,t0
vmseq.vx v10,v18,t2
vmseq.vx v12,v20,t4
vmseq.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vx v8,v16,t0,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v14,v22,t6,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vx v8,v16,t0,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v14,v22,t6,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v10,v18,t2,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvi_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vi v8,v16,13
vmseq.vi v10,v18,13
vmseq.vi v12,v20,13
vmseq.vi v14,v22,13
vmseq.vi v8,v16,13
vmseq.vi v10,v18,13
vmseq.vi v12,v20,13
vmseq.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vi v8,v16,13
vmseq.vi v10,v18,13
vmseq.vi v12,v20,13
vmseq.vi v14,v22,13
vmseq.vi v8,v16,13
vmseq.vi v10,v18,13
vmseq.vi v12,v20,13
vmseq.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvim_m2:
	m_nop
	li a0, WARMUP
1:

vmseq.vi v8,v16,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v14,v22,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmseq.vi v8,v16,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v14,v22,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v10,v18,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevv_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vv v8,v16,v24
vmsne.vv v10,v18,v26
vmsne.vv v12,v20,v28
vmsne.vv v14,v22,v30
vmsne.vv v8,v16,v24
vmsne.vv v10,v18,v26
vmsne.vv v12,v20,v28
vmsne.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vv v8,v16,v24
vmsne.vv v10,v18,v26
vmsne.vv v12,v20,v28
vmsne.vv v14,v22,v30
vmsne.vv v8,v16,v24
vmsne.vv v10,v18,v26
vmsne.vv v12,v20,v28
vmsne.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vv v8,v16,v24,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v14,v22,v30,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vv v8,v16,v24,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v14,v22,v30,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v10,v18,v26,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevx_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vx v8,v16,t0
vmsne.vx v10,v18,t2
vmsne.vx v12,v20,t4
vmsne.vx v14,v22,t6
vmsne.vx v8,v16,t0
vmsne.vx v10,v18,t2
vmsne.vx v12,v20,t4
vmsne.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vx v8,v16,t0
vmsne.vx v10,v18,t2
vmsne.vx v12,v20,t4
vmsne.vx v14,v22,t6
vmsne.vx v8,v16,t0
vmsne.vx v10,v18,t2
vmsne.vx v12,v20,t4
vmsne.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vx v8,v16,t0,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v14,v22,t6,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vx v8,v16,t0,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v14,v22,t6,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v10,v18,t2,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevi_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vi v8,v16,13
vmsne.vi v10,v18,13
vmsne.vi v12,v20,13
vmsne.vi v14,v22,13
vmsne.vi v8,v16,13
vmsne.vi v10,v18,13
vmsne.vi v12,v20,13
vmsne.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vi v8,v16,13
vmsne.vi v10,v18,13
vmsne.vi v12,v20,13
vmsne.vi v14,v22,13
vmsne.vi v8,v16,13
vmsne.vi v10,v18,13
vmsne.vi v12,v20,13
vmsne.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevim_m2:
	m_nop
	li a0, WARMUP
1:

vmsne.vi v8,v16,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v14,v22,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsne.vi v8,v16,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v14,v22,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v10,v18,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvv_m2:
	m_nop
	li a0, WARMUP
1:

vmsltu.vv v8,v16,v24
vmsltu.vv v10,v18,v26
vmsltu.vv v12,v20,v28
vmsltu.vv v14,v22,v30
vmsltu.vv v8,v16,v24
vmsltu.vv v10,v18,v26
vmsltu.vv v12,v20,v28
vmsltu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsltu.vv v8,v16,v24
vmsltu.vv v10,v18,v26
vmsltu.vv v12,v20,v28
vmsltu.vv v14,v22,v30
vmsltu.vv v8,v16,v24
vmsltu.vv v10,v18,v26
vmsltu.vv v12,v20,v28
vmsltu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v14,v22,v30,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v14,v22,v30,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v10,v18,v26,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmsltu.vx v8,v16,t0
vmsltu.vx v10,v18,t2
vmsltu.vx v12,v20,t4
vmsltu.vx v14,v22,t6
vmsltu.vx v8,v16,t0
vmsltu.vx v10,v18,t2
vmsltu.vx v12,v20,t4
vmsltu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsltu.vx v8,v16,t0
vmsltu.vx v10,v18,t2
vmsltu.vx v12,v20,t4
vmsltu.vx v14,v22,t6
vmsltu.vx v8,v16,t0
vmsltu.vx v10,v18,t2
vmsltu.vx v12,v20,t4
vmsltu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v14,v22,t6,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v14,v22,t6,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v10,v18,t2,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvv_m2:
	m_nop
	li a0, WARMUP
1:

vmslt.vv v8,v16,v24
vmslt.vv v10,v18,v26
vmslt.vv v12,v20,v28
vmslt.vv v14,v22,v30
vmslt.vv v8,v16,v24
vmslt.vv v10,v18,v26
vmslt.vv v12,v20,v28
vmslt.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmslt.vv v8,v16,v24
vmslt.vv v10,v18,v26
vmslt.vv v12,v20,v28
vmslt.vv v14,v22,v30
vmslt.vv v8,v16,v24
vmslt.vv v10,v18,v26
vmslt.vv v12,v20,v28
vmslt.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmslt.vv v8,v16,v24,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v14,v22,v30,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmslt.vv v8,v16,v24,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v14,v22,v30,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v10,v18,v26,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvx_m2:
	m_nop
	li a0, WARMUP
1:

vmslt.vx v8,v16,t0
vmslt.vx v10,v18,t2
vmslt.vx v12,v20,t4
vmslt.vx v14,v22,t6
vmslt.vx v8,v16,t0
vmslt.vx v10,v18,t2
vmslt.vx v12,v20,t4
vmslt.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmslt.vx v8,v16,t0
vmslt.vx v10,v18,t2
vmslt.vx v12,v20,t4
vmslt.vx v14,v22,t6
vmslt.vx v8,v16,t0
vmslt.vx v10,v18,t2
vmslt.vx v12,v20,t4
vmslt.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmslt.vx v8,v16,t0,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v14,v22,t6,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmslt.vx v8,v16,t0,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v14,v22,t6,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v10,v18,t2,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvv_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vv v8,v16,v24
vmsleu.vv v10,v18,v26
vmsleu.vv v12,v20,v28
vmsleu.vv v14,v22,v30
vmsleu.vv v8,v16,v24
vmsleu.vv v10,v18,v26
vmsleu.vv v12,v20,v28
vmsleu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vv v8,v16,v24
vmsleu.vv v10,v18,v26
vmsleu.vv v12,v20,v28
vmsleu.vv v14,v22,v30
vmsleu.vv v8,v16,v24
vmsleu.vv v10,v18,v26
vmsleu.vv v12,v20,v28
vmsleu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v14,v22,v30,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v14,v22,v30,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v10,v18,v26,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vx v8,v16,t0
vmsleu.vx v10,v18,t2
vmsleu.vx v12,v20,t4
vmsleu.vx v14,v22,t6
vmsleu.vx v8,v16,t0
vmsleu.vx v10,v18,t2
vmsleu.vx v12,v20,t4
vmsleu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vx v8,v16,t0
vmsleu.vx v10,v18,t2
vmsleu.vx v12,v20,t4
vmsleu.vx v14,v22,t6
vmsleu.vx v8,v16,t0
vmsleu.vx v10,v18,t2
vmsleu.vx v12,v20,t4
vmsleu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v14,v22,t6,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v14,v22,t6,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v10,v18,t2,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvi_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vi v8,v16,13
vmsleu.vi v10,v18,13
vmsleu.vi v12,v20,13
vmsleu.vi v14,v22,13
vmsleu.vi v8,v16,13
vmsleu.vi v10,v18,13
vmsleu.vi v12,v20,13
vmsleu.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vi v8,v16,13
vmsleu.vi v10,v18,13
vmsleu.vi v12,v20,13
vmsleu.vi v14,v22,13
vmsleu.vi v8,v16,13
vmsleu.vi v10,v18,13
vmsleu.vi v12,v20,13
vmsleu.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvim_m2:
	m_nop
	li a0, WARMUP
1:

vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v14,v22,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v14,v22,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v10,v18,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevv_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vv v8,v16,v24
vmsle.vv v10,v18,v26
vmsle.vv v12,v20,v28
vmsle.vv v14,v22,v30
vmsle.vv v8,v16,v24
vmsle.vv v10,v18,v26
vmsle.vv v12,v20,v28
vmsle.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vv v8,v16,v24
vmsle.vv v10,v18,v26
vmsle.vv v12,v20,v28
vmsle.vv v14,v22,v30
vmsle.vv v8,v16,v24
vmsle.vv v10,v18,v26
vmsle.vv v12,v20,v28
vmsle.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vv v8,v16,v24,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v14,v22,v30,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vv v8,v16,v24,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v14,v22,v30,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v10,v18,v26,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevx_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vx v8,v16,t0
vmsle.vx v10,v18,t2
vmsle.vx v12,v20,t4
vmsle.vx v14,v22,t6
vmsle.vx v8,v16,t0
vmsle.vx v10,v18,t2
vmsle.vx v12,v20,t4
vmsle.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vx v8,v16,t0
vmsle.vx v10,v18,t2
vmsle.vx v12,v20,t4
vmsle.vx v14,v22,t6
vmsle.vx v8,v16,t0
vmsle.vx v10,v18,t2
vmsle.vx v12,v20,t4
vmsle.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vx v8,v16,t0,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v14,v22,t6,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vx v8,v16,t0,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v14,v22,t6,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v10,v18,t2,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevi_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vi v8,v16,13
vmsle.vi v10,v18,13
vmsle.vi v12,v20,13
vmsle.vi v14,v22,13
vmsle.vi v8,v16,13
vmsle.vi v10,v18,13
vmsle.vi v12,v20,13
vmsle.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vi v8,v16,13
vmsle.vi v10,v18,13
vmsle.vi v12,v20,13
vmsle.vi v14,v22,13
vmsle.vi v8,v16,13
vmsle.vi v10,v18,13
vmsle.vi v12,v20,13
vmsle.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevim_m2:
	m_nop
	li a0, WARMUP
1:

vmsle.vi v8,v16,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v14,v22,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsle.vi v8,v16,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v14,v22,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v10,v18,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmsgtu.vx v8,v16,t0
vmsgtu.vx v10,v18,t2
vmsgtu.vx v12,v20,t4
vmsgtu.vx v14,v22,t6
vmsgtu.vx v8,v16,t0
vmsgtu.vx v10,v18,t2
vmsgtu.vx v12,v20,t4
vmsgtu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgtu.vx v8,v16,t0
vmsgtu.vx v10,v18,t2
vmsgtu.vx v12,v20,t4
vmsgtu.vx v14,v22,t6
vmsgtu.vx v8,v16,t0
vmsgtu.vx v10,v18,t2
vmsgtu.vx v12,v20,t4
vmsgtu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v14,v22,t6,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v14,v22,t6,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v10,v18,t2,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvi_m2:
	m_nop
	li a0, WARMUP
1:

vmsgtu.vi v8,v16,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v14,v22,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgtu.vi v8,v16,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v14,v22,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v10,v18,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvim_m2:
	m_nop
	li a0, WARMUP
1:

vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v14,v22,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v14,v22,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v10,v18,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvx_m2:
	m_nop
	li a0, WARMUP
1:

vmsgt.vx v8,v16,t0
vmsgt.vx v10,v18,t2
vmsgt.vx v12,v20,t4
vmsgt.vx v14,v22,t6
vmsgt.vx v8,v16,t0
vmsgt.vx v10,v18,t2
vmsgt.vx v12,v20,t4
vmsgt.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgt.vx v8,v16,t0
vmsgt.vx v10,v18,t2
vmsgt.vx v12,v20,t4
vmsgt.vx v14,v22,t6
vmsgt.vx v8,v16,t0
vmsgt.vx v10,v18,t2
vmsgt.vx v12,v20,t4
vmsgt.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v14,v22,t6,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v14,v22,t6,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v10,v18,t2,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvi_m2:
	m_nop
	li a0, WARMUP
1:

vmsgt.vi v8,v16,13
vmsgt.vi v10,v18,13
vmsgt.vi v12,v20,13
vmsgt.vi v14,v22,13
vmsgt.vi v8,v16,13
vmsgt.vi v10,v18,13
vmsgt.vi v12,v20,13
vmsgt.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgt.vi v8,v16,13
vmsgt.vi v10,v18,13
vmsgt.vi v12,v20,13
vmsgt.vi v14,v22,13
vmsgt.vi v8,v16,13
vmsgt.vi v10,v18,13
vmsgt.vi v12,v20,13
vmsgt.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvim_m2:
	m_nop
	li a0, WARMUP
1:

vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v14,v22,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v14,v22,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v10,v18,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcompressvm_m2:
	m_nop
	li a0, WARMUP
1:

vcompress.vm v8,v16,v24
vcompress.vm v10,v18,v26
vcompress.vm v12,v20,v28
vcompress.vm v14,v22,v30
vcompress.vm v8,v16,v24
vcompress.vm v10,v18,v26
vcompress.vm v12,v20,v28
vcompress.vm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vcompress.vm v8,v16,v24
vcompress.vm v10,v18,v26
vcompress.vm v12,v20,v28
vcompress.vm v14,v22,v30
vcompress.vm v8,v16,v24
vcompress.vm v10,v18,v26
vcompress.vm v12,v20,v28
vcompress.vm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmandnmm_m2:
	m_nop
	li a0, WARMUP
1:

vmandn.mm v8,v16,v24
vmandn.mm v10,v18,v26
vmandn.mm v12,v20,v28
vmandn.mm v14,v22,v30
vmandn.mm v8,v16,v24
vmandn.mm v10,v18,v26
vmandn.mm v12,v20,v28
vmandn.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmandn.mm v8,v16,v24
vmandn.mm v10,v18,v26
vmandn.mm v12,v20,v28
vmandn.mm v14,v22,v30
vmandn.mm v8,v16,v24
vmandn.mm v10,v18,v26
vmandn.mm v12,v20,v28
vmandn.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmandmm_m2:
	m_nop
	li a0, WARMUP
1:

vmand.mm v8,v16,v24
vmand.mm v10,v18,v26
vmand.mm v12,v20,v28
vmand.mm v14,v22,v30
vmand.mm v8,v16,v24
vmand.mm v10,v18,v26
vmand.mm v12,v20,v28
vmand.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmand.mm v8,v16,v24
vmand.mm v10,v18,v26
vmand.mm v12,v20,v28
vmand.mm v14,v22,v30
vmand.mm v8,v16,v24
vmand.mm v10,v18,v26
vmand.mm v12,v20,v28
vmand.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmormm_m2:
	m_nop
	li a0, WARMUP
1:

vmor.mm v8,v16,v24
vmor.mm v10,v18,v26
vmor.mm v12,v20,v28
vmor.mm v14,v22,v30
vmor.mm v8,v16,v24
vmor.mm v10,v18,v26
vmor.mm v12,v20,v28
vmor.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmor.mm v8,v16,v24
vmor.mm v10,v18,v26
vmor.mm v12,v20,v28
vmor.mm v14,v22,v30
vmor.mm v8,v16,v24
vmor.mm v10,v18,v26
vmor.mm v12,v20,v28
vmor.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxormm_m2:
	m_nop
	li a0, WARMUP
1:

vmxor.mm v8,v16,v24
vmxor.mm v10,v18,v26
vmxor.mm v12,v20,v28
vmxor.mm v14,v22,v30
vmxor.mm v8,v16,v24
vmxor.mm v10,v18,v26
vmxor.mm v12,v20,v28
vmxor.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmxor.mm v8,v16,v24
vmxor.mm v10,v18,v26
vmxor.mm v12,v20,v28
vmxor.mm v14,v22,v30
vmxor.mm v8,v16,v24
vmxor.mm v10,v18,v26
vmxor.mm v12,v20,v28
vmxor.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmornmm_m2:
	m_nop
	li a0, WARMUP
1:

vmorn.mm v8,v16,v24
vmorn.mm v10,v18,v26
vmorn.mm v12,v20,v28
vmorn.mm v14,v22,v30
vmorn.mm v8,v16,v24
vmorn.mm v10,v18,v26
vmorn.mm v12,v20,v28
vmorn.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmorn.mm v8,v16,v24
vmorn.mm v10,v18,v26
vmorn.mm v12,v20,v28
vmorn.mm v14,v22,v30
vmorn.mm v8,v16,v24
vmorn.mm v10,v18,v26
vmorn.mm v12,v20,v28
vmorn.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnandmm_m2:
	m_nop
	li a0, WARMUP
1:

vmnand.mm v8,v16,v24
vmnand.mm v10,v18,v26
vmnand.mm v12,v20,v28
vmnand.mm v14,v22,v30
vmnand.mm v8,v16,v24
vmnand.mm v10,v18,v26
vmnand.mm v12,v20,v28
vmnand.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmnand.mm v8,v16,v24
vmnand.mm v10,v18,v26
vmnand.mm v12,v20,v28
vmnand.mm v14,v22,v30
vmnand.mm v8,v16,v24
vmnand.mm v10,v18,v26
vmnand.mm v12,v20,v28
vmnand.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnormm_m2:
	m_nop
	li a0, WARMUP
1:

vmnor.mm v8,v16,v24
vmnor.mm v10,v18,v26
vmnor.mm v12,v20,v28
vmnor.mm v14,v22,v30
vmnor.mm v8,v16,v24
vmnor.mm v10,v18,v26
vmnor.mm v12,v20,v28
vmnor.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmnor.mm v8,v16,v24
vmnor.mm v10,v18,v26
vmnor.mm v12,v20,v28
vmnor.mm v14,v22,v30
vmnor.mm v8,v16,v24
vmnor.mm v10,v18,v26
vmnor.mm v12,v20,v28
vmnor.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxnormm_m2:
	m_nop
	li a0, WARMUP
1:

vmxnor.mm v8,v16,v24
vmxnor.mm v10,v18,v26
vmxnor.mm v12,v20,v28
vmxnor.mm v14,v22,v30
vmxnor.mm v8,v16,v24
vmxnor.mm v10,v18,v26
vmxnor.mm v12,v20,v28
vmxnor.mm v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmxnor.mm v8,v16,v24
vmxnor.mm v10,v18,v26
vmxnor.mm v12,v20,v28
vmxnor.mm v14,v22,v30
vmxnor.mm v8,v16,v24
vmxnor.mm v10,v18,v26
vmxnor.mm v12,v20,v28
vmxnor.mm v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vsadduvv_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vv v8,v16,v24
vsaddu.vv v10,v18,v26
vsaddu.vv v12,v20,v28
vsaddu.vv v14,v22,v30
vsaddu.vv v8,v16,v24
vsaddu.vv v10,v18,v26
vsaddu.vv v12,v20,v28
vsaddu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vv v8,v16,v24
vsaddu.vv v10,v18,v26
vsaddu.vv v12,v20,v28
vsaddu.vv v14,v22,v30
vsaddu.vv v8,v16,v24
vsaddu.vv v10,v18,v26
vsaddu.vv v12,v20,v28
vsaddu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v14,v22,v30,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v14,v22,v30,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v10,v18,v26,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvx_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vx v8,v16,t0
vsaddu.vx v10,v18,t2
vsaddu.vx v12,v20,t4
vsaddu.vx v14,v22,t6
vsaddu.vx v8,v16,t0
vsaddu.vx v10,v18,t2
vsaddu.vx v12,v20,t4
vsaddu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vx v8,v16,t0
vsaddu.vx v10,v18,t2
vsaddu.vx v12,v20,t4
vsaddu.vx v14,v22,t6
vsaddu.vx v8,v16,t0
vsaddu.vx v10,v18,t2
vsaddu.vx v12,v20,t4
vsaddu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v14,v22,t6,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v14,v22,t6,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v10,v18,t2,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvi_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vi v8,v16,13
vsaddu.vi v10,v18,13
vsaddu.vi v12,v20,13
vsaddu.vi v14,v22,13
vsaddu.vi v8,v16,13
vsaddu.vi v10,v18,13
vsaddu.vi v12,v20,13
vsaddu.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vi v8,v16,13
vsaddu.vi v10,v18,13
vsaddu.vi v12,v20,13
vsaddu.vi v14,v22,13
vsaddu.vi v8,v16,13
vsaddu.vi v10,v18,13
vsaddu.vi v12,v20,13
vsaddu.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvim_m2:
	m_nop
	li a0, WARMUP
1:

vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v14,v22,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v14,v22,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v10,v18,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vv v8,v16,v24
vsadd.vv v10,v18,v26
vsadd.vv v12,v20,v28
vsadd.vv v14,v22,v30
vsadd.vv v8,v16,v24
vsadd.vv v10,v18,v26
vsadd.vv v12,v20,v28
vsadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vv v8,v16,v24
vsadd.vv v10,v18,v26
vsadd.vv v12,v20,v28
vsadd.vv v14,v22,v30
vsadd.vv v8,v16,v24
vsadd.vv v10,v18,v26
vsadd.vv v12,v20,v28
vsadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vv v8,v16,v24,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v14,v22,v30,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vv v8,v16,v24,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v14,v22,v30,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v10,v18,v26,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvx_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vx v8,v16,t0
vsadd.vx v10,v18,t2
vsadd.vx v12,v20,t4
vsadd.vx v14,v22,t6
vsadd.vx v8,v16,t0
vsadd.vx v10,v18,t2
vsadd.vx v12,v20,t4
vsadd.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vx v8,v16,t0
vsadd.vx v10,v18,t2
vsadd.vx v12,v20,t4
vsadd.vx v14,v22,t6
vsadd.vx v8,v16,t0
vsadd.vx v10,v18,t2
vsadd.vx v12,v20,t4
vsadd.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vx v8,v16,t0,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v14,v22,t6,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vx v8,v16,t0,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v14,v22,t6,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v10,v18,t2,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvi_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vi v8,v16,13
vsadd.vi v10,v18,13
vsadd.vi v12,v20,13
vsadd.vi v14,v22,13
vsadd.vi v8,v16,13
vsadd.vi v10,v18,13
vsadd.vi v12,v20,13
vsadd.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vi v8,v16,13
vsadd.vi v10,v18,13
vsadd.vi v12,v20,13
vsadd.vi v14,v22,13
vsadd.vi v8,v16,13
vsadd.vi v10,v18,13
vsadd.vi v12,v20,13
vsadd.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvim_m2:
	m_nop
	li a0, WARMUP
1:

vsadd.vi v8,v16,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v14,v22,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsadd.vi v8,v16,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v14,v22,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v10,v18,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvv_m2:
	m_nop
	li a0, WARMUP
1:

vssubu.vv v8,v16,v24
vssubu.vv v10,v18,v26
vssubu.vv v12,v20,v28
vssubu.vv v14,v22,v30
vssubu.vv v8,v16,v24
vssubu.vv v10,v18,v26
vssubu.vv v12,v20,v28
vssubu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssubu.vv v8,v16,v24
vssubu.vv v10,v18,v26
vssubu.vv v12,v20,v28
vssubu.vv v14,v22,v30
vssubu.vv v8,v16,v24
vssubu.vv v10,v18,v26
vssubu.vv v12,v20,v28
vssubu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vssubu.vv v8,v16,v24,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v14,v22,v30,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssubu.vv v8,v16,v24,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v14,v22,v30,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v10,v18,v26,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvx_m2:
	m_nop
	li a0, WARMUP
1:

vssubu.vx v8,v16,t0
vssubu.vx v10,v18,t2
vssubu.vx v12,v20,t4
vssubu.vx v14,v22,t6
vssubu.vx v8,v16,t0
vssubu.vx v10,v18,t2
vssubu.vx v12,v20,t4
vssubu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssubu.vx v8,v16,t0
vssubu.vx v10,v18,t2
vssubu.vx v12,v20,t4
vssubu.vx v14,v22,t6
vssubu.vx v8,v16,t0
vssubu.vx v10,v18,t2
vssubu.vx v12,v20,t4
vssubu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vssubu.vx v8,v16,t0,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v14,v22,t6,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssubu.vx v8,v16,t0,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v14,v22,t6,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v10,v18,t2,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvv_m2:
	m_nop
	li a0, WARMUP
1:

vssub.vv v8,v16,v24
vssub.vv v10,v18,v26
vssub.vv v12,v20,v28
vssub.vv v14,v22,v30
vssub.vv v8,v16,v24
vssub.vv v10,v18,v26
vssub.vv v12,v20,v28
vssub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssub.vv v8,v16,v24
vssub.vv v10,v18,v26
vssub.vv v12,v20,v28
vssub.vv v14,v22,v30
vssub.vv v8,v16,v24
vssub.vv v10,v18,v26
vssub.vv v12,v20,v28
vssub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vssub.vv v8,v16,v24,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v14,v22,v30,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssub.vv v8,v16,v24,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v14,v22,v30,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v10,v18,v26,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvx_m2:
	m_nop
	li a0, WARMUP
1:

vssub.vx v8,v16,t0
vssub.vx v10,v18,t2
vssub.vx v12,v20,t4
vssub.vx v14,v22,t6
vssub.vx v8,v16,t0
vssub.vx v10,v18,t2
vssub.vx v12,v20,t4
vssub.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssub.vx v8,v16,t0
vssub.vx v10,v18,t2
vssub.vx v12,v20,t4
vssub.vx v14,v22,t6
vssub.vx v8,v16,t0
vssub.vx v10,v18,t2
vssub.vx v12,v20,t4
vssub.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vssub.vx v8,v16,t0,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v14,v22,t6,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssub.vx v8,v16,t0,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v14,v22,t6,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v10,v18,t2,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvv_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vv v8,v16,v24
vsll.vv v10,v18,v26
vsll.vv v12,v20,v28
vsll.vv v14,v22,v30
vsll.vv v8,v16,v24
vsll.vv v10,v18,v26
vsll.vv v12,v20,v28
vsll.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vv v8,v16,v24
vsll.vv v10,v18,v26
vsll.vv v12,v20,v28
vsll.vv v14,v22,v30
vsll.vv v8,v16,v24
vsll.vv v10,v18,v26
vsll.vv v12,v20,v28
vsll.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vv v8,v16,v24,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v14,v22,v30,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vv v8,v16,v24,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v14,v22,v30,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v10,v18,v26,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvx_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vx v8,v16,t0
vsll.vx v10,v18,t2
vsll.vx v12,v20,t4
vsll.vx v14,v22,t6
vsll.vx v8,v16,t0
vsll.vx v10,v18,t2
vsll.vx v12,v20,t4
vsll.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vx v8,v16,t0
vsll.vx v10,v18,t2
vsll.vx v12,v20,t4
vsll.vx v14,v22,t6
vsll.vx v8,v16,t0
vsll.vx v10,v18,t2
vsll.vx v12,v20,t4
vsll.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vx v8,v16,t0,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v14,v22,t6,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vx v8,v16,t0,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v14,v22,t6,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v10,v18,t2,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvi_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vi v8,v16,13
vsll.vi v10,v18,13
vsll.vi v12,v20,13
vsll.vi v14,v22,13
vsll.vi v8,v16,13
vsll.vi v10,v18,13
vsll.vi v12,v20,13
vsll.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vi v8,v16,13
vsll.vi v10,v18,13
vsll.vi v12,v20,13
vsll.vi v14,v22,13
vsll.vi v8,v16,13
vsll.vi v10,v18,13
vsll.vi v12,v20,13
vsll.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvim_m2:
	m_nop
	li a0, WARMUP
1:

vsll.vi v8,v16,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v14,v22,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsll.vi v8,v16,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v14,v22,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v10,v18,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvv_m2:
	m_nop
	li a0, WARMUP
1:

vsmul.vv v8,v16,v24
vsmul.vv v10,v18,v26
vsmul.vv v12,v20,v28
vsmul.vv v14,v22,v30
vsmul.vv v8,v16,v24
vsmul.vv v10,v18,v26
vsmul.vv v12,v20,v28
vsmul.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsmul.vv v8,v16,v24
vsmul.vv v10,v18,v26
vsmul.vv v12,v20,v28
vsmul.vv v14,v22,v30
vsmul.vv v8,v16,v24
vsmul.vv v10,v18,v26
vsmul.vv v12,v20,v28
vsmul.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsmul.vv v8,v16,v24,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v14,v22,v30,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsmul.vv v8,v16,v24,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v14,v22,v30,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v10,v18,v26,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvx_m2:
	m_nop
	li a0, WARMUP
1:

vsmul.vx v8,v16,t0
vsmul.vx v10,v18,t2
vsmul.vx v12,v20,t4
vsmul.vx v14,v22,t6
vsmul.vx v8,v16,t0
vsmul.vx v10,v18,t2
vsmul.vx v12,v20,t4
vsmul.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsmul.vx v8,v16,t0
vsmul.vx v10,v18,t2
vsmul.vx v12,v20,t4
vsmul.vx v14,v22,t6
vsmul.vx v8,v16,t0
vsmul.vx v10,v18,t2
vsmul.vx v12,v20,t4
vsmul.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsmul.vx v8,v16,t0,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v14,v22,t6,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsmul.vx v8,v16,t0,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v14,v22,t6,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v10,v18,t2,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv1rv_m2:
	m_nop
	li a0, WARMUP
1:

vmv1r.v v8,v16
vmv1r.v v10,v18
vmv1r.v v12,v20
vmv1r.v v14,v22
vmv1r.v v8,v16
vmv1r.v v10,v18
vmv1r.v v12,v20
vmv1r.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv1r.v v8,v16
vmv1r.v v10,v18
vmv1r.v v12,v20
vmv1r.v v14,v22
vmv1r.v v8,v16
vmv1r.v v10,v18
vmv1r.v v12,v20
vmv1r.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv2rv_m2:
	m_nop
	li a0, WARMUP
1:

vmv2r.v v8,v16
vmv2r.v v10,v18
vmv2r.v v12,v20
vmv2r.v v14,v22
vmv2r.v v8,v16
vmv2r.v v10,v18
vmv2r.v v12,v20
vmv2r.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv2r.v v8,v16
vmv2r.v v10,v18
vmv2r.v v12,v20
vmv2r.v v14,v22
vmv2r.v v8,v16
vmv2r.v v10,v18
vmv2r.v v12,v20
vmv2r.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv4rv_m2:
	m_nop
	li a0, WARMUP
1:

m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv8rv_m2:
	m_nop
	li a0, WARMUP
1:

m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvv_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vv v8,v16,v24
vsrl.vv v10,v18,v26
vsrl.vv v12,v20,v28
vsrl.vv v14,v22,v30
vsrl.vv v8,v16,v24
vsrl.vv v10,v18,v26
vsrl.vv v12,v20,v28
vsrl.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vv v8,v16,v24
vsrl.vv v10,v18,v26
vsrl.vv v12,v20,v28
vsrl.vv v14,v22,v30
vsrl.vv v8,v16,v24
vsrl.vv v10,v18,v26
vsrl.vv v12,v20,v28
vsrl.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvvm_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vv v8,v16,v24,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v14,v22,v30,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vv v8,v16,v24,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v14,v22,v30,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v10,v18,v26,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvx_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vx v8,v16,t0
vsrl.vx v10,v18,t2
vsrl.vx v12,v20,t4
vsrl.vx v14,v22,t6
vsrl.vx v8,v16,t0
vsrl.vx v10,v18,t2
vsrl.vx v12,v20,t4
vsrl.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vx v8,v16,t0
vsrl.vx v10,v18,t2
vsrl.vx v12,v20,t4
vsrl.vx v14,v22,t6
vsrl.vx v8,v16,t0
vsrl.vx v10,v18,t2
vsrl.vx v12,v20,t4
vsrl.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvxm_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vx v8,v16,t0,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v14,v22,t6,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vx v8,v16,t0,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v14,v22,t6,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v10,v18,t2,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvi_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vi v8,v16,13
vsrl.vi v10,v18,13
vsrl.vi v12,v20,13
vsrl.vi v14,v22,13
vsrl.vi v8,v16,13
vsrl.vi v10,v18,13
vsrl.vi v12,v20,13
vsrl.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vi v8,v16,13
vsrl.vi v10,v18,13
vsrl.vi v12,v20,13
vsrl.vi v14,v22,13
vsrl.vi v8,v16,13
vsrl.vi v10,v18,13
vsrl.vi v12,v20,13
vsrl.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvim_m2:
	m_nop
	li a0, WARMUP
1:

vsrl.vi v8,v16,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v14,v22,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsrl.vi v8,v16,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v14,v22,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v10,v18,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravv_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vv v8,v16,v24
vsra.vv v10,v18,v26
vsra.vv v12,v20,v28
vsra.vv v14,v22,v30
vsra.vv v8,v16,v24
vsra.vv v10,v18,v26
vsra.vv v12,v20,v28
vsra.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vv v8,v16,v24
vsra.vv v10,v18,v26
vsra.vv v12,v20,v28
vsra.vv v14,v22,v30
vsra.vv v8,v16,v24
vsra.vv v10,v18,v26
vsra.vv v12,v20,v28
vsra.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravvm_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vv v8,v16,v24,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v14,v22,v30,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vv v8,v16,v24,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v14,v22,v30,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v10,v18,v26,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravx_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vx v8,v16,t0
vsra.vx v10,v18,t2
vsra.vx v12,v20,t4
vsra.vx v14,v22,t6
vsra.vx v8,v16,t0
vsra.vx v10,v18,t2
vsra.vx v12,v20,t4
vsra.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vx v8,v16,t0
vsra.vx v10,v18,t2
vsra.vx v12,v20,t4
vsra.vx v14,v22,t6
vsra.vx v8,v16,t0
vsra.vx v10,v18,t2
vsra.vx v12,v20,t4
vsra.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravxm_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vx v8,v16,t0,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v14,v22,t6,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vx v8,v16,t0,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v14,v22,t6,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v10,v18,t2,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravi_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vi v8,v16,13
vsra.vi v10,v18,13
vsra.vi v12,v20,13
vsra.vi v14,v22,13
vsra.vi v8,v16,13
vsra.vi v10,v18,13
vsra.vi v12,v20,13
vsra.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vi v8,v16,13
vsra.vi v10,v18,13
vsra.vi v12,v20,13
vsra.vi v14,v22,13
vsra.vi v8,v16,13
vsra.vi v10,v18,13
vsra.vi v12,v20,13
vsra.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravim_m2:
	m_nop
	li a0, WARMUP
1:

vsra.vi v8,v16,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v14,v22,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsra.vi v8,v16,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v14,v22,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v10,v18,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvv_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vv v8,v16,v24
vssrl.vv v10,v18,v26
vssrl.vv v12,v20,v28
vssrl.vv v14,v22,v30
vssrl.vv v8,v16,v24
vssrl.vv v10,v18,v26
vssrl.vv v12,v20,v28
vssrl.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vv v8,v16,v24
vssrl.vv v10,v18,v26
vssrl.vv v12,v20,v28
vssrl.vv v14,v22,v30
vssrl.vv v8,v16,v24
vssrl.vv v10,v18,v26
vssrl.vv v12,v20,v28
vssrl.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvvm_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vv v8,v16,v24,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v14,v22,v30,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vv v8,v16,v24,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v14,v22,v30,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v10,v18,v26,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvx_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vx v8,v16,t0
vssrl.vx v10,v18,t2
vssrl.vx v12,v20,t4
vssrl.vx v14,v22,t6
vssrl.vx v8,v16,t0
vssrl.vx v10,v18,t2
vssrl.vx v12,v20,t4
vssrl.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vx v8,v16,t0
vssrl.vx v10,v18,t2
vssrl.vx v12,v20,t4
vssrl.vx v14,v22,t6
vssrl.vx v8,v16,t0
vssrl.vx v10,v18,t2
vssrl.vx v12,v20,t4
vssrl.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvxm_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vx v8,v16,t0,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v14,v22,t6,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vx v8,v16,t0,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v14,v22,t6,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v10,v18,t2,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvi_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vi v8,v16,13
vssrl.vi v10,v18,13
vssrl.vi v12,v20,13
vssrl.vi v14,v22,13
vssrl.vi v8,v16,13
vssrl.vi v10,v18,13
vssrl.vi v12,v20,13
vssrl.vi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vi v8,v16,13
vssrl.vi v10,v18,13
vssrl.vi v12,v20,13
vssrl.vi v14,v22,13
vssrl.vi v8,v16,13
vssrl.vi v10,v18,13
vssrl.vi v12,v20,13
vssrl.vi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvim_m2:
	m_nop
	li a0, WARMUP
1:

vssrl.vi v8,v16,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v14,v22,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vssrl.vi v8,v16,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v14,v22,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v10,v18,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vdivuvv_m2:
	m_nop
	li a0, WARMUP
1:

vdivu.vv v8,v16,v24
vdivu.vv v10,v18,v26
vdivu.vv v12,v20,v28
vdivu.vv v14,v22,v30
vdivu.vv v8,v16,v24
vdivu.vv v10,v18,v26
vdivu.vv v12,v20,v28
vdivu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdivu.vv v8,v16,v24
vdivu.vv v10,v18,v26
vdivu.vv v12,v20,v28
vdivu.vv v14,v22,v30
vdivu.vv v8,v16,v24
vdivu.vv v10,v18,v26
vdivu.vv v12,v20,v28
vdivu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vdivu.vv v8,v16,v24,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v14,v22,v30,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdivu.vv v8,v16,v24,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v14,v22,v30,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v10,v18,v26,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvx_m2:
	m_nop
	li a0, WARMUP
1:

vdivu.vx v8,v16,t0
vdivu.vx v10,v18,t2
vdivu.vx v12,v20,t4
vdivu.vx v14,v22,t6
vdivu.vx v8,v16,t0
vdivu.vx v10,v18,t2
vdivu.vx v12,v20,t4
vdivu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdivu.vx v8,v16,t0
vdivu.vx v10,v18,t2
vdivu.vx v12,v20,t4
vdivu.vx v14,v22,t6
vdivu.vx v8,v16,t0
vdivu.vx v10,v18,t2
vdivu.vx v12,v20,t4
vdivu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vdivu.vx v8,v16,t0,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v14,v22,t6,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdivu.vx v8,v16,t0,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v14,v22,t6,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v10,v18,t2,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvv_m2:
	m_nop
	li a0, WARMUP
1:

vdiv.vv v8,v16,v24
vdiv.vv v10,v18,v26
vdiv.vv v12,v20,v28
vdiv.vv v14,v22,v30
vdiv.vv v8,v16,v24
vdiv.vv v10,v18,v26
vdiv.vv v12,v20,v28
vdiv.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdiv.vv v8,v16,v24
vdiv.vv v10,v18,v26
vdiv.vv v12,v20,v28
vdiv.vv v14,v22,v30
vdiv.vv v8,v16,v24
vdiv.vv v10,v18,v26
vdiv.vv v12,v20,v28
vdiv.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvvm_m2:
	m_nop
	li a0, WARMUP
1:

vdiv.vv v8,v16,v24,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v14,v22,v30,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdiv.vv v8,v16,v24,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v14,v22,v30,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v10,v18,v26,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvx_m2:
	m_nop
	li a0, WARMUP
1:

vdiv.vx v8,v16,t0
vdiv.vx v10,v18,t2
vdiv.vx v12,v20,t4
vdiv.vx v14,v22,t6
vdiv.vx v8,v16,t0
vdiv.vx v10,v18,t2
vdiv.vx v12,v20,t4
vdiv.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdiv.vx v8,v16,t0
vdiv.vx v10,v18,t2
vdiv.vx v12,v20,t4
vdiv.vx v14,v22,t6
vdiv.vx v8,v16,t0
vdiv.vx v10,v18,t2
vdiv.vx v12,v20,t4
vdiv.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvxm_m2:
	m_nop
	li a0, WARMUP
1:

vdiv.vx v8,v16,t0,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v14,v22,t6,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vdiv.vx v8,v16,t0,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v14,v22,t6,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v10,v18,t2,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvv_m2:
	m_nop
	li a0, WARMUP
1:

vremu.vv v8,v16,v24
vremu.vv v10,v18,v26
vremu.vv v12,v20,v28
vremu.vv v14,v22,v30
vremu.vv v8,v16,v24
vremu.vv v10,v18,v26
vremu.vv v12,v20,v28
vremu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vremu.vv v8,v16,v24
vremu.vv v10,v18,v26
vremu.vv v12,v20,v28
vremu.vv v14,v22,v30
vremu.vv v8,v16,v24
vremu.vv v10,v18,v26
vremu.vv v12,v20,v28
vremu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vremu.vv v8,v16,v24,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v14,v22,v30,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vremu.vv v8,v16,v24,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v14,v22,v30,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v10,v18,v26,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvx_m2:
	m_nop
	li a0, WARMUP
1:

vremu.vx v8,v16,t0
vremu.vx v10,v18,t2
vremu.vx v12,v20,t4
vremu.vx v14,v22,t6
vremu.vx v8,v16,t0
vremu.vx v10,v18,t2
vremu.vx v12,v20,t4
vremu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vremu.vx v8,v16,t0
vremu.vx v10,v18,t2
vremu.vx v12,v20,t4
vremu.vx v14,v22,t6
vremu.vx v8,v16,t0
vremu.vx v10,v18,t2
vremu.vx v12,v20,t4
vremu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vremu.vx v8,v16,t0,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v14,v22,t6,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vremu.vx v8,v16,t0,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v14,v22,t6,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v10,v18,t2,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvv_m2:
	m_nop
	li a0, WARMUP
1:

vrem.vv v8,v16,v24
vrem.vv v10,v18,v26
vrem.vv v12,v20,v28
vrem.vv v14,v22,v30
vrem.vv v8,v16,v24
vrem.vv v10,v18,v26
vrem.vv v12,v20,v28
vrem.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrem.vv v8,v16,v24
vrem.vv v10,v18,v26
vrem.vv v12,v20,v28
vrem.vv v14,v22,v30
vrem.vv v8,v16,v24
vrem.vv v10,v18,v26
vrem.vv v12,v20,v28
vrem.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvvm_m2:
	m_nop
	li a0, WARMUP
1:

vrem.vv v8,v16,v24,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v14,v22,v30,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrem.vv v8,v16,v24,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v14,v22,v30,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v10,v18,v26,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvx_m2:
	m_nop
	li a0, WARMUP
1:

vrem.vx v8,v16,t0
vrem.vx v10,v18,t2
vrem.vx v12,v20,t4
vrem.vx v14,v22,t6
vrem.vx v8,v16,t0
vrem.vx v10,v18,t2
vrem.vx v12,v20,t4
vrem.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrem.vx v8,v16,t0
vrem.vx v10,v18,t2
vrem.vx v12,v20,t4
vrem.vx v14,v22,t6
vrem.vx v8,v16,t0
vrem.vx v10,v18,t2
vrem.vx v12,v20,t4
vrem.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvxm_m2:
	m_nop
	li a0, WARMUP
1:

vrem.vx v8,v16,t0,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v14,v22,t6,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vrem.vx v8,v16,t0,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v14,v22,t6,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v10,v18,t2,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvv_m2:
	m_nop
	li a0, WARMUP
1:

vmulhu.vv v8,v16,v24
vmulhu.vv v10,v18,v26
vmulhu.vv v12,v20,v28
vmulhu.vv v14,v22,v30
vmulhu.vv v8,v16,v24
vmulhu.vv v10,v18,v26
vmulhu.vv v12,v20,v28
vmulhu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhu.vv v8,v16,v24
vmulhu.vv v10,v18,v26
vmulhu.vv v12,v20,v28
vmulhu.vv v14,v22,v30
vmulhu.vv v8,v16,v24
vmulhu.vv v10,v18,v26
vmulhu.vv v12,v20,v28
vmulhu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v14,v22,v30,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v14,v22,v30,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v10,v18,v26,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmulhu.vx v8,v16,t0
vmulhu.vx v10,v18,t2
vmulhu.vx v12,v20,t4
vmulhu.vx v14,v22,t6
vmulhu.vx v8,v16,t0
vmulhu.vx v10,v18,t2
vmulhu.vx v12,v20,t4
vmulhu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhu.vx v8,v16,t0
vmulhu.vx v10,v18,t2
vmulhu.vx v12,v20,t4
vmulhu.vx v14,v22,t6
vmulhu.vx v8,v16,t0
vmulhu.vx v10,v18,t2
vmulhu.vx v12,v20,t4
vmulhu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v14,v22,t6,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v14,v22,t6,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v10,v18,t2,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvv_m2:
	m_nop
	li a0, WARMUP
1:

vmul.vv v8,v16,v24
vmul.vv v10,v18,v26
vmul.vv v12,v20,v28
vmul.vv v14,v22,v30
vmul.vv v8,v16,v24
vmul.vv v10,v18,v26
vmul.vv v12,v20,v28
vmul.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmul.vv v8,v16,v24
vmul.vv v10,v18,v26
vmul.vv v12,v20,v28
vmul.vv v14,v22,v30
vmul.vv v8,v16,v24
vmul.vv v10,v18,v26
vmul.vv v12,v20,v28
vmul.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmul.vv v8,v16,v24,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v14,v22,v30,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmul.vv v8,v16,v24,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v14,v22,v30,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v10,v18,v26,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvx_m2:
	m_nop
	li a0, WARMUP
1:

vmul.vx v8,v16,t0
vmul.vx v10,v18,t2
vmul.vx v12,v20,t4
vmul.vx v14,v22,t6
vmul.vx v8,v16,t0
vmul.vx v10,v18,t2
vmul.vx v12,v20,t4
vmul.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmul.vx v8,v16,t0
vmul.vx v10,v18,t2
vmul.vx v12,v20,t4
vmul.vx v14,v22,t6
vmul.vx v8,v16,t0
vmul.vx v10,v18,t2
vmul.vx v12,v20,t4
vmul.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmul.vx v8,v16,t0,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v14,v22,t6,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmul.vx v8,v16,t0,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v14,v22,t6,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v10,v18,t2,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvv_m2:
	m_nop
	li a0, WARMUP
1:

vmulhsu.vv v8,v16,v24
vmulhsu.vv v10,v18,v26
vmulhsu.vv v12,v20,v28
vmulhsu.vv v14,v22,v30
vmulhsu.vv v8,v16,v24
vmulhsu.vv v10,v18,v26
vmulhsu.vv v12,v20,v28
vmulhsu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhsu.vv v8,v16,v24
vmulhsu.vv v10,v18,v26
vmulhsu.vv v12,v20,v28
vmulhsu.vv v14,v22,v30
vmulhsu.vv v8,v16,v24
vmulhsu.vv v10,v18,v26
vmulhsu.vv v12,v20,v28
vmulhsu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v14,v22,v30,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v14,v22,v30,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v10,v18,v26,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvx_m2:
	m_nop
	li a0, WARMUP
1:

vmulhsu.vx v8,v16,t0
vmulhsu.vx v10,v18,t2
vmulhsu.vx v12,v20,t4
vmulhsu.vx v14,v22,t6
vmulhsu.vx v8,v16,t0
vmulhsu.vx v10,v18,t2
vmulhsu.vx v12,v20,t4
vmulhsu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhsu.vx v8,v16,t0
vmulhsu.vx v10,v18,t2
vmulhsu.vx v12,v20,t4
vmulhsu.vx v14,v22,t6
vmulhsu.vx v8,v16,t0
vmulhsu.vx v10,v18,t2
vmulhsu.vx v12,v20,t4
vmulhsu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v14,v22,t6,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v14,v22,t6,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v10,v18,t2,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvv_m2:
	m_nop
	li a0, WARMUP
1:

vmulh.vv v8,v16,v24
vmulh.vv v10,v18,v26
vmulh.vv v12,v20,v28
vmulh.vv v14,v22,v30
vmulh.vv v8,v16,v24
vmulh.vv v10,v18,v26
vmulh.vv v12,v20,v28
vmulh.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulh.vv v8,v16,v24
vmulh.vv v10,v18,v26
vmulh.vv v12,v20,v28
vmulh.vv v14,v22,v30
vmulh.vv v8,v16,v24
vmulh.vv v10,v18,v26
vmulh.vv v12,v20,v28
vmulh.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmulh.vv v8,v16,v24,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v14,v22,v30,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulh.vv v8,v16,v24,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v14,v22,v30,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v10,v18,v26,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvx_m2:
	m_nop
	li a0, WARMUP
1:

vmulh.vx v8,v16,t0
vmulh.vx v10,v18,t2
vmulh.vx v12,v20,t4
vmulh.vx v14,v22,t6
vmulh.vx v8,v16,t0
vmulh.vx v10,v18,t2
vmulh.vx v12,v20,t4
vmulh.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulh.vx v8,v16,t0
vmulh.vx v10,v18,t2
vmulh.vx v12,v20,t4
vmulh.vx v14,v22,t6
vmulh.vx v8,v16,t0
vmulh.vx v10,v18,t2
vmulh.vx v12,v20,t4
vmulh.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmulh.vx v8,v16,t0,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v14,v22,t6,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmulh.vx v8,v16,t0,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v14,v22,t6,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v10,v18,t2,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vmadd.vv v8,v16,v24
vmadd.vv v10,v18,v26
vmadd.vv v12,v20,v28
vmadd.vv v14,v22,v30
vmadd.vv v8,v16,v24
vmadd.vv v10,v18,v26
vmadd.vv v12,v20,v28
vmadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadd.vv v8,v16,v24
vmadd.vv v10,v18,v26
vmadd.vv v12,v20,v28
vmadd.vv v14,v22,v30
vmadd.vv v8,v16,v24
vmadd.vv v10,v18,v26
vmadd.vv v12,v20,v28
vmadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmadd.vv v8,v16,v24,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v14,v22,v30,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadd.vv v8,v16,v24,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v14,v22,v30,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v10,v18,v26,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvx_m2:
	m_nop
	li a0, WARMUP
1:

vmadd.vx v8,t0,v16
vmadd.vx v10,t2,v18
vmadd.vx v12,t4,v20
vmadd.vx v14,t6,v22
vmadd.vx v8,t0,v16
vmadd.vx v10,t2,v18
vmadd.vx v12,t4,v20
vmadd.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadd.vx v8,t0,v16
vmadd.vx v10,t2,v18
vmadd.vx v12,t4,v20
vmadd.vx v14,t6,v22
vmadd.vx v8,t0,v16
vmadd.vx v10,t2,v18
vmadd.vx v12,t4,v20
vmadd.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmadd.vx v8,t0,v16,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v14,t6,v22,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmadd.vx v8,t0,v16,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v14,t6,v22,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v10,t2,v18,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vmacc.vv v8,v16,v24
vmacc.vv v10,v18,v26
vmacc.vv v12,v20,v28
vmacc.vv v14,v22,v30
vmacc.vv v8,v16,v24
vmacc.vv v10,v18,v26
vmacc.vv v12,v20,v28
vmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmacc.vv v8,v16,v24
vmacc.vv v10,v18,v26
vmacc.vv v12,v20,v28
vmacc.vv v14,v22,v30
vmacc.vv v8,v16,v24
vmacc.vv v10,v18,v26
vmacc.vv v12,v20,v28
vmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmacc.vv v8,v16,v24,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v14,v22,v30,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmacc.vv v8,v16,v24,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v14,v22,v30,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v10,v18,v26,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvx_m2:
	m_nop
	li a0, WARMUP
1:

vmacc.vx v8,t0,v16
vmacc.vx v10,t2,v18
vmacc.vx v12,t4,v20
vmacc.vx v14,t6,v22
vmacc.vx v8,t0,v16
vmacc.vx v10,t2,v18
vmacc.vx v12,t4,v20
vmacc.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmacc.vx v8,t0,v16
vmacc.vx v10,t2,v18
vmacc.vx v12,t4,v20
vmacc.vx v14,t6,v22
vmacc.vx v8,t0,v16
vmacc.vx v10,t2,v18
vmacc.vx v12,t4,v20
vmacc.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvxm_m2:
	m_nop
	li a0, WARMUP
1:

vmacc.vx v8,t0,v16,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v14,t6,v22,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmacc.vx v8,t0,v16,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v14,t6,v22,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v10,t2,v18,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vnsrlwv_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wv v8,v16,v24
vnsrl.wv v10,v18,v26
vnsrl.wv v12,v20,v28
vnsrl.wv v14,v22,v30
vnsrl.wv v8,v16,v24
vnsrl.wv v10,v18,v26
vnsrl.wv v12,v20,v28
vnsrl.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wv v8,v16,v24
vnsrl.wv v10,v18,v26
vnsrl.wv v12,v20,v28
vnsrl.wv v14,v22,v30
vnsrl.wv v8,v16,v24
vnsrl.wv v10,v18,v26
vnsrl.wv v12,v20,v28
vnsrl.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwvm_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v14,v22,v30,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v14,v22,v30,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v10,v18,v26,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwx_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wx v8,v16,t0
vnsrl.wx v10,v18,t2
vnsrl.wx v12,v20,t4
vnsrl.wx v14,v22,t6
vnsrl.wx v8,v16,t0
vnsrl.wx v10,v18,t2
vnsrl.wx v12,v20,t4
vnsrl.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wx v8,v16,t0
vnsrl.wx v10,v18,t2
vnsrl.wx v12,v20,t4
vnsrl.wx v14,v22,t6
vnsrl.wx v8,v16,t0
vnsrl.wx v10,v18,t2
vnsrl.wx v12,v20,t4
vnsrl.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwxm_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v14,v22,t6,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v14,v22,t6,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v10,v18,t2,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwi_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wi v8,v16,13
vnsrl.wi v10,v18,13
vnsrl.wi v12,v20,13
vnsrl.wi v14,v22,13
vnsrl.wi v8,v16,13
vnsrl.wi v10,v18,13
vnsrl.wi v12,v20,13
vnsrl.wi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wi v8,v16,13
vnsrl.wi v10,v18,13
vnsrl.wi v12,v20,13
vnsrl.wi v14,v22,13
vnsrl.wi v8,v16,13
vnsrl.wi v10,v18,13
vnsrl.wi v12,v20,13
vnsrl.wi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwim_m2:
	m_nop
	li a0, WARMUP
1:

vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v14,v22,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v14,v22,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v10,v18,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawv_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wv v8,v16,v24
vnsra.wv v10,v18,v26
vnsra.wv v12,v20,v28
vnsra.wv v14,v22,v30
vnsra.wv v8,v16,v24
vnsra.wv v10,v18,v26
vnsra.wv v12,v20,v28
vnsra.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wv v8,v16,v24
vnsra.wv v10,v18,v26
vnsra.wv v12,v20,v28
vnsra.wv v14,v22,v30
vnsra.wv v8,v16,v24
vnsra.wv v10,v18,v26
vnsra.wv v12,v20,v28
vnsra.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawvm_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wv v8,v16,v24,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v14,v22,v30,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wv v8,v16,v24,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v14,v22,v30,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v10,v18,v26,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawx_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wx v8,v16,t0
vnsra.wx v10,v18,t2
vnsra.wx v12,v20,t4
vnsra.wx v14,v22,t6
vnsra.wx v8,v16,t0
vnsra.wx v10,v18,t2
vnsra.wx v12,v20,t4
vnsra.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wx v8,v16,t0
vnsra.wx v10,v18,t2
vnsra.wx v12,v20,t4
vnsra.wx v14,v22,t6
vnsra.wx v8,v16,t0
vnsra.wx v10,v18,t2
vnsra.wx v12,v20,t4
vnsra.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawxm_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wx v8,v16,t0,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v14,v22,t6,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wx v8,v16,t0,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v14,v22,t6,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v10,v18,t2,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawi_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wi v8,v16,13
vnsra.wi v10,v18,13
vnsra.wi v12,v20,13
vnsra.wi v14,v22,13
vnsra.wi v8,v16,13
vnsra.wi v10,v18,13
vnsra.wi v12,v20,13
vnsra.wi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wi v8,v16,13
vnsra.wi v10,v18,13
vnsra.wi v12,v20,13
vnsra.wi v14,v22,13
vnsra.wi v8,v16,13
vnsra.wi v10,v18,13
vnsra.wi v12,v20,13
vnsra.wi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawim_m2:
	m_nop
	li a0, WARMUP
1:

vnsra.wi v8,v16,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v14,v22,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnsra.wi v8,v16,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v14,v22,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v10,v18,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwv_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wv v8,v16,v24
vnclipu.wv v10,v18,v26
vnclipu.wv v12,v20,v28
vnclipu.wv v14,v22,v30
vnclipu.wv v8,v16,v24
vnclipu.wv v10,v18,v26
vnclipu.wv v12,v20,v28
vnclipu.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wv v8,v16,v24
vnclipu.wv v10,v18,v26
vnclipu.wv v12,v20,v28
vnclipu.wv v14,v22,v30
vnclipu.wv v8,v16,v24
vnclipu.wv v10,v18,v26
vnclipu.wv v12,v20,v28
vnclipu.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwvm_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v14,v22,v30,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v14,v22,v30,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v10,v18,v26,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwx_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wx v8,v16,t0
vnclipu.wx v10,v18,t2
vnclipu.wx v12,v20,t4
vnclipu.wx v14,v22,t6
vnclipu.wx v8,v16,t0
vnclipu.wx v10,v18,t2
vnclipu.wx v12,v20,t4
vnclipu.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wx v8,v16,t0
vnclipu.wx v10,v18,t2
vnclipu.wx v12,v20,t4
vnclipu.wx v14,v22,t6
vnclipu.wx v8,v16,t0
vnclipu.wx v10,v18,t2
vnclipu.wx v12,v20,t4
vnclipu.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwxm_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v14,v22,t6,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v14,v22,t6,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v10,v18,t2,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwi_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wi v8,v16,13
vnclipu.wi v10,v18,13
vnclipu.wi v12,v20,13
vnclipu.wi v14,v22,13
vnclipu.wi v8,v16,13
vnclipu.wi v10,v18,13
vnclipu.wi v12,v20,13
vnclipu.wi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wi v8,v16,13
vnclipu.wi v10,v18,13
vnclipu.wi v12,v20,13
vnclipu.wi v14,v22,13
vnclipu.wi v8,v16,13
vnclipu.wi v10,v18,13
vnclipu.wi v12,v20,13
vnclipu.wi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwim_m2:
	m_nop
	li a0, WARMUP
1:

vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v14,v22,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v14,v22,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v10,v18,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwv_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wv v8,v16,v24
vnclip.wv v10,v18,v26
vnclip.wv v12,v20,v28
vnclip.wv v14,v22,v30
vnclip.wv v8,v16,v24
vnclip.wv v10,v18,v26
vnclip.wv v12,v20,v28
vnclip.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wv v8,v16,v24
vnclip.wv v10,v18,v26
vnclip.wv v12,v20,v28
vnclip.wv v14,v22,v30
vnclip.wv v8,v16,v24
vnclip.wv v10,v18,v26
vnclip.wv v12,v20,v28
vnclip.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwvm_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wv v8,v16,v24,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v14,v22,v30,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wv v8,v16,v24,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v14,v22,v30,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v10,v18,v26,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwx_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wx v8,v16,t0
vnclip.wx v10,v18,t2
vnclip.wx v12,v20,t4
vnclip.wx v14,v22,t6
vnclip.wx v8,v16,t0
vnclip.wx v10,v18,t2
vnclip.wx v12,v20,t4
vnclip.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wx v8,v16,t0
vnclip.wx v10,v18,t2
vnclip.wx v12,v20,t4
vnclip.wx v14,v22,t6
vnclip.wx v8,v16,t0
vnclip.wx v10,v18,t2
vnclip.wx v12,v20,t4
vnclip.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwxm_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wx v8,v16,t0,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v14,v22,t6,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wx v8,v16,t0,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v14,v22,t6,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v10,v18,t2,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwi_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wi v8,v16,13
vnclip.wi v10,v18,13
vnclip.wi v12,v20,13
vnclip.wi v14,v22,13
vnclip.wi v8,v16,13
vnclip.wi v10,v18,13
vnclip.wi v12,v20,13
vnclip.wi v14,v22,13


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wi v8,v16,13
vnclip.wi v10,v18,13
vnclip.wi v12,v20,13
vnclip.wi v14,v22,13
vnclip.wi v8,v16,13
vnclip.wi v10,v18,13
vnclip.wi v12,v20,13
vnclip.wi v14,v22,13


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwim_m2:
	m_nop
	li a0, WARMUP
1:

vnclip.wi v8,v16,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v14,v22,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v14,v22,13,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnclip.wi v8,v16,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v14,v22,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v10,v18,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v14,v22,13,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vnmsub.vv v8,v16,v24
vnmsub.vv v10,v18,v26
vnmsub.vv v12,v20,v28
vnmsub.vv v14,v22,v30
vnmsub.vv v8,v16,v24
vnmsub.vv v10,v18,v26
vnmsub.vv v12,v20,v28
vnmsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsub.vv v8,v16,v24
vnmsub.vv v10,v18,v26
vnmsub.vv v12,v20,v28
vnmsub.vv v14,v22,v30
vnmsub.vv v8,v16,v24
vnmsub.vv v10,v18,v26
vnmsub.vv v12,v20,v28
vnmsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v14,v22,v30,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v14,v22,v30,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v10,v18,v26,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvx_m2:
	m_nop
	li a0, WARMUP
1:

vnmsub.vx v8,t0,v16
vnmsub.vx v10,t2,v18
vnmsub.vx v12,t4,v20
vnmsub.vx v14,t6,v22
vnmsub.vx v8,t0,v16
vnmsub.vx v10,t2,v18
vnmsub.vx v12,t4,v20
vnmsub.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsub.vx v8,t0,v16
vnmsub.vx v10,t2,v18
vnmsub.vx v12,t4,v20
vnmsub.vx v14,t6,v22
vnmsub.vx v8,t0,v16
vnmsub.vx v10,t2,v18
vnmsub.vx v12,t4,v20
vnmsub.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v14,t6,v22,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v14,t6,v22,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v10,t2,v18,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvv_m2:
	m_nop
	li a0, WARMUP
1:

vnmsac.vv v8,v16,v24
vnmsac.vv v10,v18,v26
vnmsac.vv v12,v20,v28
vnmsac.vv v14,v22,v30
vnmsac.vv v8,v16,v24
vnmsac.vv v10,v18,v26
vnmsac.vv v12,v20,v28
vnmsac.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsac.vv v8,v16,v24
vnmsac.vv v10,v18,v26
vnmsac.vv v12,v20,v28
vnmsac.vv v14,v22,v30
vnmsac.vv v8,v16,v24
vnmsac.vv v10,v18,v26
vnmsac.vv v12,v20,v28
vnmsac.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvvm_m2:
	m_nop
	li a0, WARMUP
1:

vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v14,v22,v30,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v14,v22,v30,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v10,v18,v26,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvx_m2:
	m_nop
	li a0, WARMUP
1:

vnmsac.vx v8,t0,v16
vnmsac.vx v10,t2,v18
vnmsac.vx v12,t4,v20
vnmsac.vx v14,t6,v22
vnmsac.vx v8,t0,v16
vnmsac.vx v10,t2,v18
vnmsac.vx v12,t4,v20
vnmsac.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsac.vx v8,t0,v16
vnmsac.vx v10,t2,v18
vnmsac.vx v12,t4,v20
vnmsac.vx v14,t6,v22
vnmsac.vx v8,t0,v16
vnmsac.vx v10,t2,v18
vnmsac.vx v12,t4,v20
vnmsac.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvxm_m2:
	m_nop
	li a0, WARMUP
1:

vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v14,t6,v22,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v14,t6,v22,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v10,t2,v18,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwadduvv_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.vv v8,v16,v24
vwaddu.vv v10,v18,v26
vwaddu.vv v12,v20,v28
vwaddu.vv v14,v22,v30
vwaddu.vv v8,v16,v24
vwaddu.vv v10,v18,v26
vwaddu.vv v12,v20,v28
vwaddu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.vv v8,v16,v24
vwaddu.vv v10,v18,v26
vwaddu.vv v12,v20,v28
vwaddu.vv v14,v22,v30
vwaddu.vv v8,v16,v24
vwaddu.vv v10,v18,v26
vwaddu.vv v12,v20,v28
vwaddu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v14,v22,v30,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v14,v22,v30,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v10,v18,v26,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvx_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.vx v8,v16,t0
vwaddu.vx v10,v18,t2
vwaddu.vx v12,v20,t4
vwaddu.vx v14,v22,t6
vwaddu.vx v8,v16,t0
vwaddu.vx v10,v18,t2
vwaddu.vx v12,v20,t4
vwaddu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.vx v8,v16,t0
vwaddu.vx v10,v18,t2
vwaddu.vx v12,v20,t4
vwaddu.vx v14,v22,t6
vwaddu.vx v8,v16,t0
vwaddu.vx v10,v18,t2
vwaddu.vx v12,v20,t4
vwaddu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v14,v22,t6,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v14,v22,t6,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v10,v18,t2,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.vv v8,v16,v24
vwadd.vv v10,v18,v26
vwadd.vv v12,v20,v28
vwadd.vv v14,v22,v30
vwadd.vv v8,v16,v24
vwadd.vv v10,v18,v26
vwadd.vv v12,v20,v28
vwadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.vv v8,v16,v24
vwadd.vv v10,v18,v26
vwadd.vv v12,v20,v28
vwadd.vv v14,v22,v30
vwadd.vv v8,v16,v24
vwadd.vv v10,v18,v26
vwadd.vv v12,v20,v28
vwadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.vv v8,v16,v24,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v14,v22,v30,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.vv v8,v16,v24,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v14,v22,v30,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v10,v18,v26,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvx_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.vx v8,v16,t0
vwadd.vx v10,v18,t2
vwadd.vx v12,v20,t4
vwadd.vx v14,v22,t6
vwadd.vx v8,v16,t0
vwadd.vx v10,v18,t2
vwadd.vx v12,v20,t4
vwadd.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.vx v8,v16,t0
vwadd.vx v10,v18,t2
vwadd.vx v12,v20,t4
vwadd.vx v14,v22,t6
vwadd.vx v8,v16,t0
vwadd.vx v10,v18,t2
vwadd.vx v12,v20,t4
vwadd.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.vx v8,v16,t0,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v14,v22,t6,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.vx v8,v16,t0,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v14,v22,t6,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v10,v18,t2,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvv_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.vv v8,v16,v24
vwsubu.vv v10,v18,v26
vwsubu.vv v12,v20,v28
vwsubu.vv v14,v22,v30
vwsubu.vv v8,v16,v24
vwsubu.vv v10,v18,v26
vwsubu.vv v12,v20,v28
vwsubu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.vv v8,v16,v24
vwsubu.vv v10,v18,v26
vwsubu.vv v12,v20,v28
vwsubu.vv v14,v22,v30
vwsubu.vv v8,v16,v24
vwsubu.vv v10,v18,v26
vwsubu.vv v12,v20,v28
vwsubu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v14,v22,v30,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v14,v22,v30,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v10,v18,v26,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvx_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.vx v8,v16,t0
vwsubu.vx v10,v18,t2
vwsubu.vx v12,v20,t4
vwsubu.vx v14,v22,t6
vwsubu.vx v8,v16,t0
vwsubu.vx v10,v18,t2
vwsubu.vx v12,v20,t4
vwsubu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.vx v8,v16,t0
vwsubu.vx v10,v18,t2
vwsubu.vx v12,v20,t4
vwsubu.vx v14,v22,t6
vwsubu.vx v8,v16,t0
vwsubu.vx v10,v18,t2
vwsubu.vx v12,v20,t4
vwsubu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v14,v22,t6,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v14,v22,t6,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v10,v18,t2,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.vv v8,v16,v24
vwsub.vv v10,v18,v26
vwsub.vv v12,v20,v28
vwsub.vv v14,v22,v30
vwsub.vv v8,v16,v24
vwsub.vv v10,v18,v26
vwsub.vv v12,v20,v28
vwsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.vv v8,v16,v24
vwsub.vv v10,v18,v26
vwsub.vv v12,v20,v28
vwsub.vv v14,v22,v30
vwsub.vv v8,v16,v24
vwsub.vv v10,v18,v26
vwsub.vv v12,v20,v28
vwsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.vv v8,v16,v24,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v14,v22,v30,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.vv v8,v16,v24,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v14,v22,v30,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v10,v18,v26,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvx_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.vx v8,v16,t0
vwsub.vx v10,v18,t2
vwsub.vx v12,v20,t4
vwsub.vx v14,v22,t6
vwsub.vx v8,v16,t0
vwsub.vx v10,v18,t2
vwsub.vx v12,v20,t4
vwsub.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.vx v8,v16,t0
vwsub.vx v10,v18,t2
vwsub.vx v12,v20,t4
vwsub.vx v14,v22,t6
vwsub.vx v8,v16,t0
vwsub.vx v10,v18,t2
vwsub.vx v12,v20,t4
vwsub.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.vx v8,v16,t0,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v14,v22,t6,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.vx v8,v16,t0,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v14,v22,t6,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v10,v18,t2,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwv_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.wv v8,v16,v24
vwaddu.wv v10,v18,v26
vwaddu.wv v12,v20,v28
vwaddu.wv v14,v22,v30
vwaddu.wv v8,v16,v24
vwaddu.wv v10,v18,v26
vwaddu.wv v12,v20,v28
vwaddu.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.wv v8,v16,v24
vwaddu.wv v10,v18,v26
vwaddu.wv v12,v20,v28
vwaddu.wv v14,v22,v30
vwaddu.wv v8,v16,v24
vwaddu.wv v10,v18,v26
vwaddu.wv v12,v20,v28
vwaddu.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwvm_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v14,v22,v30,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v14,v22,v30,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v10,v18,v26,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwx_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.wx v8,v16,t0
vwaddu.wx v10,v18,t2
vwaddu.wx v12,v20,t4
vwaddu.wx v14,v22,t6
vwaddu.wx v8,v16,t0
vwaddu.wx v10,v18,t2
vwaddu.wx v12,v20,t4
vwaddu.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.wx v8,v16,t0
vwaddu.wx v10,v18,t2
vwaddu.wx v12,v20,t4
vwaddu.wx v14,v22,t6
vwaddu.wx v8,v16,t0
vwaddu.wx v10,v18,t2
vwaddu.wx v12,v20,t4
vwaddu.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwxm_m2:
	m_nop
	li a0, WARMUP
1:

vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v14,v22,t6,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v14,v22,t6,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v10,v18,t2,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwv_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.wv v8,v16,v24
vwadd.wv v10,v18,v26
vwadd.wv v12,v20,v28
vwadd.wv v14,v22,v30
vwadd.wv v8,v16,v24
vwadd.wv v10,v18,v26
vwadd.wv v12,v20,v28
vwadd.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.wv v8,v16,v24
vwadd.wv v10,v18,v26
vwadd.wv v12,v20,v28
vwadd.wv v14,v22,v30
vwadd.wv v8,v16,v24
vwadd.wv v10,v18,v26
vwadd.wv v12,v20,v28
vwadd.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwvm_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.wv v8,v16,v24,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v14,v22,v30,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.wv v8,v16,v24,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v14,v22,v30,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v10,v18,v26,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwx_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.wx v8,v16,t0
vwadd.wx v10,v18,t2
vwadd.wx v12,v20,t4
vwadd.wx v14,v22,t6
vwadd.wx v8,v16,t0
vwadd.wx v10,v18,t2
vwadd.wx v12,v20,t4
vwadd.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.wx v8,v16,t0
vwadd.wx v10,v18,t2
vwadd.wx v12,v20,t4
vwadd.wx v14,v22,t6
vwadd.wx v8,v16,t0
vwadd.wx v10,v18,t2
vwadd.wx v12,v20,t4
vwadd.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwxm_m2:
	m_nop
	li a0, WARMUP
1:

vwadd.wx v8,v16,t0,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v14,v22,t6,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwadd.wx v8,v16,t0,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v14,v22,t6,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v10,v18,t2,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwv_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.wv v8,v16,v24
vwsubu.wv v10,v18,v26
vwsubu.wv v12,v20,v28
vwsubu.wv v14,v22,v30
vwsubu.wv v8,v16,v24
vwsubu.wv v10,v18,v26
vwsubu.wv v12,v20,v28
vwsubu.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.wv v8,v16,v24
vwsubu.wv v10,v18,v26
vwsubu.wv v12,v20,v28
vwsubu.wv v14,v22,v30
vwsubu.wv v8,v16,v24
vwsubu.wv v10,v18,v26
vwsubu.wv v12,v20,v28
vwsubu.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwvm_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v14,v22,v30,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v14,v22,v30,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v10,v18,v26,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwx_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.wx v8,v16,t0
vwsubu.wx v10,v18,t2
vwsubu.wx v12,v20,t4
vwsubu.wx v14,v22,t6
vwsubu.wx v8,v16,t0
vwsubu.wx v10,v18,t2
vwsubu.wx v12,v20,t4
vwsubu.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.wx v8,v16,t0
vwsubu.wx v10,v18,t2
vwsubu.wx v12,v20,t4
vwsubu.wx v14,v22,t6
vwsubu.wx v8,v16,t0
vwsubu.wx v10,v18,t2
vwsubu.wx v12,v20,t4
vwsubu.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwxm_m2:
	m_nop
	li a0, WARMUP
1:

vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v14,v22,t6,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v14,v22,t6,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v10,v18,t2,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwv_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.wv v8,v16,v24
vwsub.wv v10,v18,v26
vwsub.wv v12,v20,v28
vwsub.wv v14,v22,v30
vwsub.wv v8,v16,v24
vwsub.wv v10,v18,v26
vwsub.wv v12,v20,v28
vwsub.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.wv v8,v16,v24
vwsub.wv v10,v18,v26
vwsub.wv v12,v20,v28
vwsub.wv v14,v22,v30
vwsub.wv v8,v16,v24
vwsub.wv v10,v18,v26
vwsub.wv v12,v20,v28
vwsub.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwvm_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.wv v8,v16,v24,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v14,v22,v30,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.wv v8,v16,v24,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v14,v22,v30,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v10,v18,v26,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwx_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.wx v8,v16,t0
vwsub.wx v10,v18,t2
vwsub.wx v12,v20,t4
vwsub.wx v14,v22,t6
vwsub.wx v8,v16,t0
vwsub.wx v10,v18,t2
vwsub.wx v12,v20,t4
vwsub.wx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.wx v8,v16,t0
vwsub.wx v10,v18,t2
vwsub.wx v12,v20,t4
vwsub.wx v14,v22,t6
vwsub.wx v8,v16,t0
vwsub.wx v10,v18,t2
vwsub.wx v12,v20,t4
vwsub.wx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwxm_m2:
	m_nop
	li a0, WARMUP
1:

vwsub.wx v8,v16,t0,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v14,v22,t6,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwsub.wx v8,v16,t0,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v14,v22,t6,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v10,v18,t2,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmulu.vv v8,v16,v24
vwmulu.vv v10,v18,v26
vwmulu.vv v12,v20,v28
vwmulu.vv v14,v22,v30
vwmulu.vv v8,v16,v24
vwmulu.vv v10,v18,v26
vwmulu.vv v12,v20,v28
vwmulu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulu.vv v8,v16,v24
vwmulu.vv v10,v18,v26
vwmulu.vv v12,v20,v28
vwmulu.vv v14,v22,v30
vwmulu.vv v8,v16,v24
vwmulu.vv v10,v18,v26
vwmulu.vv v12,v20,v28
vwmulu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v14,v22,v30,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v14,v22,v30,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v10,v18,v26,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmulu.vx v8,v16,t0
vwmulu.vx v10,v18,t2
vwmulu.vx v12,v20,t4
vwmulu.vx v14,v22,t6
vwmulu.vx v8,v16,t0
vwmulu.vx v10,v18,t2
vwmulu.vx v12,v20,t4
vwmulu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulu.vx v8,v16,t0
vwmulu.vx v10,v18,t2
vwmulu.vx v12,v20,t4
vwmulu.vx v14,v22,t6
vwmulu.vx v8,v16,t0
vwmulu.vx v10,v18,t2
vwmulu.vx v12,v20,t4
vwmulu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v14,v22,t6,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v14,v22,t6,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v10,v18,t2,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmulsu.vv v8,v16,v24
vwmulsu.vv v10,v18,v26
vwmulsu.vv v12,v20,v28
vwmulsu.vv v14,v22,v30
vwmulsu.vv v8,v16,v24
vwmulsu.vv v10,v18,v26
vwmulsu.vv v12,v20,v28
vwmulsu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulsu.vv v8,v16,v24
vwmulsu.vv v10,v18,v26
vwmulsu.vv v12,v20,v28
vwmulsu.vv v14,v22,v30
vwmulsu.vv v8,v16,v24
vwmulsu.vv v10,v18,v26
vwmulsu.vv v12,v20,v28
vwmulsu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v14,v22,v30,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v14,v22,v30,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v10,v18,v26,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmulsu.vx v8,v16,t0
vwmulsu.vx v10,v18,t2
vwmulsu.vx v12,v20,t4
vwmulsu.vx v14,v22,t6
vwmulsu.vx v8,v16,t0
vwmulsu.vx v10,v18,t2
vwmulsu.vx v12,v20,t4
vwmulsu.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulsu.vx v8,v16,t0
vwmulsu.vx v10,v18,t2
vwmulsu.vx v12,v20,t4
vwmulsu.vx v14,v22,t6
vwmulsu.vx v8,v16,t0
vwmulsu.vx v10,v18,t2
vwmulsu.vx v12,v20,t4
vwmulsu.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v14,v22,t6,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v14,v22,t6,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v10,v18,t2,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmul.vv v8,v16,v24
vwmul.vv v10,v18,v26
vwmul.vv v12,v20,v28
vwmul.vv v14,v22,v30
vwmul.vv v8,v16,v24
vwmul.vv v10,v18,v26
vwmul.vv v12,v20,v28
vwmul.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmul.vv v8,v16,v24
vwmul.vv v10,v18,v26
vwmul.vv v12,v20,v28
vwmul.vv v14,v22,v30
vwmul.vv v8,v16,v24
vwmul.vv v10,v18,v26
vwmul.vv v12,v20,v28
vwmul.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmul.vv v8,v16,v24,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v14,v22,v30,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmul.vv v8,v16,v24,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v14,v22,v30,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v10,v18,v26,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmul.vx v8,v16,t0
vwmul.vx v10,v18,t2
vwmul.vx v12,v20,t4
vwmul.vx v14,v22,t6
vwmul.vx v8,v16,t0
vwmul.vx v10,v18,t2
vwmul.vx v12,v20,t4
vwmul.vx v14,v22,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmul.vx v8,v16,t0
vwmul.vx v10,v18,t2
vwmul.vx v12,v20,t4
vwmul.vx v14,v22,t6
vwmul.vx v8,v16,t0
vwmul.vx v10,v18,t2
vwmul.vx v12,v20,t4
vwmul.vx v14,v22,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmul.vx v8,v16,t0,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v14,v22,t6,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v14,v22,t6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmul.vx v8,v16,t0,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v14,v22,t6,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v10,v18,t2,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v14,v22,t6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccu.vv v8,v16,v24
vwmaccu.vv v10,v18,v26
vwmaccu.vv v12,v20,v28
vwmaccu.vv v14,v22,v30
vwmaccu.vv v8,v16,v24
vwmaccu.vv v10,v18,v26
vwmaccu.vv v12,v20,v28
vwmaccu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccu.vv v8,v16,v24
vwmaccu.vv v10,v18,v26
vwmaccu.vv v12,v20,v28
vwmaccu.vv v14,v22,v30
vwmaccu.vv v8,v16,v24
vwmaccu.vv v10,v18,v26
vwmaccu.vv v12,v20,v28
vwmaccu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v14,v22,v30,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v14,v22,v30,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v10,v18,v26,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccu.vx v8,t0,v16
vwmaccu.vx v10,t2,v18
vwmaccu.vx v12,t4,v20
vwmaccu.vx v14,t6,v22
vwmaccu.vx v8,t0,v16
vwmaccu.vx v10,t2,v18
vwmaccu.vx v12,t4,v20
vwmaccu.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccu.vx v8,t0,v16
vwmaccu.vx v10,t2,v18
vwmaccu.vx v12,t4,v20
vwmaccu.vx v14,t6,v22
vwmaccu.vx v8,t0,v16
vwmaccu.vx v10,t2,v18
vwmaccu.vx v12,t4,v20
vwmaccu.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v14,t6,v22,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v14,t6,v22,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v10,t2,v18,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmacc.vv v8,v16,v24
vwmacc.vv v10,v18,v26
vwmacc.vv v12,v20,v28
vwmacc.vv v14,v22,v30
vwmacc.vv v8,v16,v24
vwmacc.vv v10,v18,v26
vwmacc.vv v12,v20,v28
vwmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmacc.vv v8,v16,v24
vwmacc.vv v10,v18,v26
vwmacc.vv v12,v20,v28
vwmacc.vv v14,v22,v30
vwmacc.vv v8,v16,v24
vwmacc.vv v10,v18,v26
vwmacc.vv v12,v20,v28
vwmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v14,v22,v30,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v14,v22,v30,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v10,v18,v26,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmacc.vx v8,t0,v16
vwmacc.vx v10,t2,v18
vwmacc.vx v12,t4,v20
vwmacc.vx v14,t6,v22
vwmacc.vx v8,t0,v16
vwmacc.vx v10,t2,v18
vwmacc.vx v12,t4,v20
vwmacc.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmacc.vx v8,t0,v16
vwmacc.vx v10,t2,v18
vwmacc.vx v12,t4,v20
vwmacc.vx v14,t6,v22
vwmacc.vx v8,t0,v16
vwmacc.vx v10,t2,v18
vwmacc.vx v12,t4,v20
vwmacc.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v14,t6,v22,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v14,t6,v22,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v10,t2,v18,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvv_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v14,v22,v30
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v14,v22,v30
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v10,v18,v26
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvvm_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v14,v22,v30,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v14,v22,v30,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v10,v18,v26,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v14,t6,v22
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v14,t6,v22
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v10,t2,v18
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v14,t6,v22,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v14,t6,v22,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v10,t2,v18,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvx_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccus.vx v8,t0,v16
vwmaccus.vx v10,t2,v18
vwmaccus.vx v12,t4,v20
vwmaccus.vx v14,t6,v22
vwmaccus.vx v8,t0,v16
vwmaccus.vx v10,t2,v18
vwmaccus.vx v12,t4,v20
vwmaccus.vx v14,t6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccus.vx v8,t0,v16
vwmaccus.vx v10,t2,v18
vwmaccus.vx v12,t4,v20
vwmaccus.vx v14,t6,v22
vwmaccus.vx v8,t0,v16
vwmaccus.vx v10,t2,v18
vwmaccus.vx v12,t4,v20
vwmaccus.vx v14,t6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvxm_m2:
	m_nop
	li a0, WARMUP
1:

vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v14,t6,v22,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v14,t6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v14,t6,v22,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v10,t2,v18,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v14,t6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vfadd.vv v8,v16,v24
vfadd.vv v10,v18,v26
vfadd.vv v12,v20,v28
vfadd.vv v14,v22,v30
vfadd.vv v8,v16,v24
vfadd.vv v10,v18,v26
vfadd.vv v12,v20,v28
vfadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfadd.vv v8,v16,v24
vfadd.vv v10,v18,v26
vfadd.vv v12,v20,v28
vfadd.vv v14,v22,v30
vfadd.vv v8,v16,v24
vfadd.vv v10,v18,v26
vfadd.vv v12,v20,v28
vfadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfadd.vv v8,v16,v24,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v14,v22,v30,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfadd.vv v8,v16,v24,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v14,v22,v30,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v10,v18,v26,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvf_m2:
	m_nop
	li a0, WARMUP
1:

vfadd.vf v8,v16,ft0
vfadd.vf v10,v18,ft2
vfadd.vf v12,v20,ft4
vfadd.vf v14,v22,ft6
vfadd.vf v8,v16,ft0
vfadd.vf v10,v18,ft2
vfadd.vf v12,v20,ft4
vfadd.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfadd.vf v8,v16,ft0
vfadd.vf v10,v18,ft2
vfadd.vf v12,v20,ft4
vfadd.vf v14,v22,ft6
vfadd.vf v8,v16,ft0
vfadd.vf v10,v18,ft2
vfadd.vf v12,v20,ft4
vfadd.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v14,v22,ft6,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v14,v22,ft6,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v10,v18,ft2,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vfsub.vv v8,v16,v24
vfsub.vv v10,v18,v26
vfsub.vv v12,v20,v28
vfsub.vv v14,v22,v30
vfsub.vv v8,v16,v24
vfsub.vv v10,v18,v26
vfsub.vv v12,v20,v28
vfsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsub.vv v8,v16,v24
vfsub.vv v10,v18,v26
vfsub.vv v12,v20,v28
vfsub.vv v14,v22,v30
vfsub.vv v8,v16,v24
vfsub.vv v10,v18,v26
vfsub.vv v12,v20,v28
vfsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfsub.vv v8,v16,v24,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v14,v22,v30,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsub.vv v8,v16,v24,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v14,v22,v30,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v10,v18,v26,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvf_m2:
	m_nop
	li a0, WARMUP
1:

vfsub.vf v8,v16,ft0
vfsub.vf v10,v18,ft2
vfsub.vf v12,v20,ft4
vfsub.vf v14,v22,ft6
vfsub.vf v8,v16,ft0
vfsub.vf v10,v18,ft2
vfsub.vf v12,v20,ft4
vfsub.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsub.vf v8,v16,ft0
vfsub.vf v10,v18,ft2
vfsub.vf v12,v20,ft4
vfsub.vf v14,v22,ft6
vfsub.vf v8,v16,ft0
vfsub.vf v10,v18,ft2
vfsub.vf v12,v20,ft4
vfsub.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v14,v22,ft6,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v14,v22,ft6,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v10,v18,ft2,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmin.vv v8,v16,v24
vfmin.vv v10,v18,v26
vfmin.vv v12,v20,v28
vfmin.vv v14,v22,v30
vfmin.vv v8,v16,v24
vfmin.vv v10,v18,v26
vfmin.vv v12,v20,v28
vfmin.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmin.vv v8,v16,v24
vfmin.vv v10,v18,v26
vfmin.vv v12,v20,v28
vfmin.vv v14,v22,v30
vfmin.vv v8,v16,v24
vfmin.vv v10,v18,v26
vfmin.vv v12,v20,v28
vfmin.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmin.vv v8,v16,v24,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v14,v22,v30,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmin.vv v8,v16,v24,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v14,v22,v30,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v10,v18,v26,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmin.vf v8,v16,ft0
vfmin.vf v10,v18,ft2
vfmin.vf v12,v20,ft4
vfmin.vf v14,v22,ft6
vfmin.vf v8,v16,ft0
vfmin.vf v10,v18,ft2
vfmin.vf v12,v20,ft4
vfmin.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmin.vf v8,v16,ft0
vfmin.vf v10,v18,ft2
vfmin.vf v12,v20,ft4
vfmin.vf v14,v22,ft6
vfmin.vf v8,v16,ft0
vfmin.vf v10,v18,ft2
vfmin.vf v12,v20,ft4
vfmin.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v14,v22,ft6,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v14,v22,ft6,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v10,v18,ft2,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmax.vv v8,v16,v24
vfmax.vv v10,v18,v26
vfmax.vv v12,v20,v28
vfmax.vv v14,v22,v30
vfmax.vv v8,v16,v24
vfmax.vv v10,v18,v26
vfmax.vv v12,v20,v28
vfmax.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmax.vv v8,v16,v24
vfmax.vv v10,v18,v26
vfmax.vv v12,v20,v28
vfmax.vv v14,v22,v30
vfmax.vv v8,v16,v24
vfmax.vv v10,v18,v26
vfmax.vv v12,v20,v28
vfmax.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmax.vv v8,v16,v24,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v14,v22,v30,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmax.vv v8,v16,v24,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v14,v22,v30,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v10,v18,v26,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmax.vf v8,v16,ft0
vfmax.vf v10,v18,ft2
vfmax.vf v12,v20,ft4
vfmax.vf v14,v22,ft6
vfmax.vf v8,v16,ft0
vfmax.vf v10,v18,ft2
vfmax.vf v12,v20,ft4
vfmax.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmax.vf v8,v16,ft0
vfmax.vf v10,v18,ft2
vfmax.vf v12,v20,ft4
vfmax.vf v14,v22,ft6
vfmax.vf v8,v16,ft0
vfmax.vf v10,v18,ft2
vfmax.vf v12,v20,ft4
vfmax.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v14,v22,ft6,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v14,v22,ft6,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v10,v18,ft2,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvv_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnj.vv v8,v16,v24
vfsgnj.vv v10,v18,v26
vfsgnj.vv v12,v20,v28
vfsgnj.vv v14,v22,v30
vfsgnj.vv v8,v16,v24
vfsgnj.vv v10,v18,v26
vfsgnj.vv v12,v20,v28
vfsgnj.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnj.vv v8,v16,v24
vfsgnj.vv v10,v18,v26
vfsgnj.vv v12,v20,v28
vfsgnj.vv v14,v22,v30
vfsgnj.vv v8,v16,v24
vfsgnj.vv v10,v18,v26
vfsgnj.vv v12,v20,v28
vfsgnj.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v14,v22,v30,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v14,v22,v30,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v10,v18,v26,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvf_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnj.vf v8,v16,ft0
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v14,v22,ft6
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnj.vf v8,v16,ft0
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v14,v22,ft6
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v10,v18,ft2
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v14,v22,ft6,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v14,v22,ft6,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v10,v18,ft2,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvv_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v14,v22,v30
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v14,v22,v30
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v10,v18,v26
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v14,v22,v30,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v14,v22,v30,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v10,v18,v26,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvf_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v14,v22,ft6
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v14,v22,ft6
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v10,v18,ft2
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v10,v18,ft2,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvv_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v14,v22,v30
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v14,v22,v30
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v10,v18,v26
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v14,v22,v30,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v14,v22,v30,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v10,v18,v26,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvf_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v14,v22,ft6
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v14,v22,ft6
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v10,v18,ft2
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v10,v18,ft2,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvf_m2:
	m_nop
	li a0, WARMUP
1:

vfslide1up.vf v8,v16,ft0
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v14,v22,ft6
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfslide1up.vf v8,v16,ft0
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v14,v22,ft6
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v10,v18,ft2
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v14,v22,ft6,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v14,v22,ft6,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v10,v18,ft2,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvf_m2:
	m_nop
	li a0, WARMUP
1:

vfslide1down.vf v8,v16,ft0
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v14,v22,ft6
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfslide1down.vf v8,v16,ft0
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v14,v22,ft6
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v10,v18,ft2
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v14,v22,ft6,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v14,v22,ft6,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v10,v18,ft2,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfredusumvs_m2:
	m_nop
	li a0, WARMUP
1:

vfredusum.vs v8,v16,v24
vfredusum.vs v10,v18,v26
vfredusum.vs v12,v20,v28
vfredusum.vs v14,v22,v30
vfredusum.vs v8,v16,v24
vfredusum.vs v10,v18,v26
vfredusum.vs v12,v20,v28
vfredusum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredusum.vs v8,v16,v24
vfredusum.vs v10,v18,v26
vfredusum.vs v12,v20,v28
vfredusum.vs v14,v22,v30
vfredusum.vs v8,v16,v24
vfredusum.vs v10,v18,v26
vfredusum.vs v12,v20,v28
vfredusum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredusumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v14,v22,v30,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v14,v22,v30,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v10,v18,v26,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvs_m2:
	m_nop
	li a0, WARMUP
1:

vfredosum.vs v8,v16,v24
vfredosum.vs v10,v18,v26
vfredosum.vs v12,v20,v28
vfredosum.vs v14,v22,v30
vfredosum.vs v8,v16,v24
vfredosum.vs v10,v18,v26
vfredosum.vs v12,v20,v28
vfredosum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredosum.vs v8,v16,v24
vfredosum.vs v10,v18,v26
vfredosum.vs v12,v20,v28
vfredosum.vs v14,v22,v30
vfredosum.vs v8,v16,v24
vfredosum.vs v10,v18,v26
vfredosum.vs v12,v20,v28
vfredosum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v14,v22,v30,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v14,v22,v30,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v10,v18,v26,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvs_m2:
	m_nop
	li a0, WARMUP
1:

vfredmin.vs v8,v16,v24
vfredmin.vs v10,v18,v26
vfredmin.vs v12,v20,v28
vfredmin.vs v14,v22,v30
vfredmin.vs v8,v16,v24
vfredmin.vs v10,v18,v26
vfredmin.vs v12,v20,v28
vfredmin.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredmin.vs v8,v16,v24
vfredmin.vs v10,v18,v26
vfredmin.vs v12,v20,v28
vfredmin.vs v14,v22,v30
vfredmin.vs v8,v16,v24
vfredmin.vs v10,v18,v26
vfredmin.vs v12,v20,v28
vfredmin.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v14,v22,v30,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v14,v22,v30,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v10,v18,v26,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvs_m2:
	m_nop
	li a0, WARMUP
1:

vfredmax.vs v8,v16,v24
vfredmax.vs v10,v18,v26
vfredmax.vs v12,v20,v28
vfredmax.vs v14,v22,v30
vfredmax.vs v8,v16,v24
vfredmax.vs v10,v18,v26
vfredmax.vs v12,v20,v28
vfredmax.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredmax.vs v8,v16,v24
vfredmax.vs v10,v18,v26
vfredmax.vs v12,v20,v28
vfredmax.vs v14,v22,v30
vfredmax.vs v8,v16,v24
vfredmax.vs v10,v18,v26
vfredmax.vs v12,v20,v28
vfredmax.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v14,v22,v30,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v14,v22,v30,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v10,v18,v26,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmergevfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v14,v22,ft6,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v14,v22,ft6,v0


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v14,v22,ft6,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v10,v18,ft2,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v14,v22,ft6,v0


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmv.v.f v8,ft0
vfmv.v.f v10,ft2
vfmv.v.f v12,ft4
vfmv.v.f v14,ft6
vfmv.v.f v8,ft0
vfmv.v.f v10,ft2
vfmv.v.f v12,ft4
vfmv.v.f v14,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmv.v.f v8,ft0
vfmv.v.f v10,ft2
vfmv.v.f v12,ft4
vfmv.v.f v14,ft6
vfmv.v.f v8,ft0
vfmv.v.f v10,ft2
vfmv.v.f v12,ft4
vfmv.v.f v14,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmfeqvv_m2:
	m_nop
	li a0, WARMUP
1:

vmfeq.vv v8,v16,v24
vmfeq.vv v10,v18,v26
vmfeq.vv v12,v20,v28
vmfeq.vv v14,v22,v30
vmfeq.vv v8,v16,v24
vmfeq.vv v10,v18,v26
vmfeq.vv v12,v20,v28
vmfeq.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfeq.vv v8,v16,v24
vmfeq.vv v10,v18,v26
vmfeq.vv v12,v20,v28
vmfeq.vv v14,v22,v30
vmfeq.vv v8,v16,v24
vmfeq.vv v10,v18,v26
vmfeq.vv v12,v20,v28
vmfeq.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v14,v22,v30,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v14,v22,v30,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v10,v18,v26,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvf_m2:
	m_nop
	li a0, WARMUP
1:

vmfeq.vf v8,v16,ft0
vmfeq.vf v10,v18,ft2
vmfeq.vf v12,v20,ft4
vmfeq.vf v14,v22,ft6
vmfeq.vf v8,v16,ft0
vmfeq.vf v10,v18,ft2
vmfeq.vf v12,v20,ft4
vmfeq.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfeq.vf v8,v16,ft0
vmfeq.vf v10,v18,ft2
vmfeq.vf v12,v20,ft4
vmfeq.vf v14,v22,ft6
vmfeq.vf v8,v16,ft0
vmfeq.vf v10,v18,ft2
vmfeq.vf v12,v20,ft4
vmfeq.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvfm_m2:
	m_nop
	li a0, WARMUP
1:

vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v14,v22,ft6,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v14,v22,ft6,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v10,v18,ft2,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevv_m2:
	m_nop
	li a0, WARMUP
1:

vmfle.vv v8,v16,v24
vmfle.vv v10,v18,v26
vmfle.vv v12,v20,v28
vmfle.vv v14,v22,v30
vmfle.vv v8,v16,v24
vmfle.vv v10,v18,v26
vmfle.vv v12,v20,v28
vmfle.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfle.vv v8,v16,v24
vmfle.vv v10,v18,v26
vmfle.vv v12,v20,v28
vmfle.vv v14,v22,v30
vmfle.vv v8,v16,v24
vmfle.vv v10,v18,v26
vmfle.vv v12,v20,v28
vmfle.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmfle.vv v8,v16,v24,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v14,v22,v30,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfle.vv v8,v16,v24,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v14,v22,v30,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v10,v18,v26,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevf_m2:
	m_nop
	li a0, WARMUP
1:

vmfle.vf v8,v16,ft0
vmfle.vf v10,v18,ft2
vmfle.vf v12,v20,ft4
vmfle.vf v14,v22,ft6
vmfle.vf v8,v16,ft0
vmfle.vf v10,v18,ft2
vmfle.vf v12,v20,ft4
vmfle.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfle.vf v8,v16,ft0
vmfle.vf v10,v18,ft2
vmfle.vf v12,v20,ft4
vmfle.vf v14,v22,ft6
vmfle.vf v8,v16,ft0
vmfle.vf v10,v18,ft2
vmfle.vf v12,v20,ft4
vmfle.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevfm_m2:
	m_nop
	li a0, WARMUP
1:

vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v14,v22,ft6,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v14,v22,ft6,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v10,v18,ft2,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvv_m2:
	m_nop
	li a0, WARMUP
1:

vmflt.vv v8,v16,v24
vmflt.vv v10,v18,v26
vmflt.vv v12,v20,v28
vmflt.vv v14,v22,v30
vmflt.vv v8,v16,v24
vmflt.vv v10,v18,v26
vmflt.vv v12,v20,v28
vmflt.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmflt.vv v8,v16,v24
vmflt.vv v10,v18,v26
vmflt.vv v12,v20,v28
vmflt.vv v14,v22,v30
vmflt.vv v8,v16,v24
vmflt.vv v10,v18,v26
vmflt.vv v12,v20,v28
vmflt.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmflt.vv v8,v16,v24,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v14,v22,v30,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmflt.vv v8,v16,v24,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v14,v22,v30,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v10,v18,v26,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvf_m2:
	m_nop
	li a0, WARMUP
1:

vmflt.vf v8,v16,ft0
vmflt.vf v10,v18,ft2
vmflt.vf v12,v20,ft4
vmflt.vf v14,v22,ft6
vmflt.vf v8,v16,ft0
vmflt.vf v10,v18,ft2
vmflt.vf v12,v20,ft4
vmflt.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmflt.vf v8,v16,ft0
vmflt.vf v10,v18,ft2
vmflt.vf v12,v20,ft4
vmflt.vf v14,v22,ft6
vmflt.vf v8,v16,ft0
vmflt.vf v10,v18,ft2
vmflt.vf v12,v20,ft4
vmflt.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvfm_m2:
	m_nop
	li a0, WARMUP
1:

vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v14,v22,ft6,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v14,v22,ft6,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v10,v18,ft2,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevv_m2:
	m_nop
	li a0, WARMUP
1:

vmfne.vv v8,v16,v24
vmfne.vv v10,v18,v26
vmfne.vv v12,v20,v28
vmfne.vv v14,v22,v30
vmfne.vv v8,v16,v24
vmfne.vv v10,v18,v26
vmfne.vv v12,v20,v28
vmfne.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfne.vv v8,v16,v24
vmfne.vv v10,v18,v26
vmfne.vv v12,v20,v28
vmfne.vv v14,v22,v30
vmfne.vv v8,v16,v24
vmfne.vv v10,v18,v26
vmfne.vv v12,v20,v28
vmfne.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmfne.vv v8,v16,v24,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v14,v22,v30,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfne.vv v8,v16,v24,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v14,v22,v30,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v10,v18,v26,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevf_m2:
	m_nop
	li a0, WARMUP
1:

vmfne.vf v8,v16,ft0
vmfne.vf v10,v18,ft2
vmfne.vf v12,v20,ft4
vmfne.vf v14,v22,ft6
vmfne.vf v8,v16,ft0
vmfne.vf v10,v18,ft2
vmfne.vf v12,v20,ft4
vmfne.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfne.vf v8,v16,ft0
vmfne.vf v10,v18,ft2
vmfne.vf v12,v20,ft4
vmfne.vf v14,v22,ft6
vmfne.vf v8,v16,ft0
vmfne.vf v10,v18,ft2
vmfne.vf v12,v20,ft4
vmfne.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevfm_m2:
	m_nop
	li a0, WARMUP
1:

vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v14,v22,ft6,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v14,v22,ft6,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v10,v18,ft2,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvv_m2:
	m_nop
	li a0, WARMUP
1:

vmfgt.vv v8,v16,v24
vmfgt.vv v10,v18,v26
vmfgt.vv v12,v20,v28
vmfgt.vv v14,v22,v30
vmfgt.vv v8,v16,v24
vmfgt.vv v10,v18,v26
vmfgt.vv v12,v20,v28
vmfgt.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfgt.vv v8,v16,v24
vmfgt.vv v10,v18,v26
vmfgt.vv v12,v20,v28
vmfgt.vv v14,v22,v30
vmfgt.vv v8,v16,v24
vmfgt.vv v10,v18,v26
vmfgt.vv v12,v20,v28
vmfgt.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvvm_m2:
	m_nop
	li a0, WARMUP
1:

vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v14,v22,v30,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v14,v22,v30,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v10,v18,v26,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvf_m2:
	m_nop
	li a0, WARMUP
1:

vmfgt.vf v8,v16,ft0
vmfgt.vf v10,v18,ft2
vmfgt.vf v12,v20,ft4
vmfgt.vf v14,v22,ft6
vmfgt.vf v8,v16,ft0
vmfgt.vf v10,v18,ft2
vmfgt.vf v12,v20,ft4
vmfgt.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfgt.vf v8,v16,ft0
vmfgt.vf v10,v18,ft2
vmfgt.vf v12,v20,ft4
vmfgt.vf v14,v22,ft6
vmfgt.vf v8,v16,ft0
vmfgt.vf v10,v18,ft2
vmfgt.vf v12,v20,ft4
vmfgt.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvfm_m2:
	m_nop
	li a0, WARMUP
1:

vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v14,v22,ft6,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v14,v22,ft6,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v10,v18,ft2,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevv_m2:
	m_nop
	li a0, WARMUP
1:

vmfge.vv v8,v16,v24
vmfge.vv v10,v18,v26
vmfge.vv v12,v20,v28
vmfge.vv v14,v22,v30
vmfge.vv v8,v16,v24
vmfge.vv v10,v18,v26
vmfge.vv v12,v20,v28
vmfge.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfge.vv v8,v16,v24
vmfge.vv v10,v18,v26
vmfge.vv v12,v20,v28
vmfge.vv v14,v22,v30
vmfge.vv v8,v16,v24
vmfge.vv v10,v18,v26
vmfge.vv v12,v20,v28
vmfge.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevvm_m2:
	m_nop
	li a0, WARMUP
1:

vmfge.vv v8,v16,v24,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v14,v22,v30,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfge.vv v8,v16,v24,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v14,v22,v30,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v10,v18,v26,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevf_m2:
	m_nop
	li a0, WARMUP
1:

vmfge.vf v8,v16,ft0
vmfge.vf v10,v18,ft2
vmfge.vf v12,v20,ft4
vmfge.vf v14,v22,ft6
vmfge.vf v8,v16,ft0
vmfge.vf v10,v18,ft2
vmfge.vf v12,v20,ft4
vmfge.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfge.vf v8,v16,ft0
vmfge.vf v10,v18,ft2
vmfge.vf v12,v20,ft4
vmfge.vf v14,v22,ft6
vmfge.vf v8,v16,ft0
vmfge.vf v10,v18,ft2
vmfge.vf v12,v20,ft4
vmfge.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevfm_m2:
	m_nop
	li a0, WARMUP
1:

vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v14,v22,ft6,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v14,v22,ft6,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v10,v18,ft2,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfdivvv_m2:
	m_nop
	li a0, WARMUP
1:

vfdiv.vv v8,v16,v24
vfdiv.vv v10,v18,v26
vfdiv.vv v12,v20,v28
vfdiv.vv v14,v22,v30
vfdiv.vv v8,v16,v24
vfdiv.vv v10,v18,v26
vfdiv.vv v12,v20,v28
vfdiv.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfdiv.vv v8,v16,v24
vfdiv.vv v10,v18,v26
vfdiv.vv v12,v20,v28
vfdiv.vv v14,v22,v30
vfdiv.vv v8,v16,v24
vfdiv.vv v10,v18,v26
vfdiv.vv v12,v20,v28
vfdiv.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v14,v22,v30,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v14,v22,v30,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v10,v18,v26,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvf_m2:
	m_nop
	li a0, WARMUP
1:

vfdiv.vf v8,v16,ft0
vfdiv.vf v10,v18,ft2
vfdiv.vf v12,v20,ft4
vfdiv.vf v14,v22,ft6
vfdiv.vf v8,v16,ft0
vfdiv.vf v10,v18,ft2
vfdiv.vf v12,v20,ft4
vfdiv.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfdiv.vf v8,v16,ft0
vfdiv.vf v10,v18,ft2
vfdiv.vf v12,v20,ft4
vfdiv.vf v14,v22,ft6
vfdiv.vf v8,v16,ft0
vfdiv.vf v10,v18,ft2
vfdiv.vf v12,v20,ft4
vfdiv.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v14,v22,ft6,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v14,v22,ft6,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v10,v18,ft2,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvf_m2:
	m_nop
	li a0, WARMUP
1:

vfrdiv.vf v8,v16,ft0
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v14,v22,ft6
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrdiv.vf v8,v16,ft0
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v14,v22,ft6
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v10,v18,ft2
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v14,v22,ft6,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v14,v22,ft6,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v10,v18,ft2,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmul.vv v8,v16,v24
vfmul.vv v10,v18,v26
vfmul.vv v12,v20,v28
vfmul.vv v14,v22,v30
vfmul.vv v8,v16,v24
vfmul.vv v10,v18,v26
vfmul.vv v12,v20,v28
vfmul.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmul.vv v8,v16,v24
vfmul.vv v10,v18,v26
vfmul.vv v12,v20,v28
vfmul.vv v14,v22,v30
vfmul.vv v8,v16,v24
vfmul.vv v10,v18,v26
vfmul.vv v12,v20,v28
vfmul.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmul.vv v8,v16,v24,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v14,v22,v30,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmul.vv v8,v16,v24,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v14,v22,v30,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v10,v18,v26,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmul.vf v8,v16,ft0
vfmul.vf v10,v18,ft2
vfmul.vf v12,v20,ft4
vfmul.vf v14,v22,ft6
vfmul.vf v8,v16,ft0
vfmul.vf v10,v18,ft2
vfmul.vf v12,v20,ft4
vfmul.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmul.vf v8,v16,ft0
vfmul.vf v10,v18,ft2
vfmul.vf v12,v20,ft4
vfmul.vf v14,v22,ft6
vfmul.vf v8,v16,ft0
vfmul.vf v10,v18,ft2
vfmul.vf v12,v20,ft4
vfmul.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v14,v22,ft6,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v14,v22,ft6,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v10,v18,ft2,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvf_m2:
	m_nop
	li a0, WARMUP
1:

vfrsub.vf v8,v16,ft0
vfrsub.vf v10,v18,ft2
vfrsub.vf v12,v20,ft4
vfrsub.vf v14,v22,ft6
vfrsub.vf v8,v16,ft0
vfrsub.vf v10,v18,ft2
vfrsub.vf v12,v20,ft4
vfrsub.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrsub.vf v8,v16,ft0
vfrsub.vf v10,v18,ft2
vfrsub.vf v12,v20,ft4
vfrsub.vf v14,v22,ft6
vfrsub.vf v8,v16,ft0
vfrsub.vf v10,v18,ft2
vfrsub.vf v12,v20,ft4
vfrsub.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v14,v22,ft6,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v14,v22,ft6,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v10,v18,ft2,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmadd.vv v8,v16,v24
vfmadd.vv v10,v18,v26
vfmadd.vv v12,v20,v28
vfmadd.vv v14,v22,v30
vfmadd.vv v8,v16,v24
vfmadd.vv v10,v18,v26
vfmadd.vv v12,v20,v28
vfmadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmadd.vv v8,v16,v24
vfmadd.vv v10,v18,v26
vfmadd.vv v12,v20,v28
vfmadd.vv v14,v22,v30
vfmadd.vv v8,v16,v24
vfmadd.vv v10,v18,v26
vfmadd.vv v12,v20,v28
vfmadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v14,v22,v30,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v14,v22,v30,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v10,v18,v26,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmadd.vf v8,ft0,v16
vfmadd.vf v10,ft2,v18
vfmadd.vf v12,ft4,v20
vfmadd.vf v14,ft6,v22
vfmadd.vf v8,ft0,v16
vfmadd.vf v10,ft2,v18
vfmadd.vf v12,ft4,v20
vfmadd.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmadd.vf v8,ft0,v16
vfmadd.vf v10,ft2,v18
vfmadd.vf v12,ft4,v20
vfmadd.vf v14,ft6,v22
vfmadd.vf v8,ft0,v16
vfmadd.vf v10,ft2,v18
vfmadd.vf v12,ft4,v20
vfmadd.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v14,ft6,v22,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v14,ft6,v22,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v10,ft2,v18,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmsub.vv v8,v16,v24
vfmsub.vv v10,v18,v26
vfmsub.vv v12,v20,v28
vfmsub.vv v14,v22,v30
vfmsub.vv v8,v16,v24
vfmsub.vv v10,v18,v26
vfmsub.vv v12,v20,v28
vfmsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsub.vv v8,v16,v24
vfmsub.vv v10,v18,v26
vfmsub.vv v12,v20,v28
vfmsub.vv v14,v22,v30
vfmsub.vv v8,v16,v24
vfmsub.vv v10,v18,v26
vfmsub.vv v12,v20,v28
vfmsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v14,v22,v30,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v14,v22,v30,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v10,v18,v26,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmsub.vf v8,ft0,v16
vfmsub.vf v10,ft2,v18
vfmsub.vf v12,ft4,v20
vfmsub.vf v14,ft6,v22
vfmsub.vf v8,ft0,v16
vfmsub.vf v10,ft2,v18
vfmsub.vf v12,ft4,v20
vfmsub.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsub.vf v8,ft0,v16
vfmsub.vf v10,ft2,v18
vfmsub.vf v12,ft4,v20
vfmsub.vf v14,ft6,v22
vfmsub.vf v8,ft0,v16
vfmsub.vf v10,ft2,v18
vfmsub.vf v12,ft4,v20
vfmsub.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v14,ft6,v22,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v14,ft6,v22,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v10,ft2,v18,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmacc.vv v8,v16,v24
vfmacc.vv v10,v18,v26
vfmacc.vv v12,v20,v28
vfmacc.vv v14,v22,v30
vfmacc.vv v8,v16,v24
vfmacc.vv v10,v18,v26
vfmacc.vv v12,v20,v28
vfmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmacc.vv v8,v16,v24
vfmacc.vv v10,v18,v26
vfmacc.vv v12,v20,v28
vfmacc.vv v14,v22,v30
vfmacc.vv v8,v16,v24
vfmacc.vv v10,v18,v26
vfmacc.vv v12,v20,v28
vfmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v14,v22,v30,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v14,v22,v30,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v10,v18,v26,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmacc.vf v8,ft0,v16
vfmacc.vf v10,ft2,v18
vfmacc.vf v12,ft4,v20
vfmacc.vf v14,ft6,v22
vfmacc.vf v8,ft0,v16
vfmacc.vf v10,ft2,v18
vfmacc.vf v12,ft4,v20
vfmacc.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmacc.vf v8,ft0,v16
vfmacc.vf v10,ft2,v18
vfmacc.vf v12,ft4,v20
vfmacc.vf v14,ft6,v22
vfmacc.vf v8,ft0,v16
vfmacc.vf v10,ft2,v18
vfmacc.vf v12,ft4,v20
vfmacc.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v14,ft6,v22,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v14,ft6,v22,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v10,ft2,v18,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvv_m2:
	m_nop
	li a0, WARMUP
1:

vfmsac.vv v8,v16,v24
vfmsac.vv v10,v18,v26
vfmsac.vv v12,v20,v28
vfmsac.vv v14,v22,v30
vfmsac.vv v8,v16,v24
vfmsac.vv v10,v18,v26
vfmsac.vv v12,v20,v28
vfmsac.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsac.vv v8,v16,v24
vfmsac.vv v10,v18,v26
vfmsac.vv v12,v20,v28
vfmsac.vv v14,v22,v30
vfmsac.vv v8,v16,v24
vfmsac.vv v10,v18,v26
vfmsac.vv v12,v20,v28
vfmsac.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v14,v22,v30,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v14,v22,v30,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v10,v18,v26,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvf_m2:
	m_nop
	li a0, WARMUP
1:

vfmsac.vf v8,ft0,v16
vfmsac.vf v10,ft2,v18
vfmsac.vf v12,ft4,v20
vfmsac.vf v14,ft6,v22
vfmsac.vf v8,ft0,v16
vfmsac.vf v10,ft2,v18
vfmsac.vf v12,ft4,v20
vfmsac.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsac.vf v8,ft0,v16
vfmsac.vf v10,ft2,v18
vfmsac.vf v12,ft4,v20
vfmsac.vf v14,ft6,v22
vfmsac.vf v8,ft0,v16
vfmsac.vf v10,ft2,v18
vfmsac.vf v12,ft4,v20
vfmsac.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v14,ft6,v22,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v14,ft6,v22,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v10,ft2,v18,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfnmsacvv_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsac.vv v8,v16,v24
vfnmsac.vv v10,v18,v26
vfnmsac.vv v12,v20,v28
vfnmsac.vv v14,v22,v30
vfnmsac.vv v8,v16,v24
vfnmsac.vv v10,v18,v26
vfnmsac.vv v12,v20,v28
vfnmsac.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsac.vv v8,v16,v24
vfnmsac.vv v10,v18,v26
vfnmsac.vv v12,v20,v28
vfnmsac.vv v14,v22,v30
vfnmsac.vv v8,v16,v24
vfnmsac.vv v10,v18,v26
vfnmsac.vv v12,v20,v28
vfnmsac.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v14,v22,v30,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v14,v22,v30,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v10,v18,v26,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvf_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsac.vf v8,ft0,v16
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v14,ft6,v22
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsac.vf v8,ft0,v16
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v14,ft6,v22
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v10,ft2,v18
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v14,ft6,v22,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v14,ft6,v22,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v10,ft2,v18,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vfnmacc.vv v8,v16,v24
vfnmacc.vv v10,v18,v26
vfnmacc.vv v12,v20,v28
vfnmacc.vv v14,v22,v30
vfnmacc.vv v8,v16,v24
vfnmacc.vv v10,v18,v26
vfnmacc.vv v12,v20,v28
vfnmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmacc.vv v8,v16,v24
vfnmacc.vv v10,v18,v26
vfnmacc.vv v12,v20,v28
vfnmacc.vv v14,v22,v30
vfnmacc.vv v8,v16,v24
vfnmacc.vv v10,v18,v26
vfnmacc.vv v12,v20,v28
vfnmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v14,v22,v30,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v14,v22,v30,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v10,v18,v26,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvf_m2:
	m_nop
	li a0, WARMUP
1:

vfnmacc.vf v8,ft0,v16
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v14,ft6,v22
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmacc.vf v8,ft0,v16
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v14,ft6,v22
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v10,ft2,v18
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v14,ft6,v22,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v14,ft6,v22,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v10,ft2,v18,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsub.vv v8,v16,v24
vfnmsub.vv v10,v18,v26
vfnmsub.vv v12,v20,v28
vfnmsub.vv v14,v22,v30
vfnmsub.vv v8,v16,v24
vfnmsub.vv v10,v18,v26
vfnmsub.vv v12,v20,v28
vfnmsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsub.vv v8,v16,v24
vfnmsub.vv v10,v18,v26
vfnmsub.vv v12,v20,v28
vfnmsub.vv v14,v22,v30
vfnmsub.vv v8,v16,v24
vfnmsub.vv v10,v18,v26
vfnmsub.vv v12,v20,v28
vfnmsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v14,v22,v30,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v14,v22,v30,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v10,v18,v26,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvf_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsub.vf v8,ft0,v16
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v14,ft6,v22
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsub.vf v8,ft0,v16
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v14,ft6,v22
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v10,ft2,v18
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v14,ft6,v22,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v14,ft6,v22,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v10,ft2,v18,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vfnmadd.vv v8,v16,v24
vfnmadd.vv v10,v18,v26
vfnmadd.vv v12,v20,v28
vfnmadd.vv v14,v22,v30
vfnmadd.vv v8,v16,v24
vfnmadd.vv v10,v18,v26
vfnmadd.vv v12,v20,v28
vfnmadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmadd.vv v8,v16,v24
vfnmadd.vv v10,v18,v26
vfnmadd.vv v12,v20,v28
vfnmadd.vv v14,v22,v30
vfnmadd.vv v8,v16,v24
vfnmadd.vv v10,v18,v26
vfnmadd.vv v12,v20,v28
vfnmadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v14,v22,v30,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v14,v22,v30,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v10,v18,v26,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvf_m2:
	m_nop
	li a0, WARMUP
1:

vfnmadd.vf v8,ft0,v16
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v14,ft6,v22
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmadd.vf v8,ft0,v16
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v14,ft6,v22
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v10,ft2,v18
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v14,ft6,v22,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v14,ft6,v22,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v10,ft2,v18,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwredsumuvs_m2:
	m_nop
	li a0, WARMUP
1:

vwredsumu.vs v8,v16,v24
vwredsumu.vs v10,v18,v26
vwredsumu.vs v12,v20,v28
vwredsumu.vs v14,v22,v30
vwredsumu.vs v8,v16,v24
vwredsumu.vs v10,v18,v26
vwredsumu.vs v12,v20,v28
vwredsumu.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwredsumu.vs v8,v16,v24
vwredsumu.vs v10,v18,v26
vwredsumu.vs v12,v20,v28
vwredsumu.vs v14,v22,v30
vwredsumu.vs v8,v16,v24
vwredsumu.vs v10,v18,v26
vwredsumu.vs v12,v20,v28
vwredsumu.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumuvsm_m2:
	m_nop
	li a0, WARMUP
1:

vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v14,v22,v30,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v14,v22,v30,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v10,v18,v26,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvs_m2:
	m_nop
	li a0, WARMUP
1:

vwredsum.vs v8,v16,v24
vwredsum.vs v10,v18,v26
vwredsum.vs v12,v20,v28
vwredsum.vs v14,v22,v30
vwredsum.vs v8,v16,v24
vwredsum.vs v10,v18,v26
vwredsum.vs v12,v20,v28
vwredsum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwredsum.vs v8,v16,v24
vwredsum.vs v10,v18,v26
vwredsum.vs v12,v20,v28
vwredsum.vs v14,v22,v30
vwredsum.vs v8,v16,v24
vwredsum.vs v10,v18,v26
vwredsum.vs v12,v20,v28
vwredsum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v14,v22,v30,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v14,v22,v30,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v10,v18,v26,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwaddvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.vv v8,v16,v24
vfwadd.vv v10,v18,v26
vfwadd.vv v12,v20,v28
vfwadd.vv v14,v22,v30
vfwadd.vv v8,v16,v24
vfwadd.vv v10,v18,v26
vfwadd.vv v12,v20,v28
vfwadd.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.vv v8,v16,v24
vfwadd.vv v10,v18,v26
vfwadd.vv v12,v20,v28
vfwadd.vv v14,v22,v30
vfwadd.vv v8,v16,v24
vfwadd.vv v10,v18,v26
vfwadd.vv v12,v20,v28
vfwadd.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v14,v22,v30,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v14,v22,v30,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v10,v18,v26,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.vf v8,v16,ft0
vfwadd.vf v10,v18,ft2
vfwadd.vf v12,v20,ft4
vfwadd.vf v14,v22,ft6
vfwadd.vf v8,v16,ft0
vfwadd.vf v10,v18,ft2
vfwadd.vf v12,v20,ft4
vfwadd.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.vf v8,v16,ft0
vfwadd.vf v10,v18,ft2
vfwadd.vf v12,v20,ft4
vfwadd.vf v14,v22,ft6
vfwadd.vf v8,v16,ft0
vfwadd.vf v10,v18,ft2
vfwadd.vf v12,v20,ft4
vfwadd.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v14,v22,ft6,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v14,v22,ft6,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v10,v18,ft2,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.vv v8,v16,v24
vfwsub.vv v10,v18,v26
vfwsub.vv v12,v20,v28
vfwsub.vv v14,v22,v30
vfwsub.vv v8,v16,v24
vfwsub.vv v10,v18,v26
vfwsub.vv v12,v20,v28
vfwsub.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.vv v8,v16,v24
vfwsub.vv v10,v18,v26
vfwsub.vv v12,v20,v28
vfwsub.vv v14,v22,v30
vfwsub.vv v8,v16,v24
vfwsub.vv v10,v18,v26
vfwsub.vv v12,v20,v28
vfwsub.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v14,v22,v30,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v14,v22,v30,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v10,v18,v26,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.vf v8,v16,ft0
vfwsub.vf v10,v18,ft2
vfwsub.vf v12,v20,ft4
vfwsub.vf v14,v22,ft6
vfwsub.vf v8,v16,ft0
vfwsub.vf v10,v18,ft2
vfwsub.vf v12,v20,ft4
vfwsub.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.vf v8,v16,ft0
vfwsub.vf v10,v18,ft2
vfwsub.vf v12,v20,ft4
vfwsub.vf v14,v22,ft6
vfwsub.vf v8,v16,ft0
vfwsub.vf v10,v18,ft2
vfwsub.vf v12,v20,ft4
vfwsub.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v14,v22,ft6,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v14,v22,ft6,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v10,v18,ft2,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwv_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.wv v8,v16,v24
vfwadd.wv v10,v18,v26
vfwadd.wv v12,v20,v28
vfwadd.wv v14,v22,v30
vfwadd.wv v8,v16,v24
vfwadd.wv v10,v18,v26
vfwadd.wv v12,v20,v28
vfwadd.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.wv v8,v16,v24
vfwadd.wv v10,v18,v26
vfwadd.wv v12,v20,v28
vfwadd.wv v14,v22,v30
vfwadd.wv v8,v16,v24
vfwadd.wv v10,v18,v26
vfwadd.wv v12,v20,v28
vfwadd.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v14,v22,v30,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v14,v22,v30,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v10,v18,v26,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwf_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.wf v8,v16,ft0
vfwadd.wf v10,v18,ft2
vfwadd.wf v12,v20,ft4
vfwadd.wf v14,v22,ft6
vfwadd.wf v8,v16,ft0
vfwadd.wf v10,v18,ft2
vfwadd.wf v12,v20,ft4
vfwadd.wf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.wf v8,v16,ft0
vfwadd.wf v10,v18,ft2
vfwadd.wf v12,v20,ft4
vfwadd.wf v14,v22,ft6
vfwadd.wf v8,v16,ft0
vfwadd.wf v10,v18,ft2
vfwadd.wf v12,v20,ft4
vfwadd.wf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v14,v22,ft6,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v14,v22,ft6,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v10,v18,ft2,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwv_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.wv v8,v16,v24
vfwsub.wv v10,v18,v26
vfwsub.wv v12,v20,v28
vfwsub.wv v14,v22,v30
vfwsub.wv v8,v16,v24
vfwsub.wv v10,v18,v26
vfwsub.wv v12,v20,v28
vfwsub.wv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.wv v8,v16,v24
vfwsub.wv v10,v18,v26
vfwsub.wv v12,v20,v28
vfwsub.wv v14,v22,v30
vfwsub.wv v8,v16,v24
vfwsub.wv v10,v18,v26
vfwsub.wv v12,v20,v28
vfwsub.wv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v14,v22,v30,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v14,v22,v30,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v10,v18,v26,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwf_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.wf v8,v16,ft0
vfwsub.wf v10,v18,ft2
vfwsub.wf v12,v20,ft4
vfwsub.wf v14,v22,ft6
vfwsub.wf v8,v16,ft0
vfwsub.wf v10,v18,ft2
vfwsub.wf v12,v20,ft4
vfwsub.wf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.wf v8,v16,ft0
vfwsub.wf v10,v18,ft2
vfwsub.wf v12,v20,ft4
vfwsub.wf v14,v22,ft6
vfwsub.wf v8,v16,ft0
vfwsub.wf v10,v18,ft2
vfwsub.wf v12,v20,ft4
vfwsub.wf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v14,v22,ft6,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v14,v22,ft6,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v10,v18,ft2,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwmul.vv v8,v16,v24
vfwmul.vv v10,v18,v26
vfwmul.vv v12,v20,v28
vfwmul.vv v14,v22,v30
vfwmul.vv v8,v16,v24
vfwmul.vv v10,v18,v26
vfwmul.vv v12,v20,v28
vfwmul.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmul.vv v8,v16,v24
vfwmul.vv v10,v18,v26
vfwmul.vv v12,v20,v28
vfwmul.vv v14,v22,v30
vfwmul.vv v8,v16,v24
vfwmul.vv v10,v18,v26
vfwmul.vv v12,v20,v28
vfwmul.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v14,v22,v30,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v14,v22,v30,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v10,v18,v26,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwmul.vf v8,v16,ft0
vfwmul.vf v10,v18,ft2
vfwmul.vf v12,v20,ft4
vfwmul.vf v14,v22,ft6
vfwmul.vf v8,v16,ft0
vfwmul.vf v10,v18,ft2
vfwmul.vf v12,v20,ft4
vfwmul.vf v14,v22,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmul.vf v8,v16,ft0
vfwmul.vf v10,v18,ft2
vfwmul.vf v12,v20,ft4
vfwmul.vf v14,v22,ft6
vfwmul.vf v8,v16,ft0
vfwmul.vf v10,v18,ft2
vfwmul.vf v12,v20,ft4
vfwmul.vf v14,v22,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v14,v22,ft6,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v14,v22,ft6,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v14,v22,ft6,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v10,v18,ft2,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v14,v22,ft6,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwmacc.vv v8,v16,v24
vfwmacc.vv v10,v18,v26
vfwmacc.vv v12,v20,v28
vfwmacc.vv v14,v22,v30
vfwmacc.vv v8,v16,v24
vfwmacc.vv v10,v18,v26
vfwmacc.vv v12,v20,v28
vfwmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmacc.vv v8,v16,v24
vfwmacc.vv v10,v18,v26
vfwmacc.vv v12,v20,v28
vfwmacc.vv v14,v22,v30
vfwmacc.vv v8,v16,v24
vfwmacc.vv v10,v18,v26
vfwmacc.vv v12,v20,v28
vfwmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v14,v22,v30,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v14,v22,v30,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v10,v18,v26,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwmacc.vf v8,ft0,v16
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v14,ft6,v22
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmacc.vf v8,ft0,v16
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v14,ft6,v22
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v10,ft2,v18
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v14,ft6,v22,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v14,ft6,v22,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v10,ft2,v18,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v14,v22,v30
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v14,v22,v30
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v10,v18,v26
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v14,v22,v30,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v14,v22,v30,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v10,v18,v26,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v14,ft6,v22
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v14,ft6,v22
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v10,ft2,v18
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v10,ft2,v18,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwmsac.vv v8,v16,v24
vfwmsac.vv v10,v18,v26
vfwmsac.vv v12,v20,v28
vfwmsac.vv v14,v22,v30
vfwmsac.vv v8,v16,v24
vfwmsac.vv v10,v18,v26
vfwmsac.vv v12,v20,v28
vfwmsac.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmsac.vv v8,v16,v24
vfwmsac.vv v10,v18,v26
vfwmsac.vv v12,v20,v28
vfwmsac.vv v14,v22,v30
vfwmsac.vv v8,v16,v24
vfwmsac.vv v10,v18,v26
vfwmsac.vv v12,v20,v28
vfwmsac.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v14,v22,v30,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v14,v22,v30,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v10,v18,v26,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwmsac.vf v8,ft0,v16
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v14,ft6,v22
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmsac.vf v8,ft0,v16
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v14,ft6,v22
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v10,ft2,v18
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v14,ft6,v22,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v14,ft6,v22,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v10,ft2,v18,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvv_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v14,v22,v30
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v14,v22,v30
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v10,v18,v26
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v14,v22,v30,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v14,v22,v30,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v10,v18,v26,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvf_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v14,ft6,v22
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v14,ft6,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v14,ft6,v22
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v10,ft2,v18
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v14,ft6,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvfm_m2:
	m_nop
	li a0, WARMUP
1:

vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v10,ft2,v18,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v14,ft6,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwredosumvs_m2:
	m_nop
	li a0, WARMUP
1:

vfwredosum.vs v8,v16,v24
vfwredosum.vs v10,v18,v26
vfwredosum.vs v12,v20,v28
vfwredosum.vs v14,v22,v30
vfwredosum.vs v8,v16,v24
vfwredosum.vs v10,v18,v26
vfwredosum.vs v12,v20,v28
vfwredosum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwredosum.vs v8,v16,v24
vfwredosum.vs v10,v18,v26
vfwredosum.vs v12,v20,v28
vfwredosum.vs v14,v22,v30
vfwredosum.vs v8,v16,v24
vfwredosum.vs v10,v18,v26
vfwredosum.vs v12,v20,v28
vfwredosum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredosumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v14,v22,v30,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v14,v22,v30,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v10,v18,v26,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvs_m2:
	m_nop
	li a0, WARMUP
1:

vfwredusum.vs v8,v16,v24
vfwredusum.vs v10,v18,v26
vfwredusum.vs v12,v20,v28
vfwredusum.vs v14,v22,v30
vfwredusum.vs v8,v16,v24
vfwredusum.vs v10,v18,v26
vfwredusum.vs v12,v20,v28
vfwredusum.vs v14,v22,v30


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwredusum.vs v8,v16,v24
vfwredusum.vs v10,v18,v26
vfwredusum.vs v12,v20,v28
vfwredusum.vs v14,v22,v30
vfwredusum.vs v8,v16,v24
vfwredusum.vs v10,v18,v26
vfwredusum.vs v12,v20,v28
vfwredusum.vs v14,v22,v30


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvsm_m2:
	m_nop
	li a0, WARMUP
1:

vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v14,v22,v30,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v14,v22,v30,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v14,v22,v30,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v10,v18,v26,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v14,v22,v30,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmvsx_m2:
	m_nop
	li a0, WARMUP
1:

vmv.s.x v8,t0
vmv.s.x v10,t2
vmv.s.x v12,t4
vmv.s.x v14,t6
vmv.s.x v8,t0
vmv.s.x v10,t2
vmv.s.x v12,t4
vmv.s.x v14,t6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv.s.x v8,t0
vmv.s.x v10,t2
vmv.s.x v12,t4
vmv.s.x v14,t6
vmv.s.x v8,t0
vmv.s.x v10,t2
vmv.s.x v12,t4
vmv.s.x v14,t6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvxs_m2:
	m_nop
	li a0, WARMUP
1:

vmv.x.s t0,v8
vmv.x.s t2,v10
vmv.x.s t4,v12
vmv.x.s t6,v14
vmv.x.s t0,v8
vmv.x.s t2,v10
vmv.x.s t4,v12
vmv.x.s t6,v14


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmv.x.s t0,v8
vmv.x.s t2,v10
vmv.x.s t4,v12
vmv.x.s t6,v14
vmv.x.s t0,v8
vmv.x.s t2,v10
vmv.x.s t4,v12
vmv.x.s t6,v14


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcpopm_m2:
	m_nop
	li a0, WARMUP
1:

vcpop.m t0,v8
vcpop.m t2,v10
vcpop.m t4,v12
vcpop.m t6,v14
vcpop.m t0,v8
vcpop.m t2,v10
vcpop.m t4,v12
vcpop.m t6,v14


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vcpop.m t0,v8
vcpop.m t2,v10
vcpop.m t4,v12
vcpop.m t6,v14
vcpop.m t0,v8
vcpop.m t2,v10
vcpop.m t4,v12
vcpop.m t6,v14


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vcpopmm_m2:
	m_nop
	li a0, WARMUP
1:

vcpop.m t0,v8,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t6,v14,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t6,v14,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vcpop.m t0,v8,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t6,v14,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t2,v10,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t6,v14,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstm_m2:
	m_1bit
	li a0, WARMUP
1:

vfirst.m t0,v8
vfirst.m t2,v10
vfirst.m t4,v12
vfirst.m t6,v14
vfirst.m t0,v8
vfirst.m t2,v10
vfirst.m t4,v12
vfirst.m t6,v14


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfirst.m t0,v8
vfirst.m t2,v10
vfirst.m t4,v12
vfirst.m t6,v14
vfirst.m t0,v8
vfirst.m t2,v10
vfirst.m t4,v12
vfirst.m t6,v14


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstmm_m2:
	m_1bit
	li a0, WARMUP
1:

vfirst.m t0,v8,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t6,v14,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t6,v14,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfirst.m t0,v8,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t6,v14,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t2,v10,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t6,v14,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf2 v8,v16
vzext.vf2 v10,v18
vzext.vf2 v12,v20
vzext.vf2 v14,v22
vzext.vf2 v8,v16
vzext.vf2 v10,v18
vzext.vf2 v12,v20
vzext.vf2 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf2 v8,v16
vzext.vf2 v10,v18
vzext.vf2 v12,v20
vzext.vf2 v14,v22
vzext.vf2 v8,v16
vzext.vf2 v10,v18
vzext.vf2 v12,v20
vzext.vf2 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2m_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf2 v8,v16,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v14,v22,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf2 v8,v16,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v14,v22,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v10,v18,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf2 v8,v16
vsext.vf2 v10,v18
vsext.vf2 v12,v20
vsext.vf2 v14,v22
vsext.vf2 v8,v16
vsext.vf2 v10,v18
vsext.vf2 v12,v20
vsext.vf2 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf2 v8,v16
vsext.vf2 v10,v18
vsext.vf2 v12,v20
vsext.vf2 v14,v22
vsext.vf2 v8,v16
vsext.vf2 v10,v18
vsext.vf2 v12,v20
vsext.vf2 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2m_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf2 v8,v16,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v14,v22,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf2 v8,v16,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v14,v22,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v10,v18,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf4 v8,v16
vzext.vf4 v10,v18
vzext.vf4 v12,v20
vzext.vf4 v14,v22
vzext.vf4 v8,v16
vzext.vf4 v10,v18
vzext.vf4 v12,v20
vzext.vf4 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf4 v8,v16
vzext.vf4 v10,v18
vzext.vf4 v12,v20
vzext.vf4 v14,v22
vzext.vf4 v8,v16
vzext.vf4 v10,v18
vzext.vf4 v12,v20
vzext.vf4 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4m_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf4 v8,v16,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v14,v22,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf4 v8,v16,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v14,v22,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v10,v18,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf4 v8,v16
vsext.vf4 v10,v18
vsext.vf4 v12,v20
vsext.vf4 v14,v22
vsext.vf4 v8,v16
vsext.vf4 v10,v18
vsext.vf4 v12,v20
vsext.vf4 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf4 v8,v16
vsext.vf4 v10,v18
vsext.vf4 v12,v20
vsext.vf4 v14,v22
vsext.vf4 v8,v16
vsext.vf4 v10,v18
vsext.vf4 v12,v20
vsext.vf4 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4m_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf4 v8,v16,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v14,v22,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf4 v8,v16,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v14,v22,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v10,v18,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf8 v8,v16
vzext.vf8 v10,v18
vzext.vf8 v12,v20
vzext.vf8 v14,v22
vzext.vf8 v8,v16
vzext.vf8 v10,v18
vzext.vf8 v12,v20
vzext.vf8 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf8 v8,v16
vzext.vf8 v10,v18
vzext.vf8 v12,v20
vzext.vf8 v14,v22
vzext.vf8 v8,v16
vzext.vf8 v10,v18
vzext.vf8 v12,v20
vzext.vf8 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8m_m2:
	m_1bit
	li a0, WARMUP
1:

vzext.vf8 v8,v16,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v14,v22,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vzext.vf8 v8,v16,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v14,v22,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v10,v18,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf8 v8,v16
vsext.vf8 v10,v18
vsext.vf8 v12,v20
vsext.vf8 v14,v22
vsext.vf8 v8,v16
vsext.vf8 v10,v18
vsext.vf8 v12,v20
vsext.vf8 v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf8 v8,v16
vsext.vf8 v10,v18
vsext.vf8 v12,v20
vsext.vf8 v14,v22
vsext.vf8 v8,v16
vsext.vf8 v10,v18
vsext.vf8 v12,v20
vsext.vf8 v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8m_m2:
	m_1bit
	li a0, WARMUP
1:

vsext.vf8 v8,v16,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v14,v22,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vsext.vf8 v8,v16,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v14,v22,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v10,v18,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmvfs_m2:
	m_nop
	li a0, WARMUP
1:

vfmv.f.s ft0,v8
vfmv.f.s ft2,v10
vfmv.f.s ft4,v12
vfmv.f.s ft6,v14
vfmv.f.s ft0,v8
vfmv.f.s ft2,v10
vfmv.f.s ft4,v12
vfmv.f.s ft6,v14


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmv.f.s ft0,v8
vfmv.f.s ft2,v10
vfmv.f.s ft4,v12
vfmv.f.s ft6,v14
vfmv.f.s ft0,v8
vfmv.f.s ft2,v10
vfmv.f.s ft4,v12
vfmv.f.s ft6,v14


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvsf_m2:
	m_nop
	li a0, WARMUP
1:

vfmv.s.f v8,ft0
vfmv.s.f v10,ft2
vfmv.s.f v12,ft4
vfmv.s.f v14,ft6
vfmv.s.f v8,ft0
vfmv.s.f v10,ft2
vfmv.s.f v12,ft4
vfmv.s.f v14,ft6


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfmv.s.f v8,ft0
vfmv.s.f v10,ft2
vfmv.s.f v12,ft4
vfmv.s.f v14,ft6
vfmv.s.f v8,ft0
vfmv.s.f v10,ft2
vfmv.s.f v12,ft4
vfmv.s.f v14,ft6


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfcvtxufv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v10,v18
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v14,v22
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v10,v18
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v10,v18
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v14,v22
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v10,v18
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxufvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v10,v18,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v14,v22,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v10,v18,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v10,v18,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v14,v22,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v10,v18,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.x.f.v v8,v16
vfcvt.x.f.v v10,v18
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v14,v22
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v10,v18
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.x.f.v v8,v16
vfcvt.x.f.v v10,v18
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v14,v22
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v10,v18
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v10,v18,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v14,v22,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v10,v18,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v10,v18,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v14,v22,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v10,v18,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v10,v18
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v14,v22
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v10,v18
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v10,v18
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v14,v22
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v10,v18
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v10,v18,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v14,v22,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v10,v18,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v10,v18,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v14,v22,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v10,v18,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.f.x.v v8,v16
vfcvt.f.x.v v10,v18
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v14,v22
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v10,v18
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.f.x.v v8,v16
vfcvt.f.x.v v10,v18
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v14,v22
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v10,v18
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v10,v18,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v14,v22,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v10,v18,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v10,v18,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v14,v22,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v10,v18,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v10,v18
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v14,v22
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v10,v18
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v10,v18
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v14,v22
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v10,v18
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v10,v18,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v14,v22,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v10,v18,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v10,v18,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v14,v22,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v10,v18,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufv_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v10,v18
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v14,v22
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v10,v18
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v10,v18
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v14,v22
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v10,v18
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufvm_m2:
	m_nop
	li a0, WARMUP
1:

vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v10,v18,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v14,v22,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v10,v18,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v10,v18,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v14,v22,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v10,v18,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwcvtxufv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v10,v18
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v14,v22
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v10,v18
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v10,v18
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v14,v22
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v10,v18
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxufvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v10,v18,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v14,v22,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v10,v18,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v10,v18,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v14,v22,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v10,v18,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v10,v18
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v14,v22
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v10,v18
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v10,v18
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v14,v22
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v10,v18
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v10,v18,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v14,v22,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v10,v18,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v10,v18,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v14,v22,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v10,v18,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v10,v18
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v14,v22
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v10,v18
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v10,v18
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v14,v22
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v10,v18
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v10,v18,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v14,v22,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v10,v18,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v10,v18,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v14,v22,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v10,v18,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v10,v18
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v14,v22
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v10,v18
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v10,v18
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v14,v22
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v10,v18
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v10,v18,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v14,v22,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v10,v18,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v10,v18,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v14,v22,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v10,v18,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v10,v18
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v14,v22
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v10,v18
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v10,v18
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v14,v22
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v10,v18
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v10,v18,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v14,v22,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v10,v18,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v10,v18,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v14,v22,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v10,v18,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v10,v18
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v14,v22
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v10,v18
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v10,v18
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v14,v22
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v10,v18
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v10,v18,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v14,v22,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v10,v18,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v10,v18,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v14,v22,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v10,v18,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfv_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v10,v18
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v14,v22
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v10,v18
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v10,v18
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v14,v22
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v10,v18
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfvm_m2:
	m_nop
	li a0, WARMUP
1:

vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v10,v18,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v14,v22,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v10,v18,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v10,v18,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v14,v22,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v10,v18,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfncvtxufw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v10,v18
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v14,v22
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v10,v18
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v10,v18
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v14,v22
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v10,v18
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxufwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v10,v18,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v14,v22,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v10,v18,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v10,v18,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v14,v22,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v10,v18,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.x.f.w v8,v16
vfncvt.x.f.w v10,v18
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v14,v22
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v10,v18
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.x.f.w v8,v16
vfncvt.x.f.w v10,v18
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v14,v22
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v10,v18
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v10,v18,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v14,v22,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v10,v18,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v10,v18,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v14,v22,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v10,v18,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v10,v18
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v14,v22
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v10,v18
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v10,v18
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v14,v22
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v10,v18
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v10,v18,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v14,v22,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v10,v18,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v10,v18,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v14,v22,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v10,v18,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.x.w v8,v16
vfncvt.f.x.w v10,v18
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v14,v22
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v10,v18
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.x.w v8,v16
vfncvt.f.x.w v10,v18
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v14,v22
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v10,v18
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v10,v18,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v14,v22,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v10,v18,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v10,v18,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v14,v22,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v10,v18,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.f.w v8,v16
vfncvt.f.f.w v10,v18
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v14,v22
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v10,v18
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.f.w v8,v16
vfncvt.f.f.w v10,v18
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v14,v22
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v10,v18
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v10,v18,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v14,v22,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v10,v18,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v10,v18,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v14,v22,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v10,v18,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v10,v18
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v14,v22
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v10,v18
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v10,v18
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v14,v22
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v10,v18
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v10,v18,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v14,v22,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v10,v18,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v10,v18,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v14,v22,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v10,v18,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufw_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v10,v18
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v14,v22
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v10,v18
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v10,v18
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v14,v22
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v10,v18
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufwm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v10,v18,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v14,v22,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v10,v18,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v10,v18,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v14,v22,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v10,v18,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.w_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v10,v18
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v14,v22
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v10,v18
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v10,v18
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v14,v22
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v10,v18
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.wm_m2:
	m_nop
	li a0, WARMUP
1:

vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v10,v18,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v14,v22,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v10,v18,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v10,v18,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v14,v22,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v10,v18,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfsqrtv_m2:
	m_nop
	li a0, WARMUP
1:

vfsqrt.v v8,v16
vfsqrt.v v10,v18
vfsqrt.v v12,v20
vfsqrt.v v14,v22
vfsqrt.v v8,v16
vfsqrt.v v10,v18
vfsqrt.v v12,v20
vfsqrt.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsqrt.v v8,v16
vfsqrt.v v10,v18
vfsqrt.v v12,v20
vfsqrt.v v14,v22
vfsqrt.v v8,v16
vfsqrt.v v10,v18
vfsqrt.v v12,v20
vfsqrt.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsqrtvm_m2:
	m_nop
	li a0, WARMUP
1:

vfsqrt.v v8,v16,v0.t
vfsqrt.v v10,v18,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v14,v22,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v10,v18,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfsqrt.v v8,v16,v0.t
vfsqrt.v v10,v18,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v14,v22,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v10,v18,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7v_m2:
	m_nop
	li a0, WARMUP
1:

vfrsqrt7.v v8,v16
vfrsqrt7.v v10,v18
vfrsqrt7.v v12,v20
vfrsqrt7.v v14,v22
vfrsqrt7.v v8,v16
vfrsqrt7.v v10,v18
vfrsqrt7.v v12,v20
vfrsqrt7.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrsqrt7.v v8,v16
vfrsqrt7.v v10,v18
vfrsqrt7.v v12,v20
vfrsqrt7.v v14,v22
vfrsqrt7.v v8,v16
vfrsqrt7.v v10,v18
vfrsqrt7.v v12,v20
vfrsqrt7.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7vm_m2:
	m_nop
	li a0, WARMUP
1:

vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v10,v18,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v14,v22,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v10,v18,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v10,v18,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v14,v22,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v10,v18,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7v_m2:
	m_nop
	li a0, WARMUP
1:

vfrec7.v v8,v16
vfrec7.v v10,v18
vfrec7.v v12,v20
vfrec7.v v14,v22
vfrec7.v v8,v16
vfrec7.v v10,v18
vfrec7.v v12,v20
vfrec7.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrec7.v v8,v16
vfrec7.v v10,v18
vfrec7.v v12,v20
vfrec7.v v14,v22
vfrec7.v v8,v16
vfrec7.v v10,v18
vfrec7.v v12,v20
vfrec7.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7vm_m2:
	m_nop
	li a0, WARMUP
1:

vfrec7.v v8,v16,v0.t
vfrec7.v v10,v18,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v14,v22,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v10,v18,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfrec7.v v8,v16,v0.t
vfrec7.v v10,v18,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v14,v22,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v10,v18,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassv_m2:
	m_nop
	li a0, WARMUP
1:

vfclass.v v8,v16
vfclass.v v10,v18
vfclass.v v12,v20
vfclass.v v14,v22
vfclass.v v8,v16
vfclass.v v10,v18
vfclass.v v12,v20
vfclass.v v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfclass.v v8,v16
vfclass.v v10,v18
vfclass.v v12,v20
vfclass.v v14,v22
vfclass.v v8,v16
vfclass.v v10,v18
vfclass.v v12,v20
vfclass.v v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassvm_m2:
	m_nop
	li a0, WARMUP
1:

vfclass.v v8,v16,v0.t
vfclass.v v10,v18,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v14,v22,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v10,v18,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vfclass.v v8,v16,v0.t
vfclass.v v10,v18,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v14,v22,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v10,v18,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmsbfm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsbf.m v8,v16
vmsbf.m v10,v18
vmsbf.m v12,v20
vmsbf.m v14,v22
vmsbf.m v8,v16
vmsbf.m v10,v18
vmsbf.m v12,v20
vmsbf.m v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbf.m v8,v16
vmsbf.m v10,v18
vmsbf.m v12,v20
vmsbf.m v14,v22
vmsbf.m v8,v16
vmsbf.m v10,v18
vmsbf.m v12,v20
vmsbf.m v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbfmm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsbf.m v8,v16,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v14,v22,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsbf.m v8,v16,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v14,v22,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v10,v18,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsof.m v8,v16
vmsof.m v10,v18
vmsof.m v12,v20
vmsof.m v14,v22
vmsof.m v8,v16
vmsof.m v10,v18
vmsof.m v12,v20
vmsof.m v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsof.m v8,v16
vmsof.m v10,v18
vmsof.m v12,v20
vmsof.m v14,v22
vmsof.m v8,v16
vmsof.m v10,v18
vmsof.m v12,v20
vmsof.m v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofmm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsof.m v8,v16,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v14,v22,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsof.m v8,v16,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v14,v22,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v10,v18,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsif.m v8,v16
vmsif.m v10,v18
vmsif.m v12,v20
vmsif.m v14,v22
vmsif.m v8,v16
vmsif.m v10,v18
vmsif.m v12,v20
vmsif.m v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsif.m v8,v16
vmsif.m v10,v18
vmsif.m v12,v20
vmsif.m v14,v22
vmsif.m v8,v16
vmsif.m v10,v18
vmsif.m v12,v20
vmsif.m v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifmm_m2:
	m_1bit
	li a0, WARMUP
1:

vmsif.m v8,v16,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v14,v22,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vmsif.m v8,v16,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v14,v22,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v10,v18,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotam_m2:
	m_nop
	li a0, WARMUP
1:

viota.m v8,v16
viota.m v10,v18
viota.m v12,v20
viota.m v14,v22
viota.m v8,v16
viota.m v10,v18
viota.m v12,v20
viota.m v14,v22


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

viota.m v8,v16
viota.m v10,v18
viota.m v12,v20
viota.m v14,v22
viota.m v8,v16
viota.m v10,v18
viota.m v12,v20
viota.m v14,v22


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotamm_m2:
	m_nop
	li a0, WARMUP
1:

viota.m v8,v16,v0.t
viota.m v10,v18,v0.t
viota.m v12,v20,v0.t
viota.m v14,v22,v0.t
viota.m v8,v16,v0.t
viota.m v10,v18,v0.t
viota.m v12,v20,v0.t
viota.m v14,v22,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

viota.m v8,v16,v0.t
viota.m v10,v18,v0.t
viota.m v12,v20,v0.t
viota.m v14,v22,v0.t
viota.m v8,v16,v0.t
viota.m v10,v18,v0.t
viota.m v12,v20,v0.t
viota.m v14,v22,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidv_m2:
	m_nop
	li a0, WARMUP
1:

vid.v v8
vid.v v10
vid.v v12
vid.v v14
vid.v v8
vid.v v10
vid.v v12
vid.v v14


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vid.v v8
vid.v v10
vid.v v12
vid.v v14
vid.v v8
vid.v v10
vid.v v12
vid.v v14


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidvm_m2:
	m_nop
	li a0, WARMUP
1:

vid.v v8,v0.t
vid.v v10,v0.t
vid.v v12,v0.t
vid.v v14,v0.t
vid.v v8,v0.t
vid.v v10,v0.t
vid.v v12,v0.t
vid.v v14,v0.t


	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL

vid.v v8,v0.t
vid.v v10,v0.t
vid.v v12,v0.t
vid.v v14,v0.t
vid.v v8,v0.t
vid.v v10,v0.t
vid.v v12,v0.t
vid.v v14,v0.t


.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret






bench_add_m4:
	m_nop
	li a0, WARMUP
1:


add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2
add t0,t1,t2
add t4,t1,t2

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_mul_m4:
	m_nop
	li a0, WARMUP
1:


mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2
mul t0,t1,t2
mul t4,t1,t2

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28
vadd.vv v8,v16,v24
vadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvx_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4
vadd.vx v8,v16,t0
vadd.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvxm_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvi_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13
vadd.vi v8,v16,13
vadd.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvim_m4:
	m_nop
	li a0, WARMUP
1:


vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28
vsub.vv v8,v16,v24
vsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvx_m4:
	m_nop
	li a0, WARMUP
1:


vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4
vsub.vx v8,v16,t0
vsub.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvx_m4:
	m_nop
	li a0, WARMUP
1:


vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4
vrsub.vx v8,v16,t0
vrsub.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvi_m4:
	m_nop
	li a0, WARMUP
1:


vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13
vrsub.vi v8,v16,13
vrsub.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvim_m4:
	m_nop
	li a0, WARMUP
1:


vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvv_m4:
	m_nop
	li a0, WARMUP
1:


vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28
vminu.vv v8,v16,v24
vminu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvx_m4:
	m_nop
	li a0, WARMUP
1:


vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4
vminu.vx v8,v16,t0
vminu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvv_m4:
	m_nop
	li a0, WARMUP
1:


vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28
vmin.vv v8,v16,v24
vmin.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvx_m4:
	m_nop
	li a0, WARMUP
1:


vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4
vmin.vx v8,v16,t0
vmin.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvv_m4:
	m_nop
	li a0, WARMUP
1:


vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28
vmaxu.vv v8,v16,v24
vmaxu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4
vmaxu.vx v8,v16,t0
vmaxu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvv_m4:
	m_nop
	li a0, WARMUP
1:


vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28
vmax.vv v8,v16,v24
vmax.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvx_m4:
	m_nop
	li a0, WARMUP
1:


vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4
vmax.vx v8,v16,t0
vmax.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvv_m4:
	m_nop
	li a0, WARMUP
1:


vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28
vand.vv v8,v16,v24
vand.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvvm_m4:
	m_nop
	li a0, WARMUP
1:


vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvx_m4:
	m_nop
	li a0, WARMUP
1:


vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4
vand.vx v8,v16,t0
vand.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvxm_m4:
	m_nop
	li a0, WARMUP
1:


vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvi_m4:
	m_nop
	li a0, WARMUP
1:


vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13
vand.vi v8,v16,13
vand.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvim_m4:
	m_nop
	li a0, WARMUP
1:


vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvv_m4:
	m_nop
	li a0, WARMUP
1:


vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28
vor.vv v8,v16,v24
vor.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvvm_m4:
	m_nop
	li a0, WARMUP
1:


vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvx_m4:
	m_nop
	li a0, WARMUP
1:


vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4
vor.vx v8,v16,t0
vor.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvxm_m4:
	m_nop
	li a0, WARMUP
1:


vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvi_m4:
	m_nop
	li a0, WARMUP
1:


vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13
vor.vi v8,v16,13
vor.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvim_m4:
	m_nop
	li a0, WARMUP
1:


vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvv_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28
vxor.vv v8,v16,v24
vxor.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvvm_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvx_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4
vxor.vx v8,v16,t0
vxor.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvxm_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvi_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13
vxor.vi v8,v16,13
vxor.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvim_m4:
	m_nop
	li a0, WARMUP
1:


vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vrgathervv_m4:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:


vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28
vrgather.vv v8,v16,v24
vrgather.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervvm_m4:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:


vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervx_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4
vrgather.vx v8,v16,t0
vrgather.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervxm_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervi_m4:
	m_nop
	li a0, WARMUP
1:


vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3
vrgather.vi v8,v16,3
vrgather.vi v12,v20,3

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervim_m4:
	m_nop
	li a0, WARMUP
1:


vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v12,v20,3,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvx_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4
vslideup.vx v8,v16,t0
vslideup.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvxm_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvi_m4:
	m_nop
	li a0, WARMUP
1:


vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3
vslideup.vi v8,v16,3
vslideup.vi v12,v20,3

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvim_m4:
	m_nop
	li a0, WARMUP
1:


vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v12,v20,3,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vv_m4:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:


vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vvm_m4:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:


vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslidedownvx_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4
vslidedown.vx v8,v16,t0
vslidedown.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvxm_m4:
	m_mod_t0_vl
	li a0, WARMUP
1:


vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvi_m4:
	m_nop
	li a0, WARMUP
1:


vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3
vslidedown.vi v8,v16,3
vslidedown.vi v12,v20,3

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvim_m4:
	m_nop
	li a0, WARMUP
1:


vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v12,v20,3,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vredsumvs_m4:
	m_nop
	li a0, WARMUP
1:


vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28
vredsum.vs v8,v16,v24
vredsum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredsumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvs_m4:
	m_nop
	li a0, WARMUP
1:


vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28
vredand.vs v8,v16,v24
vredand.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvs_m4:
	m_nop
	li a0, WARMUP
1:


vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28
vredor.vs v8,v16,v24
vredor.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvs_m4:
	m_nop
	li a0, WARMUP
1:


vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28
vredxor.vs v8,v16,v24
vredxor.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvs_m4:
	m_nop
	li a0, WARMUP
1:


vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28
vredminu.vs v8,v16,v24
vredminu.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvs_m4:
	m_nop
	li a0, WARMUP
1:


vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28
vredmin.vs v8,v16,v24
vredmin.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvs_m4:
	m_nop
	li a0, WARMUP
1:


vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28
vredmaxu.vs v8,v16,v24
vredmaxu.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvs_m4:
	m_nop
	li a0, WARMUP
1:


vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28
vredmax.vs v8,v16,v24
vredmax.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvsm_m4:
	m_nop
	li a0, WARMUP
1:


vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vaadduvv_m4:
	m_nop
	li a0, WARMUP
1:


vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28
vaaddu.vv v8,v16,v24
vaaddu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvvm_m4:
	m_nop
	li a0, WARMUP
1:


vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvx_m4:
	m_nop
	li a0, WARMUP
1:


vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4
vaaddu.vx v8,v16,t0
vaaddu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvxm_m4:
	m_nop
	li a0, WARMUP
1:


vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28
vaadd.vv v8,v16,v24
vaadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvx_m4:
	m_nop
	li a0, WARMUP
1:


vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4
vaadd.vx v8,v16,t0
vaadd.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvxm_m4:
	m_nop
	li a0, WARMUP
1:


vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvv_m4:
	m_nop
	li a0, WARMUP
1:


vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28
vasubu.vv v8,v16,v24
vasubu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvx_m4:
	m_nop
	li a0, WARMUP
1:


vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4
vasubu.vx v8,v16,t0
vasubu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvv_m4:
	m_nop
	li a0, WARMUP
1:


vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28
vasub.vv v8,v16,v24
vasub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvx_m4:
	m_nop
	li a0, WARMUP
1:


vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4
vasub.vx v8,v16,t0
vasub.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslide1upvx_m4:
	m_nop
	li a0, WARMUP
1:


vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4
vslide1up.vx v8,v16,t0
vslide1up.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1upvxm_m4:
	m_nop
	li a0, WARMUP
1:


vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvx_m4:
	m_nop
	li a0, WARMUP
1:


vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4
vslide1down.vx v8,v16,t0
vslide1down.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvxm_m4:
	m_nop
	li a0, WARMUP
1:


vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vadcvvm_m4:
	m_nop
	li a0, WARMUP
1:


vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v12,v20,v28,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvxm_m4:
	m_nop
	li a0, WARMUP
1:


vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v12,v20,t4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvim_m4:
	m_nop
	li a0, WARMUP
1:


vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v12,v20,13,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v12,v20,v28,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v12,v20,t4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvim_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v12,v20,13,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvv_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28
vmadc.vv v8,v16,v24
vmadc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvx_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4
vmadc.vx v8,v16,t0
vmadc.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvi_m4:
	m_nop
	li a0, WARMUP
1:


vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13
vmadc.vi v8,v16,13
vmadc.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v12,v20,v28,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v12,v20,t4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v12,v20,v28,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v12,v20,t4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvv_m4:
	m_nop
	li a0, WARMUP
1:


vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28
vmsbc.vv v8,v16,v24
vmsbc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvx_m4:
	m_nop
	li a0, WARMUP
1:


vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4
vmsbc.vx v8,v16,t0
vmsbc.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmergevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v12,v20,v28,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevxm_m4:
	m_nop
	li a0, WARMUP
1:


vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v12,v20,t4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevim_m4:
	m_nop
	li a0, WARMUP
1:


vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v12,v20,13,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvv_m4:
	m_nop
	li a0, WARMUP
1:


vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20
vmv.v.v v8,v16
vmv.v.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvx_m4:
	m_nop
	li a0, WARMUP
1:


vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4
vmv.v.x v8,t0
vmv.v.x v12,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvi_m4:
	m_nop
	li a0, WARMUP
1:


vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13
vmv.v.i v8,13
vmv.v.i v12,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvv_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28
vmseq.vv v8,v16,v24
vmseq.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvx_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4
vmseq.vx v8,v16,t0
vmseq.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvi_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13
vmseq.vi v8,v16,13
vmseq.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvim_m4:
	m_nop
	li a0, WARMUP
1:


vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevv_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28
vmsne.vv v8,v16,v24
vmsne.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevx_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4
vmsne.vx v8,v16,t0
vmsne.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevi_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13
vmsne.vi v8,v16,13
vmsne.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevim_m4:
	m_nop
	li a0, WARMUP
1:


vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvv_m4:
	m_nop
	li a0, WARMUP
1:


vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28
vmsltu.vv v8,v16,v24
vmsltu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4
vmsltu.vx v8,v16,t0
vmsltu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvv_m4:
	m_nop
	li a0, WARMUP
1:


vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28
vmslt.vv v8,v16,v24
vmslt.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvx_m4:
	m_nop
	li a0, WARMUP
1:


vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4
vmslt.vx v8,v16,t0
vmslt.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvv_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28
vmsleu.vv v8,v16,v24
vmsleu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4
vmsleu.vx v8,v16,t0
vmsleu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvi_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13
vmsleu.vi v8,v16,13
vmsleu.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvim_m4:
	m_nop
	li a0, WARMUP
1:


vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevv_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28
vmsle.vv v8,v16,v24
vmsle.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevx_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4
vmsle.vx v8,v16,t0
vmsle.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevi_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13
vmsle.vi v8,v16,13
vmsle.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevim_m4:
	m_nop
	li a0, WARMUP
1:


vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4
vmsgtu.vx v8,v16,t0
vmsgtu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvi_m4:
	m_nop
	li a0, WARMUP
1:


vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvim_m4:
	m_nop
	li a0, WARMUP
1:


vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvx_m4:
	m_nop
	li a0, WARMUP
1:


vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4
vmsgt.vx v8,v16,t0
vmsgt.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvi_m4:
	m_nop
	li a0, WARMUP
1:


vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13
vmsgt.vi v8,v16,13
vmsgt.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvim_m4:
	m_nop
	li a0, WARMUP
1:


vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcompressvm_m4:
	m_nop
	li a0, WARMUP
1:


vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28
vcompress.vm v8,v16,v24
vcompress.vm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmandnmm_m4:
	m_nop
	li a0, WARMUP
1:


vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28
vmandn.mm v8,v16,v24
vmandn.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmandmm_m4:
	m_nop
	li a0, WARMUP
1:


vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28
vmand.mm v8,v16,v24
vmand.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmormm_m4:
	m_nop
	li a0, WARMUP
1:


vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28
vmor.mm v8,v16,v24
vmor.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxormm_m4:
	m_nop
	li a0, WARMUP
1:


vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28
vmxor.mm v8,v16,v24
vmxor.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmornmm_m4:
	m_nop
	li a0, WARMUP
1:


vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28
vmorn.mm v8,v16,v24
vmorn.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnandmm_m4:
	m_nop
	li a0, WARMUP
1:


vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28
vmnand.mm v8,v16,v24
vmnand.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnormm_m4:
	m_nop
	li a0, WARMUP
1:


vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28
vmnor.mm v8,v16,v24
vmnor.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxnormm_m4:
	m_nop
	li a0, WARMUP
1:


vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28
vmxnor.mm v8,v16,v24
vmxnor.mm v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vsadduvv_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28
vsaddu.vv v8,v16,v24
vsaddu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvx_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4
vsaddu.vx v8,v16,t0
vsaddu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvi_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13
vsaddu.vi v8,v16,13
vsaddu.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvim_m4:
	m_nop
	li a0, WARMUP
1:


vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28
vsadd.vv v8,v16,v24
vsadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvx_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4
vsadd.vx v8,v16,t0
vsadd.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvi_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13
vsadd.vi v8,v16,13
vsadd.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvim_m4:
	m_nop
	li a0, WARMUP
1:


vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvv_m4:
	m_nop
	li a0, WARMUP
1:


vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28
vssubu.vv v8,v16,v24
vssubu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvx_m4:
	m_nop
	li a0, WARMUP
1:


vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4
vssubu.vx v8,v16,t0
vssubu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvv_m4:
	m_nop
	li a0, WARMUP
1:


vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28
vssub.vv v8,v16,v24
vssub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvx_m4:
	m_nop
	li a0, WARMUP
1:


vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4
vssub.vx v8,v16,t0
vssub.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvv_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28
vsll.vv v8,v16,v24
vsll.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvx_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4
vsll.vx v8,v16,t0
vsll.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvi_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13
vsll.vi v8,v16,13
vsll.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvim_m4:
	m_nop
	li a0, WARMUP
1:


vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvv_m4:
	m_nop
	li a0, WARMUP
1:


vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28
vsmul.vv v8,v16,v24
vsmul.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvx_m4:
	m_nop
	li a0, WARMUP
1:


vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4
vsmul.vx v8,v16,t0
vsmul.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv1rv_m4:
	m_nop
	li a0, WARMUP
1:


vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20
vmv1r.v v8,v16
vmv1r.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv2rv_m4:
	m_nop
	li a0, WARMUP
1:


vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20
vmv2r.v v8,v16
vmv2r.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv4rv_m4:
	m_nop
	li a0, WARMUP
1:


vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20
vmv4r.v v8,v16
vmv4r.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv8rv_m4:
	m_nop
	li a0, WARMUP
1:


m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl
m_unimpl

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvv_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28
vsrl.vv v8,v16,v24
vsrl.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvvm_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvx_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4
vsrl.vx v8,v16,t0
vsrl.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvxm_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvi_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13
vsrl.vi v8,v16,13
vsrl.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvim_m4:
	m_nop
	li a0, WARMUP
1:


vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravv_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28
vsra.vv v8,v16,v24
vsra.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravvm_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravx_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4
vsra.vx v8,v16,t0
vsra.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravxm_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravi_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13
vsra.vi v8,v16,13
vsra.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravim_m4:
	m_nop
	li a0, WARMUP
1:


vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvv_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28
vssrl.vv v8,v16,v24
vssrl.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvvm_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvx_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4
vssrl.vx v8,v16,t0
vssrl.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvxm_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvi_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13
vssrl.vi v8,v16,13
vssrl.vi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvim_m4:
	m_nop
	li a0, WARMUP
1:


vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vdivuvv_m4:
	m_nop
	li a0, WARMUP
1:


vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28
vdivu.vv v8,v16,v24
vdivu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvx_m4:
	m_nop
	li a0, WARMUP
1:


vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4
vdivu.vx v8,v16,t0
vdivu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvv_m4:
	m_nop
	li a0, WARMUP
1:


vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28
vdiv.vv v8,v16,v24
vdiv.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvvm_m4:
	m_nop
	li a0, WARMUP
1:


vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvx_m4:
	m_nop
	li a0, WARMUP
1:


vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4
vdiv.vx v8,v16,t0
vdiv.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvxm_m4:
	m_nop
	li a0, WARMUP
1:


vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvv_m4:
	m_nop
	li a0, WARMUP
1:


vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28
vremu.vv v8,v16,v24
vremu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvx_m4:
	m_nop
	li a0, WARMUP
1:


vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4
vremu.vx v8,v16,t0
vremu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvv_m4:
	m_nop
	li a0, WARMUP
1:


vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28
vrem.vv v8,v16,v24
vrem.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvvm_m4:
	m_nop
	li a0, WARMUP
1:


vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvx_m4:
	m_nop
	li a0, WARMUP
1:


vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4
vrem.vx v8,v16,t0
vrem.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvxm_m4:
	m_nop
	li a0, WARMUP
1:


vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvv_m4:
	m_nop
	li a0, WARMUP
1:


vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28
vmulhu.vv v8,v16,v24
vmulhu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4
vmulhu.vx v8,v16,t0
vmulhu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvv_m4:
	m_nop
	li a0, WARMUP
1:


vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28
vmul.vv v8,v16,v24
vmul.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvx_m4:
	m_nop
	li a0, WARMUP
1:


vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4
vmul.vx v8,v16,t0
vmul.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvv_m4:
	m_nop
	li a0, WARMUP
1:


vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28
vmulhsu.vv v8,v16,v24
vmulhsu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvx_m4:
	m_nop
	li a0, WARMUP
1:


vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4
vmulhsu.vx v8,v16,t0
vmulhsu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvv_m4:
	m_nop
	li a0, WARMUP
1:


vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28
vmulh.vv v8,v16,v24
vmulh.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvx_m4:
	m_nop
	li a0, WARMUP
1:


vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4
vmulh.vx v8,v16,t0
vmulh.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28
vmadd.vv v8,v16,v24
vmadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvx_m4:
	m_nop
	li a0, WARMUP
1:


vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20
vmadd.vx v8,t0,v16
vmadd.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28
vmacc.vv v8,v16,v24
vmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvx_m4:
	m_nop
	li a0, WARMUP
1:


vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20
vmacc.vx v8,t0,v16
vmacc.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvxm_m4:
	m_nop
	li a0, WARMUP
1:


vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vnsrlwv_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28
vnsrl.wv v8,v16,v24
vnsrl.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwvm_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwx_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4
vnsrl.wx v8,v16,t0
vnsrl.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwxm_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwi_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13
vnsrl.wi v8,v16,13
vnsrl.wi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwim_m4:
	m_nop
	li a0, WARMUP
1:


vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawv_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28
vnsra.wv v8,v16,v24
vnsra.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawvm_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawx_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4
vnsra.wx v8,v16,t0
vnsra.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawxm_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawi_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13
vnsra.wi v8,v16,13
vnsra.wi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawim_m4:
	m_nop
	li a0, WARMUP
1:


vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwv_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28
vnclipu.wv v8,v16,v24
vnclipu.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwvm_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwx_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4
vnclipu.wx v8,v16,t0
vnclipu.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwxm_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwi_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13
vnclipu.wi v8,v16,13
vnclipu.wi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwim_m4:
	m_nop
	li a0, WARMUP
1:


vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwv_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28
vnclip.wv v8,v16,v24
vnclip.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwvm_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwx_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4
vnclip.wx v8,v16,t0
vnclip.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwxm_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwi_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13
vnclip.wi v8,v16,13
vnclip.wi v12,v20,13

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwim_m4:
	m_nop
	li a0, WARMUP
1:


vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v12,v20,13,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28
vnmsub.vv v8,v16,v24
vnmsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvx_m4:
	m_nop
	li a0, WARMUP
1:


vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20
vnmsub.vx v8,t0,v16
vnmsub.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvv_m4:
	m_nop
	li a0, WARMUP
1:


vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28
vnmsac.vv v8,v16,v24
vnmsac.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvvm_m4:
	m_nop
	li a0, WARMUP
1:


vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvx_m4:
	m_nop
	li a0, WARMUP
1:


vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20
vnmsac.vx v8,t0,v16
vnmsac.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvxm_m4:
	m_nop
	li a0, WARMUP
1:


vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwadduvv_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28
vwaddu.vv v8,v16,v24
vwaddu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvx_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4
vwaddu.vx v8,v16,t0
vwaddu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28
vwadd.vv v8,v16,v24
vwadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvx_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4
vwadd.vx v8,v16,t0
vwadd.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvv_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28
vwsubu.vv v8,v16,v24
vwsubu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvx_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4
vwsubu.vx v8,v16,t0
vwsubu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28
vwsub.vv v8,v16,v24
vwsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvx_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4
vwsub.vx v8,v16,t0
vwsub.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwv_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28
vwaddu.wv v8,v16,v24
vwaddu.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwvm_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwx_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4
vwaddu.wx v8,v16,t0
vwaddu.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwxm_m4:
	m_nop
	li a0, WARMUP
1:


vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwv_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28
vwadd.wv v8,v16,v24
vwadd.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwvm_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwx_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4
vwadd.wx v8,v16,t0
vwadd.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwxm_m4:
	m_nop
	li a0, WARMUP
1:


vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwv_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28
vwsubu.wv v8,v16,v24
vwsubu.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwvm_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwx_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4
vwsubu.wx v8,v16,t0
vwsubu.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwxm_m4:
	m_nop
	li a0, WARMUP
1:


vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwv_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28
vwsub.wv v8,v16,v24
vwsub.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwvm_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwx_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4
vwsub.wx v8,v16,t0
vwsub.wx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwxm_m4:
	m_nop
	li a0, WARMUP
1:


vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28
vwmulu.vv v8,v16,v24
vwmulu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4
vwmulu.vx v8,v16,t0
vwmulu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28
vwmulsu.vv v8,v16,v24
vwmulsu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4
vwmulsu.vx v8,v16,t0
vwmulsu.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28
vwmul.vv v8,v16,v24
vwmul.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4
vwmul.vx v8,v16,t0
vwmul.vx v12,v20,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v12,v20,t4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28
vwmaccu.vv v8,v16,v24
vwmaccu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20
vwmaccu.vx v8,t0,v16
vwmaccu.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28
vwmacc.vv v8,v16,v24
vwmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20
vwmacc.vx v8,t0,v16
vwmacc.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvv_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvvm_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvx_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20
vwmaccus.vx v8,t0,v16
vwmaccus.vx v12,t4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvxm_m4:
	m_nop
	li a0, WARMUP
1:


vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v12,t4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28
vfadd.vv v8,v16,v24
vfadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvf_m4:
	m_nop
	li a0, WARMUP
1:


vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4
vfadd.vf v8,v16,ft0
vfadd.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28
vfsub.vv v8,v16,v24
vfsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvf_m4:
	m_nop
	li a0, WARMUP
1:


vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4
vfsub.vf v8,v16,ft0
vfsub.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28
vfmin.vv v8,v16,v24
vfmin.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4
vfmin.vf v8,v16,ft0
vfmin.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28
vfmax.vv v8,v16,v24
vfmax.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4
vfmax.vf v8,v16,ft0
vfmax.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvv_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28
vfsgnj.vv v8,v16,v24
vfsgnj.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvf_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvv_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvf_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvv_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvf_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvf_m4:
	m_nop
	li a0, WARMUP
1:


vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvf_m4:
	m_nop
	li a0, WARMUP
1:


vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfredusumvs_m4:
	m_nop
	li a0, WARMUP
1:


vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28
vfredusum.vs v8,v16,v24
vfredusum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredusumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvs_m4:
	m_nop
	li a0, WARMUP
1:


vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28
vfredosum.vs v8,v16,v24
vfredosum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvs_m4:
	m_nop
	li a0, WARMUP
1:


vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28
vfredmin.vs v8,v16,v24
vfredmin.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvs_m4:
	m_nop
	li a0, WARMUP
1:


vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28
vfredmax.vs v8,v16,v24
vfredmax.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmergevfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v12,v20,ft4,v0

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4
vfmv.v.f v8,ft0
vfmv.v.f v12,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmfeqvv_m4:
	m_nop
	li a0, WARMUP
1:


vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28
vmfeq.vv v8,v16,v24
vmfeq.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvf_m4:
	m_nop
	li a0, WARMUP
1:


vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4
vmfeq.vf v8,v16,ft0
vmfeq.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvfm_m4:
	m_nop
	li a0, WARMUP
1:


vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevv_m4:
	m_nop
	li a0, WARMUP
1:


vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28
vmfle.vv v8,v16,v24
vmfle.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevf_m4:
	m_nop
	li a0, WARMUP
1:


vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4
vmfle.vf v8,v16,ft0
vmfle.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevfm_m4:
	m_nop
	li a0, WARMUP
1:


vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvv_m4:
	m_nop
	li a0, WARMUP
1:


vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28
vmflt.vv v8,v16,v24
vmflt.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvf_m4:
	m_nop
	li a0, WARMUP
1:


vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4
vmflt.vf v8,v16,ft0
vmflt.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvfm_m4:
	m_nop
	li a0, WARMUP
1:


vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevv_m4:
	m_nop
	li a0, WARMUP
1:


vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28
vmfne.vv v8,v16,v24
vmfne.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevf_m4:
	m_nop
	li a0, WARMUP
1:


vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4
vmfne.vf v8,v16,ft0
vmfne.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevfm_m4:
	m_nop
	li a0, WARMUP
1:


vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvv_m4:
	m_nop
	li a0, WARMUP
1:


vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28
vmfgt.vv v8,v16,v24
vmfgt.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvvm_m4:
	m_nop
	li a0, WARMUP
1:


vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvf_m4:
	m_nop
	li a0, WARMUP
1:


vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4
vmfgt.vf v8,v16,ft0
vmfgt.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvfm_m4:
	m_nop
	li a0, WARMUP
1:


vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevv_m4:
	m_nop
	li a0, WARMUP
1:


vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28
vmfge.vv v8,v16,v24
vmfge.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevvm_m4:
	m_nop
	li a0, WARMUP
1:


vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevf_m4:
	m_nop
	li a0, WARMUP
1:


vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4
vmfge.vf v8,v16,ft0
vmfge.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevfm_m4:
	m_nop
	li a0, WARMUP
1:


vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfdivvv_m4:
	m_nop
	li a0, WARMUP
1:


vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28
vfdiv.vv v8,v16,v24
vfdiv.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvf_m4:
	m_nop
	li a0, WARMUP
1:


vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4
vfdiv.vf v8,v16,ft0
vfdiv.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvf_m4:
	m_nop
	li a0, WARMUP
1:


vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28
vfmul.vv v8,v16,v24
vfmul.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4
vfmul.vf v8,v16,ft0
vfmul.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvf_m4:
	m_nop
	li a0, WARMUP
1:


vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4
vfrsub.vf v8,v16,ft0
vfrsub.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28
vfmadd.vv v8,v16,v24
vfmadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20
vfmadd.vf v8,ft0,v16
vfmadd.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28
vfmsub.vv v8,v16,v24
vfmsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20
vfmsub.vf v8,ft0,v16
vfmsub.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28
vfmacc.vv v8,v16,v24
vfmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20
vfmacc.vf v8,ft0,v16
vfmacc.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvv_m4:
	m_nop
	li a0, WARMUP
1:


vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28
vfmsac.vv v8,v16,v24
vfmsac.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvf_m4:
	m_nop
	li a0, WARMUP
1:


vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20
vfmsac.vf v8,ft0,v16
vfmsac.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfnmsacvv_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28
vfnmsac.vv v8,v16,v24
vfnmsac.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvf_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28
vfnmacc.vv v8,v16,v24
vfnmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvf_m4:
	m_nop
	li a0, WARMUP
1:


vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28
vfnmsub.vv v8,v16,v24
vfnmsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvf_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28
vfnmadd.vv v8,v16,v24
vfnmadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvf_m4:
	m_nop
	li a0, WARMUP
1:


vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwredsumuvs_m4:
	m_nop
	li a0, WARMUP
1:


vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28
vwredsumu.vs v8,v16,v24
vwredsumu.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumuvsm_m4:
	m_nop
	li a0, WARMUP
1:


vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvs_m4:
	m_nop
	li a0, WARMUP
1:


vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28
vwredsum.vs v8,v16,v24
vwredsum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwaddvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28
vfwadd.vv v8,v16,v24
vfwadd.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4
vfwadd.vf v8,v16,ft0
vfwadd.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28
vfwsub.vv v8,v16,v24
vfwsub.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4
vfwsub.vf v8,v16,ft0
vfwsub.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwv_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28
vfwadd.wv v8,v16,v24
vfwadd.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwf_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4
vfwadd.wf v8,v16,ft0
vfwadd.wf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwv_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28
vfwsub.wv v8,v16,v24
vfwsub.wv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwf_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4
vfwsub.wf v8,v16,ft0
vfwsub.wf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28
vfwmul.vv v8,v16,v24
vfwmul.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4
vfwmul.vf v8,v16,ft0
vfwmul.vf v12,v20,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v12,v20,ft4,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28
vfwmacc.vv v8,v16,v24
vfwmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28
vfwmsac.vv v8,v16,v24
vfwmsac.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvv_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvf_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v12,ft4,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvfm_m4:
	m_nop
	li a0, WARMUP
1:


vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v12,ft4,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwredosumvs_m4:
	m_nop
	li a0, WARMUP
1:


vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28
vfwredosum.vs v8,v16,v24
vfwredosum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredosumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvs_m4:
	m_nop
	li a0, WARMUP
1:


vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28
vfwredusum.vs v8,v16,v24
vfwredusum.vs v12,v20,v28

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvsm_m4:
	m_nop
	li a0, WARMUP
1:


vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v12,v20,v28,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmvsx_m4:
	m_nop
	li a0, WARMUP
1:


vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4
vmv.s.x v8,t0
vmv.s.x v12,t4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvxs_m4:
	m_nop
	li a0, WARMUP
1:


vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12
vmv.x.s t0,v8
vmv.x.s t4,v12

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcpopm_m4:
	m_nop
	li a0, WARMUP
1:


vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12
vcpop.m t0,v8
vcpop.m t4,v12

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vcpopmm_m4:
	m_nop
	li a0, WARMUP
1:


vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t4,v12,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstm_m4:
	m_1bit
	li a0, WARMUP
1:


vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12
vfirst.m t0,v8
vfirst.m t4,v12

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstmm_m4:
	m_1bit
	li a0, WARMUP
1:


vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t4,v12,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20
vzext.vf2 v8,v16
vzext.vf2 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2m_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20
vsext.vf2 v8,v16
vsext.vf2 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2m_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20
vzext.vf4 v8,v16
vzext.vf4 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4m_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20
vsext.vf4 v8,v16
vsext.vf4 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4m_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20
vzext.vf8 v8,v16
vzext.vf8 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8m_m4:
	m_1bit
	li a0, WARMUP
1:


vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20
vsext.vf8 v8,v16
vsext.vf8 v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8m_m4:
	m_1bit
	li a0, WARMUP
1:


vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmvfs_m4:
	m_nop
	li a0, WARMUP
1:


vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12
vfmv.f.s ft0,v8
vfmv.f.s ft4,v12

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvsf_m4:
	m_nop
	li a0, WARMUP
1:


vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4
vfmv.s.f v8,ft0
vfmv.s.f v12,ft4

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfcvtxufv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxufvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufv_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufvm_m4:
	m_nop
	li a0, WARMUP
1:


vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwcvtxufv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxufvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfv_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfvm_m4:
	m_nop
	li a0, WARMUP
1:


vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfncvtxufw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxufwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufw_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufwm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.w_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.wm_m4:
	m_nop
	li a0, WARMUP
1:


vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfsqrtv_m4:
	m_nop
	li a0, WARMUP
1:


vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20
vfsqrt.v v8,v16
vfsqrt.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsqrtvm_m4:
	m_nop
	li a0, WARMUP
1:


vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7v_m4:
	m_nop
	li a0, WARMUP
1:


vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20
vfrsqrt7.v v8,v16
vfrsqrt7.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7vm_m4:
	m_nop
	li a0, WARMUP
1:


vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7v_m4:
	m_nop
	li a0, WARMUP
1:


vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20
vfrec7.v v8,v16
vfrec7.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7vm_m4:
	m_nop
	li a0, WARMUP
1:


vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassv_m4:
	m_nop
	li a0, WARMUP
1:


vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20
vfclass.v v8,v16
vfclass.v v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassvm_m4:
	m_nop
	li a0, WARMUP
1:


vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmsbfm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20
vmsbf.m v8,v16
vmsbf.m v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbfmm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20
vmsof.m v8,v16
vmsof.m v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofmm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20
vmsif.m v8,v16
vmsif.m v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifmm_m4:
	m_1bit
	li a0, WARMUP
1:


vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotam_m4:
	m_nop
	li a0, WARMUP
1:


viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20
viota.m v8,v16
viota.m v12,v20

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotamm_m4:
	m_nop
	li a0, WARMUP
1:


viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t
viota.m v8,v16,v0.t
viota.m v12,v20,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidv_m4:
	m_nop
	li a0, WARMUP
1:


vid.v v8
vid.v v12
vid.v v8
vid.v v12
vid.v v8
vid.v v12
vid.v v8
vid.v v12

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vid.v v8
vid.v v12
vid.v v8
vid.v v12
vid.v v8
vid.v v12
vid.v v8
vid.v v12

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidvm_m4:
	m_nop
	li a0, WARMUP
1:


vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t

	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL


vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t
vid.v v8,v0.t
vid.v v12,v0.t

.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret






bench_add_m8:
	m_nop
	li a0, WARMUP
1:



add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
add t0,t1,t2
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_mul_m8:
	m_nop
	li a0, WARMUP
1:



mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
mul t0,t1,t2
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
vadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
vadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvx_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
vadd.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvxm_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
vadd.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvi_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
vadd.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaddvim_m8:
	m_nop
	li a0, WARMUP
1:



vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
vadd.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
vsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
vsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvx_m8:
	m_nop
	li a0, WARMUP
1:



vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
vsub.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
vsub.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvx_m8:
	m_nop
	li a0, WARMUP
1:



vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
vrsub.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
vrsub.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvi_m8:
	m_nop
	li a0, WARMUP
1:



vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
vrsub.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrsubvim_m8:
	m_nop
	li a0, WARMUP
1:



vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
vrsub.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvv_m8:
	m_nop
	li a0, WARMUP
1:



vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
vminu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
vminu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvx_m8:
	m_nop
	li a0, WARMUP
1:



vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
vminu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
vminu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvv_m8:
	m_nop
	li a0, WARMUP
1:



vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
vmin.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
vmin.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvx_m8:
	m_nop
	li a0, WARMUP
1:



vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
vmin.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vminvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
vmin.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvv_m8:
	m_nop
	li a0, WARMUP
1:



vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
vmaxu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
vmaxu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
vmaxu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
vmaxu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvv_m8:
	m_nop
	li a0, WARMUP
1:



vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
vmax.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
vmax.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvx_m8:
	m_nop
	li a0, WARMUP
1:



vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
vmax.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaxvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
vmax.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvv_m8:
	m_nop
	li a0, WARMUP
1:



vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
vand.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvvm_m8:
	m_nop
	li a0, WARMUP
1:



vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
vand.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvx_m8:
	m_nop
	li a0, WARMUP
1:



vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
vand.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvxm_m8:
	m_nop
	li a0, WARMUP
1:



vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
vand.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvi_m8:
	m_nop
	li a0, WARMUP
1:



vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
vand.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vandvim_m8:
	m_nop
	li a0, WARMUP
1:



vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
vand.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvv_m8:
	m_nop
	li a0, WARMUP
1:



vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
vor.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvvm_m8:
	m_nop
	li a0, WARMUP
1:



vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
vor.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvx_m8:
	m_nop
	li a0, WARMUP
1:



vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
vor.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvxm_m8:
	m_nop
	li a0, WARMUP
1:



vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
vor.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvi_m8:
	m_nop
	li a0, WARMUP
1:



vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
vor.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vorvim_m8:
	m_nop
	li a0, WARMUP
1:



vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
vor.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvv_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
vxor.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvvm_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
vxor.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvx_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
vxor.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvxm_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
vxor.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvi_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
vxor.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vxorvim_m8:
	m_nop
	li a0, WARMUP
1:



vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
vxor.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vrgathervv_m8:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:



vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
vrgather.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervvm_m8:
	m_mod_v24_e8_vl
	li a0, WARMUP
1:



vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
vrgather.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervx_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
vrgather.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervxm_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
vrgather.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervi_m8:
	m_nop
	li a0, WARMUP
1:



vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
vrgather.vi v8,v16,3
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgathervim_m8:
	m_nop
	li a0, WARMUP
1:



vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
vrgather.vi v8,v16,3,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvx_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
vslideup.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvxm_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
vslideup.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvi_m8:
	m_nop
	li a0, WARMUP
1:



vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
vslideup.vi v8,v16,3
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslideupvim_m8:
	m_nop
	li a0, WARMUP
1:



vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
vslideup.vi v8,v16,3,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vv_m8:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:



vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
vrgatherei16.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vrgatherei16vvm_m8:
	m_mod_v24_e16_vl
	li a0, WARMUP
1:



vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
vrgatherei16.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslidedownvx_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
vslidedown.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvxm_m8:
	m_mod_t0_vl
	li a0, WARMUP
1:



vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
vslidedown.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvi_m8:
	m_nop
	li a0, WARMUP
1:



vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
vslidedown.vi v8,v16,3
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslidedownvim_m8:
	m_nop
	li a0, WARMUP
1:



vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
vslidedown.vi v8,v16,3,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vredsumvs_m8:
	m_nop
	li a0, WARMUP
1:



vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
vredsum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredsumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
vredsum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvs_m8:
	m_nop
	li a0, WARMUP
1:



vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
vredand.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredandvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
vredand.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvs_m8:
	m_nop
	li a0, WARMUP
1:



vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
vredor.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredorvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
vredor.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvs_m8:
	m_nop
	li a0, WARMUP
1:



vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
vredxor.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredxorvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
vredxor.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvs_m8:
	m_nop
	li a0, WARMUP
1:



vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
vredminu.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminuvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
vredminu.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvs_m8:
	m_nop
	li a0, WARMUP
1:



vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
vredmin.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredminvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
vredmin.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvs_m8:
	m_nop
	li a0, WARMUP
1:



vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
vredmaxu.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxuvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
vredmaxu.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvs_m8:
	m_nop
	li a0, WARMUP
1:



vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
vredmax.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vredmaxvsm_m8:
	m_nop
	li a0, WARMUP
1:



vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
vredmax.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vaadduvv_m8:
	m_nop
	li a0, WARMUP
1:



vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
vaaddu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvvm_m8:
	m_nop
	li a0, WARMUP
1:



vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
vaaddu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvx_m8:
	m_nop
	li a0, WARMUP
1:



vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
vaaddu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaadduvxm_m8:
	m_nop
	li a0, WARMUP
1:



vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
vaaddu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
vaadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
vaadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvx_m8:
	m_nop
	li a0, WARMUP
1:



vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
vaadd.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vaaddvxm_m8:
	m_nop
	li a0, WARMUP
1:



vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
vaadd.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvv_m8:
	m_nop
	li a0, WARMUP
1:



vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
vasubu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
vasubu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvx_m8:
	m_nop
	li a0, WARMUP
1:



vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
vasubu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
vasubu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvv_m8:
	m_nop
	li a0, WARMUP
1:



vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
vasub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
vasub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvx_m8:
	m_nop
	li a0, WARMUP
1:



vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
vasub.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vasubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
vasub.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vslide1upvx_m8:
	m_nop
	li a0, WARMUP
1:



vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
vslide1up.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1upvxm_m8:
	m_nop
	li a0, WARMUP
1:



vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
vslide1up.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvx_m8:
	m_nop
	li a0, WARMUP
1:



vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
vslide1down.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vslide1downvxm_m8:
	m_nop
	li a0, WARMUP
1:



vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
vslide1down.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vadcvvm_m8:
	m_nop
	li a0, WARMUP
1:



vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
vadc.vvm v8,v16,v24,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvxm_m8:
	m_nop
	li a0, WARMUP
1:



vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
vadc.vxm v8,v16,t0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vadcvim_m8:
	m_nop
	li a0, WARMUP
1:



vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
vadc.vim v8,v16,13,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
vmadc.vvm v8,v16,v24,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
vmadc.vxm v8,v16,t0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvim_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
vmadc.vim v8,v16,13,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvv_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
vmadc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvx_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
vmadc.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmadcvi_m8:
	m_nop
	li a0, WARMUP
1:



vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
vmadc.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
vsbc.vvm v8,v16,v24,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsbcvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
vsbc.vxm v8,v16,t0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
vmsbc.vvm v8,v16,v24,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
vmsbc.vxm v8,v16,t0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvv_m8:
	m_nop
	li a0, WARMUP
1:



vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
vmsbc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbcvx_m8:
	m_nop
	li a0, WARMUP
1:



vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
vmsbc.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmergevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
vmerge.vvm v8,v16,v24,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevxm_m8:
	m_nop
	li a0, WARMUP
1:



vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
vmerge.vxm v8,v16,t0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmergevim_m8:
	m_nop
	li a0, WARMUP
1:



vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
vmerge.vim v8,v16,13,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvv_m8:
	m_nop
	li a0, WARMUP
1:



vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
vmv.v.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvx_m8:
	m_nop
	li a0, WARMUP
1:



vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
vmv.v.x v8,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvvi_m8:
	m_nop
	li a0, WARMUP
1:



vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
vmv.v.i v8,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvv_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
vmseq.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
vmseq.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvx_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
vmseq.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
vmseq.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvi_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
vmseq.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmseqvim_m8:
	m_nop
	li a0, WARMUP
1:



vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
vmseq.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevv_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
vmsne.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
vmsne.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevx_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
vmsne.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
vmsne.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevi_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
vmsne.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsnevim_m8:
	m_nop
	li a0, WARMUP
1:



vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
vmsne.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvv_m8:
	m_nop
	li a0, WARMUP
1:



vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
vmsltu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
vmsltu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
vmsltu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
vmsltu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvv_m8:
	m_nop
	li a0, WARMUP
1:



vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
vmslt.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
vmslt.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvx_m8:
	m_nop
	li a0, WARMUP
1:



vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
vmslt.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsltvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
vmslt.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvv_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
vmsleu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
vmsleu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
vmsleu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
vmsleu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvi_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
vmsleu.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsleuvim_m8:
	m_nop
	li a0, WARMUP
1:



vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
vmsleu.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevv_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
vmsle.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
vmsle.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevx_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
vmsle.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
vmsle.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevi_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
vmsle.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmslevim_m8:
	m_nop
	li a0, WARMUP
1:



vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
vmsle.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
vmsgtu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
vmsgtu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvi_m8:
	m_nop
	li a0, WARMUP
1:



vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
vmsgtu.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtuvim_m8:
	m_nop
	li a0, WARMUP
1:



vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
vmsgtu.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvx_m8:
	m_nop
	li a0, WARMUP
1:



vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
vmsgt.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
vmsgt.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvi_m8:
	m_nop
	li a0, WARMUP
1:



vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
vmsgt.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsgtvim_m8:
	m_nop
	li a0, WARMUP
1:



vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
vmsgt.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcompressvm_m8:
	m_nop
	li a0, WARMUP
1:



vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
vcompress.vm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmandnmm_m8:
	m_nop
	li a0, WARMUP
1:



vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
vmandn.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmandmm_m8:
	m_nop
	li a0, WARMUP
1:



vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
vmand.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmormm_m8:
	m_nop
	li a0, WARMUP
1:



vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
vmor.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxormm_m8:
	m_nop
	li a0, WARMUP
1:



vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
vmxor.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmornmm_m8:
	m_nop
	li a0, WARMUP
1:



vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
vmorn.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnandmm_m8:
	m_nop
	li a0, WARMUP
1:



vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
vmnand.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmnormm_m8:
	m_nop
	li a0, WARMUP
1:



vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
vmnor.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmxnormm_m8:
	m_nop
	li a0, WARMUP
1:



vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
vmxnor.mm v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vsadduvv_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
vsaddu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
vsaddu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvx_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
vsaddu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
vsaddu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvi_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
vsaddu.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsadduvim_m8:
	m_nop
	li a0, WARMUP
1:



vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
vsaddu.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
vsadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
vsadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvx_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
vsadd.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
vsadd.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvi_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
vsadd.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsaddvim_m8:
	m_nop
	li a0, WARMUP
1:



vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
vsadd.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvv_m8:
	m_nop
	li a0, WARMUP
1:



vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
vssubu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
vssubu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvx_m8:
	m_nop
	li a0, WARMUP
1:



vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
vssubu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
vssubu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvv_m8:
	m_nop
	li a0, WARMUP
1:



vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
vssub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
vssub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvx_m8:
	m_nop
	li a0, WARMUP
1:



vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
vssub.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
vssub.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvv_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
vsll.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
vsll.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvx_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
vsll.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
vsll.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvi_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
vsll.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsllvim_m8:
	m_nop
	li a0, WARMUP
1:



vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
vsll.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvv_m8:
	m_nop
	li a0, WARMUP
1:



vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
vsmul.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
vsmul.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvx_m8:
	m_nop
	li a0, WARMUP
1:



vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
vsmul.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsmulvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
vsmul.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv1rv_m8:
	m_nop
	li a0, WARMUP
1:



vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
vmv1r.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv2rv_m8:
	m_nop
	li a0, WARMUP
1:



vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
vmv2r.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv4rv_m8:
	m_nop
	li a0, WARMUP
1:



vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
vmv4r.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmv8rv_m8:
	m_nop
	li a0, WARMUP
1:



vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
vmv8r.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvv_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
vsrl.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvvm_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
vsrl.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvx_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
vsrl.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvxm_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
vsrl.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvi_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
vsrl.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsrlvim_m8:
	m_nop
	li a0, WARMUP
1:



vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
vsrl.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravv_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
vsra.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravvm_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
vsra.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravx_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
vsra.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravxm_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
vsra.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravi_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
vsra.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsravim_m8:
	m_nop
	li a0, WARMUP
1:



vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
vsra.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvv_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
vssrl.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvvm_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
vssrl.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvx_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
vssrl.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvxm_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
vssrl.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvi_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
vssrl.vi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vssrlvim_m8:
	m_nop
	li a0, WARMUP
1:



vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
vssrl.vi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vdivuvv_m8:
	m_nop
	li a0, WARMUP
1:



vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
vdivu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
vdivu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvx_m8:
	m_nop
	li a0, WARMUP
1:



vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
vdivu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
vdivu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvv_m8:
	m_nop
	li a0, WARMUP
1:



vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
vdiv.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvvm_m8:
	m_nop
	li a0, WARMUP
1:



vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
vdiv.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvx_m8:
	m_nop
	li a0, WARMUP
1:



vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
vdiv.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vdivvxm_m8:
	m_nop
	li a0, WARMUP
1:



vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
vdiv.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvv_m8:
	m_nop
	li a0, WARMUP
1:



vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
vremu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
vremu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvx_m8:
	m_nop
	li a0, WARMUP
1:



vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
vremu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
vremu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvv_m8:
	m_nop
	li a0, WARMUP
1:



vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
vrem.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvvm_m8:
	m_nop
	li a0, WARMUP
1:



vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
vrem.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvx_m8:
	m_nop
	li a0, WARMUP
1:



vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
vrem.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vremvxm_m8:
	m_nop
	li a0, WARMUP
1:



vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
vrem.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvv_m8:
	m_nop
	li a0, WARMUP
1:



vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
vmulhu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
vmulhu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
vmulhu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
vmulhu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvv_m8:
	m_nop
	li a0, WARMUP
1:



vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
vmul.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
vmul.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvx_m8:
	m_nop
	li a0, WARMUP
1:



vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
vmul.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
vmul.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvv_m8:
	m_nop
	li a0, WARMUP
1:



vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
vmulhsu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
vmulhsu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvx_m8:
	m_nop
	li a0, WARMUP
1:



vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
vmulhsu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhsuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
vmulhsu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvv_m8:
	m_nop
	li a0, WARMUP
1:



vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
vmulh.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
vmulh.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvx_m8:
	m_nop
	li a0, WARMUP
1:



vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
vmulh.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmulhvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
vmulh.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
vmadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
vmadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvx_m8:
	m_nop
	li a0, WARMUP
1:



vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
vmadd.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaddvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
vmadd.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
vmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
vmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvx_m8:
	m_nop
	li a0, WARMUP
1:



vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
vmacc.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmaccvxm_m8:
	m_nop
	li a0, WARMUP
1:



vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
vmacc.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vnsrlwv_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
vnsrl.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwvm_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
vnsrl.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwx_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
vnsrl.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwxm_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
vnsrl.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwi_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
vnsrl.wi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrlwim_m8:
	m_nop
	li a0, WARMUP
1:



vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
vnsrl.wi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawv_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
vnsra.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawvm_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
vnsra.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawx_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
vnsra.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawxm_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
vnsra.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawi_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
vnsra.wi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnsrawim_m8:
	m_nop
	li a0, WARMUP
1:



vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
vnsra.wi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwv_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
vnclipu.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwvm_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
vnclipu.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwx_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
vnclipu.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwxm_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
vnclipu.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwi_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
vnclipu.wi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipuwim_m8:
	m_nop
	li a0, WARMUP
1:



vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
vnclipu.wi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwv_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
vnclip.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwvm_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
vnclip.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwx_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
vnclip.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwxm_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
vnclip.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwi_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
vnclip.wi v8,v16,13
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnclipwim_m8:
	m_nop
	li a0, WARMUP
1:



vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
vnclip.wi v8,v16,13,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
vnmsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
vnmsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvx_m8:
	m_nop
	li a0, WARMUP
1:



vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
vnmsub.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
vnmsub.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvv_m8:
	m_nop
	li a0, WARMUP
1:



vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
vnmsac.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvvm_m8:
	m_nop
	li a0, WARMUP
1:



vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
vnmsac.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvx_m8:
	m_nop
	li a0, WARMUP
1:



vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
vnmsac.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vnmsacvxm_m8:
	m_nop
	li a0, WARMUP
1:



vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
vnmsac.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwadduvv_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
vwaddu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
vwaddu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvx_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
vwaddu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
vwaddu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
vwadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
vwadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvx_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
vwadd.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
vwadd.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvv_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
vwsubu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
vwsubu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvx_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
vwsubu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
vwsubu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
vwsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
vwsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvx_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
vwsub.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
vwsub.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwv_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
vwaddu.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwvm_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
vwaddu.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwx_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
vwaddu.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwadduwxm_m8:
	m_nop
	li a0, WARMUP
1:



vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
vwaddu.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwv_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
vwadd.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwvm_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
vwadd.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwx_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
vwadd.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwaddwxm_m8:
	m_nop
	li a0, WARMUP
1:



vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
vwadd.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwv_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
vwsubu.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwvm_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
vwsubu.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwx_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
vwsubu.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubuwxm_m8:
	m_nop
	li a0, WARMUP
1:



vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
vwsubu.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwv_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
vwsub.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwvm_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
vwsub.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwx_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
vwsub.wx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwsubwxm_m8:
	m_nop
	li a0, WARMUP
1:



vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
vwsub.wx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
vwmulu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
vwmulu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
vwmulu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmuluvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
vwmulu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
vwmulsu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
vwmulsu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
vwmulsu.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulsuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
vwmulsu.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
vwmul.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
vwmul.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
vwmul.vx v8,v16,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmulvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
vwmul.vx v8,v16,t0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
vwmaccu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
vwmaccu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
vwmaccu.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
vwmaccu.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
vwmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
vwmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
vwmacc.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
vwmacc.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvv_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
vwmaccsu.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvvm_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
vwmaccsu.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
vwmaccsu.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccsuvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
vwmaccsu.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvx_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
vwmaccus.vx v8,t0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwmaccusvxm_m8:
	m_nop
	li a0, WARMUP
1:



vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
vwmaccus.vx v8,t0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
vfadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
vfadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvf_m8:
	m_nop
	li a0, WARMUP
1:



vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
vfadd.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfaddvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
vfadd.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
vfsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
vfsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvf_m8:
	m_nop
	li a0, WARMUP
1:



vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
vfsub.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsubvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
vfsub.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
vfmin.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
vfmin.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
vfmin.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfminvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
vfmin.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
vfmax.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
vfmax.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
vfmax.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaxvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
vfmax.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvv_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
vfsgnj.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
vfsgnj.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvf_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
vfsgnj.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
vfsgnj.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvv_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
vfsgnjn.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
vfsgnjn.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvf_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
vfsgnjn.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjnvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
vfsgnjn.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvv_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
vfsgnjx.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
vfsgnjx.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvf_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
vfsgnjx.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsgnjxvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
vfsgnjx.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvf_m8:
	m_nop
	li a0, WARMUP
1:



vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
vfslide1up.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1upvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
vfslide1up.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvf_m8:
	m_nop
	li a0, WARMUP
1:



vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
vfslide1down.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfslide1downvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
vfslide1down.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfredusumvs_m8:
	m_nop
	li a0, WARMUP
1:



vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
vfredusum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredusumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
vfredusum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvs_m8:
	m_nop
	li a0, WARMUP
1:



vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
vfredosum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredosumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
vfredosum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvs_m8:
	m_nop
	li a0, WARMUP
1:



vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
vfredmin.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredminvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
vfredmin.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvs_m8:
	m_nop
	li a0, WARMUP
1:



vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
vfredmax.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfredmaxvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
vfredmax.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmergevfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
vfmerge.vfm v8,v16,ft0,v0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
vfmv.v.f v8,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmfeqvv_m8:
	m_nop
	li a0, WARMUP
1:



vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
vmfeq.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
vmfeq.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvf_m8:
	m_nop
	li a0, WARMUP
1:



vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
vmfeq.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfeqvfm_m8:
	m_nop
	li a0, WARMUP
1:



vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
vmfeq.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevv_m8:
	m_nop
	li a0, WARMUP
1:



vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
vmfle.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
vmfle.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevf_m8:
	m_nop
	li a0, WARMUP
1:



vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
vmfle.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmflevfm_m8:
	m_nop
	li a0, WARMUP
1:



vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
vmfle.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvv_m8:
	m_nop
	li a0, WARMUP
1:



vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
vmflt.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
vmflt.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvf_m8:
	m_nop
	li a0, WARMUP
1:



vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
vmflt.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfltvfm_m8:
	m_nop
	li a0, WARMUP
1:



vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
vmflt.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevv_m8:
	m_nop
	li a0, WARMUP
1:



vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
vmfne.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
vmfne.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevf_m8:
	m_nop
	li a0, WARMUP
1:



vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
vmfne.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfnevfm_m8:
	m_nop
	li a0, WARMUP
1:



vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
vmfne.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvv_m8:
	m_nop
	li a0, WARMUP
1:



vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
vmfgt.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvvm_m8:
	m_nop
	li a0, WARMUP
1:



vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
vmfgt.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvf_m8:
	m_nop
	li a0, WARMUP
1:



vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
vmfgt.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgtvfm_m8:
	m_nop
	li a0, WARMUP
1:



vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
vmfgt.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevv_m8:
	m_nop
	li a0, WARMUP
1:



vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
vmfge.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevvm_m8:
	m_nop
	li a0, WARMUP
1:



vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
vmfge.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevf_m8:
	m_nop
	li a0, WARMUP
1:



vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
vmfge.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmfgevfm_m8:
	m_nop
	li a0, WARMUP
1:



vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
vmfge.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfdivvv_m8:
	m_nop
	li a0, WARMUP
1:



vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
vfdiv.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
vfdiv.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvf_m8:
	m_nop
	li a0, WARMUP
1:



vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
vfdiv.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfdivvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
vfdiv.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvf_m8:
	m_nop
	li a0, WARMUP
1:



vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
vfrdiv.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrdivvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
vfrdiv.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
vfmul.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
vfmul.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
vfmul.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmulvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
vfmul.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvf_m8:
	m_nop
	li a0, WARMUP
1:



vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
vfrsub.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsubvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
vfrsub.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
vfmadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
vfmadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
vfmadd.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaddvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
vfmadd.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
vfmsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
vfmsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
vfmsub.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsubvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
vfmsub.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
vfmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
vfmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
vfmacc.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmaccvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
vfmacc.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvv_m8:
	m_nop
	li a0, WARMUP
1:



vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
vfmsac.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
vfmsac.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvf_m8:
	m_nop
	li a0, WARMUP
1:



vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
vfmsac.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmsacvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
vfmsac.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfnmsacvv_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
vfnmsac.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
vfnmsac.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvf_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
vfnmsac.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsacvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
vfnmsac.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
vfnmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
vfnmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvf_m8:
	m_nop
	li a0, WARMUP
1:



vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
vfnmacc.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaccvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
vfnmacc.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
vfnmsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
vfnmsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvf_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
vfnmsub.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmsubvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
vfnmsub.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
vfnmadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
vfnmadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvf_m8:
	m_nop
	li a0, WARMUP
1:



vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
vfnmadd.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfnmaddvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
vfnmadd.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vwredsumuvs_m8:
	m_nop
	li a0, WARMUP
1:



vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
vwredsumu.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumuvsm_m8:
	m_nop
	li a0, WARMUP
1:



vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
vwredsumu.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvs_m8:
	m_nop
	li a0, WARMUP
1:



vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
vwredsum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vwredsumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
vwredsum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwaddvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
vfwadd.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
vfwadd.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
vfwadd.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
vfwadd.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
vfwsub.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
vfwsub.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
vfwsub.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
vfwsub.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwv_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
vfwadd.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
vfwadd.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwf_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
vfwadd.wf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwaddwfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
vfwadd.wf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwv_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
vfwsub.wv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
vfwsub.wv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwf_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
vfwsub.wf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwsubwfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
vfwsub.wf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
vfwmul.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
vfwmul.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
vfwmul.vf v8,v16,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmulvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
vfwmul.vf v8,v16,ft0,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
vfwmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
vfwmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
vfwmacc.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmaccvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
vfwmacc.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
vfwnmacc.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
vfwnmacc.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
vfwnmacc.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmaccvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
vfwnmacc.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
vfwmsac.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
vfwmsac.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
vfwmsac.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwmsacvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
vfwmsac.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvv_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
vfwnmsac.vv v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
vfwnmsac.vv v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvf_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
vfwnmsac.vf v8,ft0,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwnmsacvfm_m8:
	m_nop
	li a0, WARMUP
1:



vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
vfwnmsac.vf v8,ft0,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwredosumvs_m8:
	m_nop
	li a0, WARMUP
1:



vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
vfwredosum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredosumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
vfwredosum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvs_m8:
	m_nop
	li a0, WARMUP
1:



vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
vfwredusum.vs v8,v16,v24
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwredusumvsm_m8:
	m_nop
	li a0, WARMUP
1:



vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
vfwredusum.vs v8,v16,v24,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmvsx_m8:
	m_nop
	li a0, WARMUP
1:



vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
vmv.s.x v8,t0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmvxs_m8:
	m_nop
	li a0, WARMUP
1:



vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
vmv.x.s t0,v8
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vcpopm_m8:
	m_nop
	li a0, WARMUP
1:



vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
vcpop.m t0,v8
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vcpopmm_m8:
	m_nop
	li a0, WARMUP
1:



vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
vcpop.m t0,v8,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstm_m8:
	m_1bit
	li a0, WARMUP
1:



vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
vfirst.m t0,v8
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfirstmm_m8:
	m_1bit
	li a0, WARMUP
1:



vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
vfirst.m t0,v8,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
vzext.vf2 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf2m_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
vzext.vf2 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
vsext.vf2 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf2m_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
vsext.vf2 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
vzext.vf4 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf4m_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
vzext.vf4 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
vsext.vf4 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf4m_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
vsext.vf4 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
vzext.vf8 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vzextvf8m_m8:
	m_1bit
	li a0, WARMUP
1:



vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
vzext.vf8 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
vsext.vf8 v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vsextvf8m_m8:
	m_1bit
	li a0, WARMUP
1:



vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
vsext.vf8 v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfmvfs_m8:
	m_nop
	li a0, WARMUP
1:



vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
vfmv.f.s ft0,v8
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfmvsf_m8:
	m_nop
	li a0, WARMUP
1:



vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
vfmv.s.f v8,ft0
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfcvtxufv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
vfcvt.xu.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxufvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
vfcvt.xu.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
vfcvt.x.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtxfvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
vfcvt.x.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
vfcvt.f.xu.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxuvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
vfcvt.f.xu.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
vfcvt.f.x.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtfxvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
vfcvt.f.x.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
vfcvt.rtz.x.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxfvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
vfcvt.rtz.x.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufv_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
vfcvt.rtz.xu.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfcvtrtzxufvm_m8:
	m_nop
	li a0, WARMUP
1:



vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
vfcvt.rtz.xu.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfwcvtxufv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
vfwcvt.xu.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxufvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
vfwcvt.xu.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
vfwcvt.x.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtxfvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
vfwcvt.x.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
vfwcvt.f.xu.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxuvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
vfwcvt.f.xu.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
vfwcvt.f.x.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtfxvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
vfwcvt.f.x.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
vfwcvt.f.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtffvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
vfwcvt.f.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
vfwcvt.rtz.xu.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxufvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
vfwcvt.rtz.xu.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfv_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
vfwcvt.rtz.x.f.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfwcvtrtzxfvm_m8:
	m_nop
	li a0, WARMUP
1:



vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
vfwcvt.rtz.x.f.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfncvtxufw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
vfncvt.xu.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxufwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
vfncvt.xu.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
vfncvt.x.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtxfwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
vfncvt.x.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
vfncvt.f.xu.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxuwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
vfncvt.f.xu.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
vfncvt.f.x.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtfxwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
vfncvt.f.x.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
vfncvt.f.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtffwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
vfncvt.f.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
vfncvt.rtz.x.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxfwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
vfncvt.rtz.x.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufw_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
vfncvt.rtz.xu.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvtrtzxufwm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
vfncvt.rtz.xu.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.w_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
vfncvt.rod.f.f.w v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfncvt.rod.f.f.wm_m8:
	m_nop
	li a0, WARMUP
1:



vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
vfncvt.rod.f.f.w v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vfsqrtv_m8:
	m_nop
	li a0, WARMUP
1:



vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
vfsqrt.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfsqrtvm_m8:
	m_nop
	li a0, WARMUP
1:



vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
vfsqrt.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7v_m8:
	m_nop
	li a0, WARMUP
1:



vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
vfrsqrt7.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrsqrt7vm_m8:
	m_nop
	li a0, WARMUP
1:



vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
vfrsqrt7.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7v_m8:
	m_nop
	li a0, WARMUP
1:



vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
vfrec7.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfrec7vm_m8:
	m_nop
	li a0, WARMUP
1:



vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
vfrec7.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassv_m8:
	m_nop
	li a0, WARMUP
1:



vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
vfclass.v v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vfclassvm_m8:
	m_nop
	li a0, WARMUP
1:



vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
vfclass.v v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret



bench_vmsbfm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
vmsbf.m v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsbfmm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
vmsbf.m v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
vmsof.m v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsofmm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
vmsof.m v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
vmsif.m v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vmsifmm_m8:
	m_1bit
	li a0, WARMUP
1:



vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
vmsif.m v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotam_m8:
	m_nop
	li a0, WARMUP
1:



viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
viota.m v8,v16
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_viotamm_m8:
	m_nop
	li a0, WARMUP
1:



viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
viota.m v8,v16,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidv_m8:
	m_nop
	li a0, WARMUP
1:



vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
vid.v v8
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret


bench_vidvm_m8:
	m_nop
	li a0, WARMUP
1:



vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
	addi a0, a0, -1
	bnez a0, 1b
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a3, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a3, mcycle
#else
	csrr a3, cycle
#endif
	li a0, LOOP
1:
.rept UNROLL



vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
vid.v v8,v0.t
.endr
	addi a0, a0, -1
	bnez a0, 1b
	fence.i
#if defined(USE_PERF_EVENT_SLOW)
	ld a0, nolibc_perf_event_fd
	la a1, u64_cycle
	li a2, 8
	li a7, 63
	ecall # clobbers vregs and vtype
	ld a0, u64_cycle
	vsetvl VL, VL, s2
	bnez VL, 2f
	vsetvl VL, x0, s2
	2:
#elif defined(READ_MCYCLE)
	csrr a0, mcycle
#else
	csrr a0, cycle
#endif
	sub a0, a0, a3
ret





randomize:
#if __riscv_xlen == 32
	li a1, 0x85ebca6b
	li a2, 0xc2b2ae35
	li a3, 0x7feb352d
#else
	li a1, 0xa0761d6485ebca6b
	li a2, 0x78bd642fc2b2ae35
	li a3, 0xe7037ed17feb352d
#endif

	vsetvli a4, x0, e8, m8, ta, ma
	vid.v v0
	vsetvli a4, x0, e16, m8, ta, ma
	vmul.vx v0, v0, a3
	vid.v v8
	vadd.vv v0, v0, v8
	vxor.vx v0, v0, a0

	# murmurhash32 finalizer
	vsetvli a4, x0, e32, m8, ta, ma

	vsrl.vi v8, v8, 16
	vxor.vv v0, v0, v8
	vmul.vx v0, v0, a1

	vsrl.vi v8, v8, 13
	vxor.vv v0, v0, v8
	vmul.vx v0, v0, a2

	vsrl.vi v8, v8, 16
	vxor.vv v0, v0, v8

	# mix to other registers
	vmul.vx v8,  v0, a1
	vmul.vx v16, v0, a2
	vmul.vx v24, v0, a3


	# zero floating point exponent bit to avoid NaN/Inf
#if __riscv_xlen == 32
	vsetvli a4, x0, e32, m8, ta, ma
	li a4, ~0x40004000 # zero upper f16/f32/f64 exponent bit
#else
	vsetvli a4, x0, e64, m8, ta, ma
	li a4, ~0x4000000040004000 # zero upper f16/f32/f64 exponent bit
#endif
	vand.vx v0, v0, a4
	vand.vx v8, v8, a4
	vand.vx v16, v16, a4
	vand.vx v24, v24, a4

	# fill t* & ft* with wyhash
	.macro randomize_reg x xs:vararg
		add a0, a0, a1
		xor a5, a0, a2
		mulh a6, a0, a5
		mulhu a7, a0, a5
		xor \x, a6, a7
		and a5, \x, a4 # zero upper f16/f32/f64 exponent bit
		// fmv.w.x f\()\x, a5
		.ifnb
			randomize_reg \xs
		.endif
	.endm
	randomize_reg t0, t1, t2, t3, t4, t5, t6, t7, t8

	ret

# u64 f(u64 (*bench)(void), u64 type, u64 vl, u64 seed)
.global run_bench
run_bench:
	addi sp, sp, -48
	sx ra, 8(sp)
	sx VL, 16(sp)
	sx s1, 24(sp)
	sx s2, 32(sp)
	sx VL, 40(sp)

	mv s1, a0
	mv s2, a1 # type
	mv VL, a2
	mv a0, a3 # seed
	call randomize
	vsetvl VL, VL, s2
	bnez VL, 1f
	vsetvl VL, x0, s2
	1:
	jalr s1

	lx ra, 8(sp)
	lx VL, 16(sp)
	lx s1, 24(sp)
	lx s2, 32(sp)
	lx VL, 40(sp)
	addi sp, sp, 48
	ret
